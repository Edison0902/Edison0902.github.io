<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Python学习]]></title>
    <url>%2F%E8%AF%AD%E8%A8%80%E5%9F%BA%E7%A1%80%2FPython%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[本文档仅作为个人和团队学习使用，不带有商业用途，未经许可不得转载。 如有侵权，请联系笔者：JiahaoChen@whu.edu.cn Python注意事项 参考自廖雪峰《Python3基础教程》 变量与内存123a = 'ABC'b = aa = 'xyz' 对于第一句话，解释器的工作是：创建字符串 ‘ABC’ 和变量 a，并且将 a 指向 ‘ABC’；第二句话，创建变量 b，并且把b指向a指的字符串。 关于编码对bytes类型的数据用带b前缀表示，如：**x = b&#39;ABC&#39;**; bytes的每个字符都只占用一个字节。可以通过**encode()**方法和**decode()**方法实现bytes和str之间的转换，如： 123'ABC' .encode('ascii')'中文' .encode('utf-8')b'ABC' .decode('ascii') 要确保文本编辑器采用的编码是：UTF-8 without BOM.py文件可以声明一下： # -*- coding: utf-8 -*-不要用windows自带的记事本编辑文件，自带记事本会在里面加几个带BOM的字符！！！ ### 容器 (一) lists（列表） (1)下标负的为从后往前数12array = [3,2,1,5,4]print (array[-2]) result: 5_ (2) 可以不同类型123array = [3,2,1,5,4]array[2] = 'fool'print (array) result: [3, 2, ‘fool’, 5, 4]_ (3) append方法，添加元素123array = [3,2,1,5,4]array.append('bar')print (array) result: [3, 2, 1, 5, 4, ‘bar’] (4) 删除尾部元素123array = [3,2,1,5,4]x = array.pop()print (x, array) result: (4, [3, 2, 1, 5]) (5) Slicing（切片）类似于matlab语法 12array = [3,2,1,5,4]print (array[2:5]) result: [1, 5, 4] (6) Loop（循环）方式一： 12for c in array : print (c) 方式二：如果要在循环体内访问每个元素的指针，使用“enumerate”函数 123tools = ['vs2017', 'eclipse', 'pycharm']for idx, tools in enumerate(tools): print ('#%d: %s\n' % (idx+1, tools)) result: #1: vs2017_ #2: eclipse__ #3: pycharm_ 方式三：使用迭代器iter() 1234array = [1,2,3,4, 'chen', 2.36]it = iter(list)for c in it : print (c) ####方式四：使用zip()遍历多个序列 1234lastname = ['chen', 'long']firstname = ['jiahao', 'yindi']for i, j in zip(lastname, firstname): print(' &#123;0&#125; - &#123;1&#125;.'.format(i, j)) result: chen - jiahao._ long - yindi.方式五：特殊方式遍历 12345678#颠倒输出for i in reversed(range(1,10,2)): print(i,end = ' ') #排好序输出basket = ['apple', 'orange', 'apple', 'pear', 'orange', 'banana']for f in sorted(set(basket)): print(f,end=',') result: 9 7 5 3 1_ apple,banana,orange,pear,_ (7) List Comprehensions（列表推导）1234567891011array = [3,2,1,5,4]#传统方法s1 = []for x in array: s1.append(x**2)print (s1)#列表推导法s2 = [x**2 for x in array]print (s2) result: [9, 4, 1, 25, 16] 列表推导还可以包含条件： 12345array = [3,2,1,5,4]#列表推导法s2 = [x**2 for x in array if x % 2 == 0]print (s2) result: [4, 16]_ (8) List 常用方法1234567891011max(list); min(list) #计算最大、最小元素del(list[2]) list.count(obj) #统计obj的出现次数list.extend(obj) #追加多个值list.index(obj) #查找obj的下标list.insert(index, obj) list.pop()list.remove(obj) #删除第一个list.reverse(obj)list.sort(cmp=None, key=None, reverse=False)list.clear() (二) dictionaries（字典） 类似于Java中的Map，存储键值对。 123dict = &#123;'visual-studio': 'c++', 'pycharm': 'python'&#125;for k, v in dict.items(): print(k, v) result: visual-studio c++_ pycharm python_ (三) sets（集合） 独立不同个体的无序集合。 1234567school = set(('Wuhan University', 'Central China Normal University'))basket = &#123;'apple', 'orange', 'apple', 'pear', 'orange', 'banana'&#125;A - B #差集A | B #并集A &amp; B #交集A ^ B #在A、B中均不存在的 迭代器 迭代器有两个基本的方法：iter() 和 next()。 12345678910import sys #引入sys模块list = [1, 2, 3, 4]it = iter(list) #创建迭代器对象while True: #使用异常处理机制 try: print(next(it), end =' ') except StopIteration: sys.exit() result: 1 2 3 4 (四) tuples（元组） 元组是一个值的有序列表（不可改变）。元组可以在字典中作键，还可以作集合的元素；列表不行。 1tup = ('JiahaoChen', 'YindiLong', 369, 336); 函数和类 函数和类的例子 在自定义类中创建迭代器123456789101112class MyNumbers: def __iter__(self): self.a = 1 return self def __next__(self): if self.a &lt;= 20: #与下面的StopIteration对应 x = self.a self.a += 1 return x else: raise StopIteration #用于标识迭代的完成，防止出现无限循环的情况 生成器：使用yeild实现Fibonacci数列 在调用生成器运行的过程中，每次遇到 yield 时函数会暂停并保存当前所有的运行信息，返回 yield 的值, 并在下一次执行 next() 方法时从当前位置继续运行。调用一个生成器函数，返回的是一个迭代器对象。 1234567891011121314151617import sysdef fibonacci(n): # 生成器函数 - 斐波那契 a, b, counter = 0, 1, 0 while True: if (counter &gt; n): return yield a a, b = b, a + b counter += 1f = fibonacci(10) # f 是一个迭代器，由生成器返回生成while True: try: print(next(f), end=" ") except StopIteration: sys.exit() 参数传递 可更改or不可更改对象 strings, tuples, 和 numbers 是不可更改的对象，而 list,dict 等则是可以修改的对象。 123456789def changeme( mylist ): "修改传入的列表" mylist.append([1,2,3,4]) print ("函数内取值: ", mylist) return mylist = [10,20,30]changeme( mylist )print ("函数外取值: ", mylist) result: 函数内取值: [10, 20, 30, [1, 2, 3, 4]]_ 函数外取值: [10, 20, 30, [1, 2, 3, 4]]_ 不定长参数 加了星号 * 的参数会以元组(tuple)的形式导入，存放所有未命名的变量参数。加了两个星号 ** 的参数会以字典的形式导入。 123456789101112def print1(arg1, *vartuple): "打印任何传入的参数" print(arg1) print(vartuple)def print2(arg1, **vardict): "打印任何传入的参数" print(arg1) print(vardict) print1(70, 60, 50)print2(1, a=2, b=3) result: 70_ (60, 50) 1__ {‘a’: 2, ‘b’: 3}_ 匿名函数与lamada lambda 只是一个表达式，函数体比 def 简单很多。lambda的主体是一个表达式，而不是一个代码块。仅仅能在lambda表达式中封装有限的逻辑进去。lambda 函数拥有自己的命名空间，且不能访问自己参数列表之外或全局命名空间里的参数。 虽然lambda函数看起来只能写一行，却不等同于C或C++的内联函数，后者的目的是调用小函数时不占用栈内存从而增加运行效率。lambda [arg1 [,arg2,…..argn]]:expression 123sum = lambda arg1, arg2: arg1 + arg2print("相加后的值为 : ", sum(10, 20)) result: 30_ 变量作用域 L （Local） 局部作用域E （Enclosing） 闭包函数外的函数中G （Global） 全局作用域B （Built-in） 内建作用域注意：Python 中只有模块（module），类（class）以及函数（def、lambda）才会引入新的作用域，其它的代码块（如 if/elif/else/、try/except、for/while等）是不会引入新的作用域的。global 和 nonlocal关键字。特殊情况： 12345a = 10def test(): a = a + 1 print(a)test() result: __Error修改为： 12345a = 10def test(a): a = a + 1 print(a)test(a) result: 11 关于Numpy 科学计算的核心库要求数组的元素类型必须相同 1import numpy as np Arrays（数组）一个numpy数组是一个由不同数值组成的网格。网格中的数据都是同一种数据类型，可以通过非负整型数的元组来访问。维度的数量被称为数组的阶，数组的大小是一个由整型数构成的元组，可以描述数组不同维度上的大小。 123456x = np.array([[1,2,3],[4,5,6],[7,8,9]])print ("sum: ", np.sum(x))print ("shape: ", np.shape(x))print ("column sum: ", np.sum(x, axis=0))print ("row sum: ", np.sum(x, axis=1))print ("tranpose: ", x.T) &lt;br /&gt;创建数组的方法还有： 12345a = np.full((2,2), 8)b = np.random.random((2,2))c = np.zeros((2,2))d = np.ones((2,2))e = np.eye(2) 注意： python中的切片跟matlab中的有点区别，python的是左闭右开区间。 改变切片中的值，原始数组也随之改变。 a[1, :] 和 a[1:2, :] 不太一样，前者的shape为(n, )，后者的shape为(1, n)。 例如前者为[5,6,7,8]，后者为[[5,6,7,8]]。 整型数组访问12345a = np.array([[1,2], [4,5], [7,8]])#way 1print (a[[0,1,2], [0,1,0]])#way 2print (np.array([a[0,0], a[1,1], a[2,0]])) 整型数组访问的技巧： 123a = np.array([[1,2,3], [4,5,6], [7,8,9], [10,11,12]])b = np.array([0,2,0,1])print (a[np.arange(4), b]) result: [1 6 7 11] 还可以这样： 1a[np.arange(4), b] += 10 布尔型数组访问12345a = np.array([[1,2,3], [4,5,6], [7,8,9], [10,11,12]])bool_idx = (a&gt;2)print (bool_idx)print (a[bool_idx])print (a[a&gt;2]) 数据类型12a = np.array([1,2], dtype = np.int64)print (x.dtype) result: int64 数组计算 操作符12345678910111213141516a = np.array([1,2], [4,5]], dtype = np.float64)b = np.array([3,5], [7,2]], dtype = np.float64)print (x+y)print (np.add(x,y))print (x-y)print (np.subtract(x,y))print (x*y)print (np.multiply(x,y))print (x/y)print (np.divide(x,y))print (np.sqrt(x)) 注意： 点乘和matlab有区别，使用 dot 方法 **v.dot(w)** **np.dot(v, w)** Broadcasting（广播）（未完待续。。。） 关于Scipy 科学计算的函数集合。涵盖：线性代数、数学函数优化、信号处理、特殊数学函数、统计分布等。scikit-learn利用Scipy实现。scipy.sparse给出了稀疏矩阵。 关于Pandas]]></content>
      <categories>
        <category>语言基础</category>
      </categories>
      <tags>
        <tag>标签1</tag>
        <tag>标签2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DenseNet]]></title>
    <url>%2F%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%2FDenseNet%2F</url>
    <content type="text"><![CDATA[DenseNetDensenet可视化 Resnet可以写作下面的式子（其中加号代表 和 对应位置的元素相加）：Desnet则可以写为：可以看出Denset是将前面 到 层输出的所有feature map进行拼接，然后作为第 层的输入。 比较好懂的一份 1. DenseLayer12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061class DenseLayer(nn.Module): """ Dense layer """ def __init__(self, in_channels, expand_factor=4, growth_rate=32): super(DenseLayer, self).__init__() self.in_channels = in_channels self.growth_rate = growth_rate self.bottleneck_size = expand_factor * growth_rate self.conv1x1 = self.get_conv1x1() self.conv3x3 = self.get_conv3x3() def get_conv1x1(self): """ returns a stack of Batch Normalization, ReLU, and 1x1 Convolution layers """ layers = [] layers.append(nn.BatchNorm2d(num_features=self.in_channels)) layers.append(nn.ReLU(inplace=True)) layers.append(nn.Conv2d(in_channels=self.in_channels, out_channels=self.bottleneck_size, kernel_size=1, stride=1, bias=False)) return nn.Sequential(*layers) def get_conv3x3(self): """ returns a stack of Batch Normalization, ReLU, and 3x3 Convolutional layers """ layers = [] layers.append(nn.BatchNorm2d(num_features=self.bottleneck_size)) layers.append(nn.ReLU(inplace=True)) layers.append(nn.Conv2d(in_channels=self.bottleneck_size, out_channels=self.growth_rate, kernel_size=3, stride=1, padding=1, bias=False)) return nn.Sequential(*layers) def forward(self, x): """ feed forward """ y = self.conv1x1(x) y = self.conv3x3(y) y = torch.cat([x, y], 1) return y 2. DenseBlock 1234567891011121314151617181920212223242526272829303132333435363738class DenseBlock(nn.Module): """ Dense block """ def __init__(self, in_channels, num_layers, expand_factor=4, growth_rate=32): super(DenseBlock, self).__init__() self.in_channels = in_channels self.num_layers = num_layers self.expand_factor = expand_factor self.growth_rate = growth_rate self.net = self.get_network() def get_network(self): """ return num_layers dense layers """ layers = [] for i in range(self.num_layers): in_channels = self.in_channels + i * self.growth_rate layers.append(DenseLayer(in_channels=in_channels, expand_factor=self.expand_factor, growth_rate=self.growth_rate)) return nn.Sequential(*layers) def forward(self, x): """ feed forward """ return self.net(x) 3. Transtition1234567891011121314151617181920212223242526272829303132333435class TransitionBlock(nn.Module): """ Transition block """ def __init__(self, in_channels, out_channels): super(TransitionBlock, self).__init__() self.in_channels = in_channels self.out_channels = out_channels self.net = self.get_network() def get_network(self): """ returns the structure of the block """ layers = [] layers.append(nn.BatchNorm2d(num_features=self.in_channels)) layers.append(nn.ReLU(inplace=True)) layers.append(nn.Conv2d(in_channels=self.in_channels, out_channels=self.out_channels, kernel_size=1, stride=1, bias=False)) layers.append(nn.AvgPool2d(kernel_size=2, stride=2)) return nn.Sequential(*layers) def forward(self, x): """ forward pass """ return self.net(x) 4. DenseNet123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104"""different configurations of DenseNet"""configs = &#123; '121': [6, 12, 24, 16], '169': [6, 12, 32, 32], '201': [6, 12, 48, 32], '264': [6, 12, 64, 48]&#125;class DenseNet(nn.Module): """DenseNet Architecture""" def __init__(self, config, channels, class_count, num_features=64, compress_factor=2, expand_factor=4, growth_rate=32): super(DenseNet, self).__init__() self.config = configs[config] self.channels = channels self.class_count = class_count self.num_features = num_features self.compress_factor = compress_factor self.expand_factor = expand_factor self.growth_rate = growth_rate self.conv_net = self.get_conv_network() self.fc_net = self.get_fc_net() self.init_weights() def get_conv_network(self): """ returns the convolutional layers of the network """ layers = [] layers.append(nn.Conv2d(in_channels=self.channels, out_channels=self.num_features, kernel_size=7, stride=2, padding=3, bias=False)) layers.append(nn.BatchNorm2d(num_features=self.num_features)) layers.append(nn.ReLU(inplace=True)) layers.append(nn.MaxPool2d(kernel_size=3, stride=2, padding=1)) for i, num_layers in enumerate(self.config): layers.append(DenseBlock(in_channels=self.num_features, num_layers=num_layers, expand_factor=self.expand_factor, growth_rate=self.growth_rate)) self.num_features += num_layers * self.growth_rate if i != len(self.config) - 1: out_channels = self.num_features // self.compress_factor layers.append(TransitionBlock(in_channels=self.num_features, out_channels=out_channels)) self.num_features = out_channels layers.append(nn.BatchNorm2d(num_features=self.num_features)) layers.append(nn.ReLU(inplace=True)) layers.append(nn.AvgPool2d(kernel_size=7, stride=1)) return nn.Sequential(*layers) def get_fc_net(self): """ returns the fully connected layers of the network """ return nn.Linear(in_features=self.num_features, out_features=self.class_count) def init_weights(self): """ initializes weights for each layer """ for module in self.modules(): if isinstance(module, nn.Conv2d): nn.init.kaiming_normal_(module.weight) elif isinstance(module, nn.BatchNorm2d): nn.init.constant_(module.weight, 1) nn.init.constant_(module.bias, 0) elif isinstance(module, nn.Linear): nn.init.constant_(module.bias, 0) def forward(self, x): """ feed forward """ y = self.conv_net(x) y = y.view(-1, y.size(1) * y.size(2) * y.size(3)) y = self.fc_net(y) return y 手工复现https://blog.csdn.net/zhanghao3389/article/details/85038961https://github.com/bamos/densenet.pytorch/blob/master/train.pyhttps://blog.csdn.net/u012938704/article/details/53468483 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196# ! /usr/bin/env python# -*- coding:utf-8 -*-# @author: Edison Jia-hao-Chen# time: 2019-6-10# email: JiahaoChen@whu.edu.cn# A implementation of DenseNet (Pytorch)import loggingimport torchimport torch.nn as nnfrom torch.nn.modules.batchnorm import _BatchNormimport torch.nn.functional as Ffrom collections import OrderedDictfrom mmcv.runner import load_checkpointfrom mmcv.cnn import constant_init, kaiming_initfrom ..registry import BACKBONESimport torch.utils.checkpoint as cpclass DenseLayer(nn.Sequential): def __init__(self, num_input_features, growth_rate, bn_size, drop_rate): super(DenseLayer, self).__init__() self.add_module('norm1', nn.BatchNorm2d(num_input_features)), self.add_module('relu1', nn.ReLU(inplace=True)), self.add_module('conv1', nn.Conv2d(num_input_features, bn_size * growth_rate, kernel_size=1, stride=1, bias=False)), self.add_module('norm2', nn.BatchNorm2d(bn_size * growth_rate)), self.add_module('relu2', nn.ReLU(inplace=True)), self.add_module('conv2', nn.Conv2d(bn_size * growth_rate, growth_rate, kernel_size=3, stride=1, padding=1, bias=False)), self.drop_rate = drop_rate def forward(self, x): new_features = super(DenseLayer, self).forward(x) if self.drop_rate &gt; 0: new_features = F.dropout(new_features, p=self.drop_rate, training=self.training) return torch.cat([x, new_features], 1)class DenseBlock(nn.Sequential): def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate): super(DenseBlock, self).__init__() for i in range(num_layers): layer = DenseLayer(num_input_features + i * growth_rate, growth_rate, bn_size, drop_rate) self.add_module('denselayer%d' % (i + 1), layer)class Transition(nn.Sequential): def __init__(self, num_input_features, num_output_features): super(Transition, self).__init__() self.add_module('norm', nn.BatchNorm2d(num_input_features)) self.add_module('relu', nn.ReLU(inplace=True)) self.add_module('conv', nn.Conv2d(num_input_features, num_output_features, kernel_size=1, stride=1, bias=False)) self.add_module('pool', nn.AvgPool2d(kernel_size=2, stride=2))# class DenseBlock(nn.Module):# pass# DenseNet By Edison@BACKBONES.register_moduleclass DenseNet_CJH(nn.Module): arch_settings = &#123; 121: (DenseBlock, (6, 12, 24, 16)), 169: (DenseBlock, (6, 12, 32, 32)), 201: (DenseBlock, (6, 12, 48, 32)), 161: (DenseBlock, (6, 12, 36, 24)) &#125; def __init__(self, idx_version=121, # (6, 12, 24, 16) growth_rate=32, num_init_features=64, bn_size=4, drop_rate=0): super(DenseNet_CJH, self).__init__() if idx_version not in self.arch_settings: raise KeyError('invalid depth &#123;&#125; for resnet'.format(idx_version)) # First convolution self.features = nn.Sequential(OrderedDict([ ('conv0', nn.Conv2d(3, num_init_features, kernel_size=7, stride=2, padding=3, bias=False)), ('norm0', nn.BatchNorm2d(num_init_features)), ('relu0', nn.ReLU(inplace=True)), ('pool0', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)), ])) # Each denseblock num_features = num_init_features self.block, stage_blocks = self.arch_settings[idx_version] self.stage_blocks = stage_blocks self.layers_block = [] self.layers_trans = [] for i, num_layers in enumerate(self.stage_blocks): block = DenseBlock(num_layers=num_layers, num_input_features=num_features, bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate) # self.features.add_module('denseblock%d' % (i + 1), block) layer_block_name = 'denseblock%d' % (i + 1) self.add_module(layer_block_name, block) self.layers_block.append(layer_block_name) num_features = num_features + num_layers * growth_rate if i != len(self.stage_blocks) - 1 : # -1 trans = Transition(num_input_features=num_features, num_output_features=num_features // 2) # self.features.add_module('transition%d' % (i + 1), trans) layer_trans_name = 'transition%d' % (i + 1) self.add_module(layer_trans_name, trans) self.layers_trans.append(layer_trans_name) num_features = num_features // 2 # Final batch norm # self.features.add_module('norm5', nn.BatchNorm2d(num_features)) # # Linear layer # self.classifier = nn.Linear(num_features, num_classes) self.conv_final = nn.Conv2d(num_features, num_features * 2, kernel_size=1, stride=1, bias=False) # Official init from torch repo. for m in self.modules(): if isinstance(m, nn.Conv2d): nn.init.kaiming_normal(m.weight.data) elif isinstance(m, nn.BatchNorm2d): m.weight.data.fill_(1) m.bias.data.zero_() elif isinstance(m, nn.Linear): m.bias.data.zero_() def forward(self, x): x = self.features(x) # out = F.relu(features, inplace=True) # out = F.avg_pool2d(out, kernel_size=7, stride=1).view(features.size(0), -1) # out = self.classifier(out) # return out out = [] for idx, layer_block_name in enumerate(self.layers_block): block_layer = getattr(self, layer_block_name) x = block_layer(x) if idx in range(3): trans_layer = getattr(self, self.layers_trans[idx]) out.append(x) # print((x.size())) x = trans_layer(x) # print('='* 100) # print('trans\n\n') # print('='* 100) x = self.conv_final(x) out.append(x) # print((x.size())) # print('='* 100) # raise Exception('fuck out[&#123;&#125;]'.format(len(out))) return tuple(out) def init_weights(self, pretrained=None): if isinstance(pretrained, str): logger = logging.getLogger() load_checkpoint(self, pretrained, strict=False, logger=logger) elif pretrained is None: for m in self.modules(): if isinstance(m, nn.Conv2d): kaiming_init(m) elif isinstance(m, (_BatchNorm, nn.GroupNorm)): constant_init(m, 1) # if self.dcn is not None: # # # raise Exception('Dcn!!!!!!: &#123;&#125;'.format(dcn)) # for m in self.modules(): # if isinstance(m, Bottleneck) and hasattr( # m, 'conv2_offset'): # constant_init(m.conv2_offset, 0) # if self.zero_init_residual: # for m in self.modules(): # if isinstance(m, Bottleneck): # constant_init(m.norm3, 0) # elif isinstance(m, BasicBlock): # constant_init(m.norm2, 0) else: raise TypeError('pretrained must be a str or None') torch的小tricks上面两种定义方式得到CNN功能都是相同的，至于喜欢哪一种方式，是个人口味问题，但PyTorch官方推荐：具有学习参数的（例如，conv2d, linear, batch_norm)采用nn.Xxx方式，没有学习参数的（例如，maxpool, loss func, activation func）等根据个人选择使用nn.functional.xxx或者nn.Xxx方式。但关于dropout，个人强烈推荐使用nn.Xxx方式，因为一般情况下只有训练阶段才进行dropout，在eval阶段都不会进行dropout。使用nn.Xxx方式定义dropout，在调用model.eval()之后，model中所有的dropout layer都关闭，但以nn.function.dropout方式定义dropout，在调用model.eval()之后并不能关闭dropout。 maskrcnn的结构 (1) backBone(ResNet, DenseNet) (2) neck(FPN) (3) rpn_head (4) bbox_roi_extractor(SingleRoIExtractor) (5) bbox_head(SharedFCBBoxHead) (6) mask_roi_extractor(SingleRoIExtractor) (7) mask_head(FCNMaskHead)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594MaskRCNN( (backbone): DenseNet_CJH( (features): Sequential( (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu0): ReLU(inplace) (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) ) (denseblock1): DenseBlock( (denselayer1): DenseLayer( (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) (denselayer2): DenseLayer( (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) (denselayer3): DenseLayer( (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) (denselayer4): DenseLayer( (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) (denselayer5): DenseLayer( (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) (denselayer6): DenseLayer( (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) ) (transition1): Transition( (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace) (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (pool): AvgPool2d(kernel_size=2, stride=2, padding=0) ) (denseblock2): DenseBlock( (denselayer1): DenseLayer( (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) (denselayer2): DenseLayer( (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) (denselayer3): DenseLayer( (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) (denselayer4): DenseLayer( (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) (denselayer5): DenseLayer( (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) (denselayer6): DenseLayer( (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) (denselayer7): DenseLayer( (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) (denselayer8): DenseLayer( (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) (denselayer9): DenseLayer( (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) (denselayer10): DenseLayer( (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) (denselayer11): DenseLayer( (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) (denselayer12): DenseLayer( (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) ) (transition2): Transition( (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace) (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (pool): AvgPool2d(kernel_size=2, stride=2, padding=0) ) (denseblock3): DenseBlock( (denselayer1): DenseLayer( (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) (denselayer2): DenseLayer( (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) (denselayer3): DenseLayer( (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) (denselayer4): DenseLayer( (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) (denselayer5): DenseLayer( (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) (denselayer6): DenseLayer( (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) (denselayer7): DenseLayer( (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) (denselayer8): DenseLayer( (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) (denselayer9): DenseLayer( (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) (denselayer10): DenseLayer( (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) (denselayer11): DenseLayer( (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) (denselayer12): DenseLayer( (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) (denselayer13): DenseLayer( (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) (denselayer14): DenseLayer( (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) (denselayer15): DenseLayer( (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) (denselayer16): DenseLayer( (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) (denselayer17): DenseLayer( (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) (denselayer18): DenseLayer( (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) (denselayer19): DenseLayer( (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) (denselayer20): DenseLayer( (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) (denselayer21): DenseLayer( (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) (denselayer22): DenseLayer( (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) (denselayer23): DenseLayer( (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) (denselayer24): DenseLayer( (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) ) (transition3): Transition( (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace) (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (pool): AvgPool2d(kernel_size=2, stride=2, padding=0) ) (denseblock4): DenseBlock( (denselayer1): DenseLayer( (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) (denselayer2): DenseLayer( (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) (denselayer3): DenseLayer( (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) (denselayer4): DenseLayer( (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) (denselayer5): DenseLayer( (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) (denselayer6): DenseLayer( (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) (denselayer7): DenseLayer( (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) (denselayer8): DenseLayer( (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) (denselayer9): DenseLayer( (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) (denselayer10): DenseLayer( (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) (denselayer11): DenseLayer( (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) (denselayer12): DenseLayer( (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) (denselayer13): DenseLayer( (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) (denselayer14): DenseLayer( (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) (denselayer15): DenseLayer( (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) (denselayer16): DenseLayer( (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) ) (conv_final): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False) ) (neck): FPN( (lateral_convs): ModuleList( (0): ConvModule( (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1)) ) (1): ConvModule( (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1)) ) (2): ConvModule( (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1)) ) (3): ConvModule( (conv): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1)) ) ) (fpn_convs): ModuleList( (0): ConvModule( (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) ) (1): ConvModule( (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) ) (2): ConvModule( (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) ) (3): ConvModule( (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) ) ) ) (rpn_head): RPNHead( (loss_cls): CrossEntropyLoss() (loss_bbox): SmoothL1Loss() (rpn_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (rpn_cls): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1)) (rpn_reg): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1)) ) (bbox_roi_extractor): SingleRoIExtractor( (roi_layers): ModuleList( (0): RoIAlign() (1): RoIAlign() (2): RoIAlign() (3): RoIAlign() ) ) (bbox_head): SharedFCBBoxHead( (loss_cls): CrossEntropyLoss() (loss_bbox): SmoothL1Loss() (fc_cls): Linear(in_features=1024, out_features=2, bias=True) (fc_reg): Linear(in_features=1024, out_features=8, bias=True) (shared_convs): ModuleList() (shared_fcs): ModuleList( (0): Linear(in_features=12544, out_features=1024, bias=True) (1): Linear(in_features=1024, out_features=1024, bias=True) ) (cls_convs): ModuleList() (cls_fcs): ModuleList() (reg_convs): ModuleList() (reg_fcs): ModuleList() (relu): ReLU(inplace) ) (mask_roi_extractor): SingleRoIExtractor( (roi_layers): ModuleList( (0): RoIAlign() (1): RoIAlign() (2): RoIAlign() (3): RoIAlign() ) ) (mask_head): FCNMaskHead( (loss_mask): CrossEntropyLoss() (convs): ModuleList( (0): ConvModule( (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (activate): ReLU(inplace) ) (1): ConvModule( (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (activate): ReLU(inplace) ) (2): ConvModule( (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (activate): ReLU(inplace) ) (3): ConvModule( (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (activate): ReLU(inplace) ) ) (upsample): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2)) (conv_logits): Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1)) (relu): ReLU(inplace) ))]]></content>
      <categories>
        <category>经典网络结构</category>
      </categories>
      <tags>
        <tag>标签1</tag>
        <tag>标签2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fast RCNN]]></title>
    <url>%2F%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%2FFast%20RCNN%2F</url>
    <content type="text"><![CDATA[Fast RCNN 步骤 输入图片。 输入到卷积网络中，它生成感兴趣区域。 利用Rol池化层对这些区域重新调整，将其输入到完全连接网络中。 在网络的顶层用softmax层输出类别。同样使用一个线性回归层，输出相对应的边界框。 图解我们还用上面的图像作为案例，进行更直观的讲解。 首先，输入图像： 图像被传递到卷积网络中，返回感兴趣区域： 之后，在区域上应用Rol池化层，保证每个区域的尺寸相同：最后，这些区域被传递到一个完全连接的网络中进行分类，并用softmax和线性回归层同时返回边界框：3.2 Fast RCNN的问题但是即使这样，Fast RCNN也有某些局限性。它同样用的是选择性搜索作为寻找感兴趣区域的，这一过程通常较慢。与RCNN不同的是，Fast RCNN处理一张图片大约需要2秒。但是在大型真实数据集上，这种速度仍然不够理想。 详解Fast R-CNN是要解决R-CNN和SPP-net两千个左右候选框带来的重复计算问题，其主要思想为： 使用一个简化的SPP层 —— RoI（Region of Interesting） Pooling层，操作与SPP类似； 训练和测试是不再分多步：不再需要额外的硬盘来存储中间层的特征，梯度能够通过RoI Pooling层直接传播；此外，分类和回归用Multi-task的方式一起进行； SVD：使用SVD分解全连接层的参数矩阵，压缩为两个规模小很多的全连接层。 如图9所示，Fast R-CNN的主要步骤如下： 特征提取：以整张图片为输入利用CNN得到图片的特征层； 候选区域：通过Selective Search等方法从原始图片提取区域候选框，并把这些候选框一一投影到最后的特征层； 区域归一化：针对特征层上的每个区域候选框进行RoI Pooling操作，得到固定大小的特征表示； 分类与回归：然后再通过两个全连接层，分别用softmax多分类做目标识别，用回归模型进行边框位置与大小微调。Fast R-CNN比R-CNN的训练速度（大模型L）快8.8倍，测试时间快213倍，比SPP-net训练速度快2.6倍，测试速度快10倍左右。]]></content>
      <categories>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>Two Stage</tag>
        <tag>RCNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AlexNet 实现]]></title>
    <url>%2F%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%2FAlexNet%20%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[前言 本文使用 torch 框架实现了 AlexNet仅供学习使用，无任何商业用途。如有错误的地方，欢迎读者给予批评。 作者：Alex（Hiton的学生） 时间：2012年 网络输入大小 原论文：2242243 caffee/tensorflow：2272273（常用） 网络结构 5个conv，3个pool，3个fc 成就 2012年ILSVRC第一名，Top-5错误率15.3%，碾压第二名（SVM）的性能 2012年首次在CNN中应用了ReLu、Dropout层，使得CNN成为在图像分类上的核心算法模型 AlexNet取得效果好几个原因： 使用ReLU作为激活函数 局部正则化 Dropout Data Augmentation(数据增强) AlexNet 的形状 网络结构参数 综述 卷积层：5层 全连接层：3层 深度：8层 参数个数：60.3M 神经元个数：650k 分类数目：1000类 注：由于当时的显卡容量问题，AlexNet 的60M个参数无法全部放在一张显卡上操作，所以采用了两张显卡分开操作的形式，其中在C3，R1，R2，R3层上出现交互，所谓的交互就是通道的合并，是一种串接操作。 计算公式 卷积模型：上图中左边的一幅输入图的三个通道，中间是卷积层，尺寸为3x3x3x2（长x宽x输入维度x输出通道），这里就是三维卷积，得到的特征图还是一个通道，有两个三维卷积得到两个featuremap。 全连接模型：类似上图，卷积层尺寸为(输入图像长x输入图像宽x输入维度x输出通道)；注意：卷积核大小和输入图像大小一致 CNN网络中一般卷积层学习参数量占比小，计算量占比大，而全连接中学习参数量占比大，计算量占比小。 1.输出size 假定输入维度是nn，卷积核维度kk，stride是s 当padding = ‘VALID’时(不填充，舍弃)，N = (n-k)/s+1 (向上取整) pad_width = 0（边缘不填充） 当padding = ‘SAME’时(自动填充)，N = n/k (向上取整) ， pad_width = (dim_out - 1) * s + k -n ， pad_left = pad_width / 2 ， pad_right = pad_width - pad_left pytorch最新计算方法： N = (W − K + 2P )/S+1 (W:输入维度 K:卷积核大小kernel_size P:padding S:stride) 2.学习参数个数 param = h w c_in * c_out + c_out （卷积核的宽，高，输入通道数，输出通道数，bias偏移个数） 3.连接个数：每个的学习参数与输出图像每个像素之间的连接数 LinkNum = ( h w c_in * c_out + c_out ) x 输出图像长 x 输出图像宽 （卷积核的宽 x 卷积核的高 x 输入通道数 x 输出通道数 + bias偏移个数) x 输出图像长 x 输出图像宽技术点 概述 使用Relu AlexNet使用ReLU代替了Sigmoid作为CNN的激活函数，效果在深度的网络上超过了sigmoid，同时解决sigmoid在训练较深的网络中出现的梯度消失（梯度弥散）的问题 基于ReLU的深度卷积网络比基于tanh和sigmoid的网络训练快数倍 多个GPU 训练 由于硬件资源有限，训练AlexNet时卷积操作不能全部放在同一个GPU中处理，因此作者把feature maps分给两个GPU分别进行处理，最后把两个GPU的结果在C3和R1层进行融合 group conv可以降低参数量，提高计算速度 重叠的pool池化 （之前主要用平均池化）避免平均池化模糊效果 步长比池化的核的尺寸小，这样池化层的结果之间有重叠，提升了特征的丰富性 论文中提到，使用这种池化可以一定程度上减少过拟合现象 Dropout AlexNet在最后几个全连接层使用了dropout，避免过拟合 数据增强 随机中256_256的原始图像中截取224_224大小的区域（以及水平翻转镜像），避免过拟合 局部响应归一化LRN 增强模型的泛化能力：对局部神经元创建了竞争的机制，使得其中响应大的值变得更大，并抑制反馈较小的。 论文Very Deep Convolution Networks for Large-Scale image Recognition(VGG网络的文章)中证明。LRN对CNN没有什么作用，反而增加了计算复杂度，因此，这一技术也不再使用了 详细技术内容 减少过度拟合方法 数据增强：通常会从现有数据中生成额外的数据。 通过镜像实现数据增强。如果训练集中有一只猫的图像，那么它的镜像也是一只猫。训练数据集可变为原来的两倍。 通过随机裁剪实现数据增强，即原始数据的移位版本。AlexNet的作者从大小为256×256的图像中随机裁剪出大小为227×227的图像，作为网络的输入。使用这种方法将数据的大小变为原来的2048倍。如下图，这四个随机裁剪的图像相似但不相同，对神经网络来说，都是一只猫。 改变原图的RGB的强弱来增加样本总数。及时图片颜色变化或亮度变化，不会影响预测效果。 Dropout 在对6000多万个参数的训练中，作者采用了Dropout的技术来减少过拟合。在丢弃过程中，一个神经元被从网络中丢弃的概率为0.5，当一个神经元被丢弃时，并不影响正向传播和反向传播 ,因此，所学习的权重参数更可靠。Dropout层使迭代次数减少，如果没有使用丢弃技术，AlexNet的过拟合会更加严重。 重叠最大池化 通常使用最大池化层来对张量的宽度和高度进行采样，且保持深度不变。重叠的最大池层与最大池层类似，除了重叠最大池层的相邻窗口是相互重叠的。作者使用的池化窗口是大小为3×3，相邻窗口步幅为2的窗口。在输出尺寸相同的情况下，与使用大小为2×2，相邻窗口步幅为2的非重叠池化窗口相比，重叠池化窗口能够分别将第一名的错误率降低0.4%，第5名的错误率降低0.3%。 ReLU非线性 AlexNet的另一个重要特性就是使用了ReLU激活函数。优点：1、与使用tanh相比ReLUs能以6倍快的速度到达25%的训练错误率（CIFAR-10数据集）。2、解决了sigmoid在网络层次较深时的梯度弥散问题。 ReLU函数： f(x)= max(0,x) 注：梯度弥散：在梯度下降过程中，随着算法反向反馈到前面几层，梯度会越来越小，最终会在还没有找到最优解时就收敛。并且深度学习遭受不稳定梯度，不同层学习在不同的速度上，后面几层变化大，前面几层变化小（甚至基本没有变化）。 激活函数介绍、梯度消失及梯度弥散可参考链接： http://www.360doc.com/content/17/1102/21/1489589_700400500.shtml 局部响应归一化（LRN） 对局部神经元创建了竞争的机制，使得其中响应较大的值变得更大，并抑制反馈较小的（强者更强，弱者更弱）。 需要学习两个参数k,β 但人们对LRN层的评价都不高，下面是一个参考链接： https://blog.csdn.net/searobbers_duck/article/details/51645941 代码详解 头文件123456import torch import torch.nn as nn # from torch.autograd import Variable # 自动求导from torch.utils.data import DataLoader # 加载数据集的工具import torchvision # 处理图像的包import torch.nn.functional as F # 激活函数 网络定义123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354class AlexNet(nn.Module): def __init__(self, in_channel, n_class): super(AlexNet, self).__init__() self.conv1 = nn.Sequential( nn.Conv2d(in_channel, 96, kernel_size=11, stride=4), # 96个[11*11*3]卷积核 nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), ) self.conv2 = nn.Sequential( # p = ((W_output - 1) * s + k - W_input) / 2 nn.Conv2d(96, 256, kernel_size=5, padding=2), # 256个[5*5*48]卷积核 nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), ) self.conv3 = nn.Sequential( nn.Conv2d(256, 384, kernel_size=3, padding=1), # 384个[3*3*]卷积核 nn.ReLU(inplace=True), nn.Conv2d(384, 384, kernel_size=3, padding=1), # 384个[3*3*192]卷积核 nn.ReLU(inplace=True), nn.Conv2d(384, 256, kernel_size=3, padding=1), # 256个[3*3*192]卷积核 nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), ) self.fc4 = nn.Sequential( nn.Linear(1*1*256, 4096), nn.ReLU(inplace=True), nn.Dropout(0.5), # 防止过拟合，丢掉 p=0.5 的 nn.Linear(4096, 4096), nn.ReLU(inplace=True), nn.Dropout(0.5), # 防止过拟合，丢掉 p=0.5 的 nn.Linear(4096, n_class) ) def forward(self, x): con1_x = self.conv1(x) con2_x = self.conv2(con1_x) con3_x = self.conv3(con2_x) # 注意：卷积层到全连接层需要经过变换 lin_x = con3_x.view(con3_x.size(0), -1) # 用 reshape 也可以？注意和 reshape 的区别！ out = self.fc4(lin_x) return out 输出 AlexNet 的形状 AlexNet( (conv1): Sequential( (0): Conv2d(3, 96, kernel_size=(11, 11), stride=(4, 4)) (1): ReLU(inplace) (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) ) (conv2): Sequential( (0): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)) (1): ReLU(inplace) (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) ) (conv3): Sequential( (0): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU(inplace) (2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (3): ReLU(inplace) (4): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (5): ReLU(inplace) (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) ) (fc4): Sequential( (0): Linear(in_features=256, out_features=4096, bias=True) (1): ReLU(inplace) (2): Dropout(p=0.5) (3): Linear(in_features=4096, out_features =4096, bias=True) (4): ReLU(inplace) (5): Dropout(p=0.5) (6): Linear(in_features=4096, out_features=1000, bias=True) )) 图像预处理 AlexNet 使用的是 227227 的图片而本文使用的 CIFAR-10 数据集中已有 3232 的图片 12345transform = torchvision.transforms.Compose([ torchvision.transforms.Resize(96), torchvision.transforms.ToTensor(), # 将数据转化成 tensor torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # 归一化。参数为数据的平均差、标准差]) 定义超参数 12345use_gpu = torch.cuda.is_available()EPOCH = 3learning_rate = 1e-3BATCH_SIZE = 256DOWNLOAD = True 加载数据集12345678# Download Datasettrain_dataset = torchvision.datasets.CIFAR10(root='./data/', train=True, transform=transform, download=DOWNLOAD)test_dataset = torchvision.datasets.CIFAR10(root='./data/', train=False, transform=transform)# Data Loader# DataLoader对象，可以对数据洗牌、批处理、多处理来加载数据train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False) 定义损失函数和优化函数12345# 定义损失函数和优化函数# CrossEntropyLoss 包括了 cross entropy 和 softmax activationlose_fn = nn.CrossEntropyLoss()# nn.parameters() 可以追踪所有 CNN 中需要训练的模型参数optimizer = torch.optim.Adam(alexnet.parameters(), lr=learning_rate) 使用 GPU123456789# 转换成 variable 类型，在torch 0.4.0 版本中，已舍弃此用法def to_var(x): x = Variable(x) if use_gpu: x = x.cuda() return x if use_gpu: alexnet = alexnet.cuda() 开始训练12345678910111213141516171819202122232425262728293031323334353637383940414243444546# 设置模型为训练模式alexnet.train()for epoch in range(EPOCH): total = 0 correct = 0 # 遍历训练数据 (images, labels) for batch_idx, (images, labels) in enumerate(train_loader): images, labels = to_var(images), to_var(labels) # 前向传播 optimizer.zero_grad() # 清零梯度 outputs = alexnet(images) # 会自动调用 model.forward(images) 函数 # 计算损失 loss = lose_fn(outputs, labels) loss_list.append(loss.item()) # 反向传播 loss.backward() # 更新梯度 optimizer.step() if (batch_idx + 1) % 100 == 0: print('Epoch[&#123;&#125;/&#123;&#125;], step[&#123;&#125;,&#123;&#125;], Loss:&#123;:.4f&#125;, Acc:&#123;:.2f&#125;%' .format(epoch+1, EPOCH, batch_idx+1, total_step, loss.item(), (correct/total)*100), end=' -- ') print('[Time: &#123;:&#125;]'.format(time.strftime('%H:%M:%S', time.localtime()))) # 记录精度 total += labels.size(0) # torch.max(x, 1) 为按行取最大值 # output 的每一行的最大值，存放在 _ 中； 索引存放在 predicted 中 # output 的每一行每一个元素表示的是： 这一类的概率 _, predicted = torch.max(outputs.data, 1) # sum 计算出 predicted 和 label 相同元素的个数，返回一个张量，.item()方法得到他的数值 correct += (predicted == labels).sum().item() # #############去掉 acc_list.append(correct / total) print('======================= 【Training Acc: &#123;:&#125;%】====================='.format(100 * correct / total), end='') print('[Time: &#123;:&#125;]'.format(time.strftime('%H:%M:%S', time.localtime()))) 开始测试123456789101112131415161718# 将模型设置成评估模式# 禁用 dropout 或者 batch normalization 层alexnet.eval()# 禁用 autograd 功能，加速计算with torch.no_grad(): correct = 0 total = 0 for images, labels in test_loader: images, labels = to_var(images), to_var(labels) outputs = alexnet(images) _, predicted = torch.max(outputs, 1) total += labels.size(0) temp = (predicted == labels.data).sum() correct += temp print('======================= 【Testing Acc: &#123;:&#125;%】====================='.format(100 * correct / total), end='') print('[Time: &#123;:&#125;]'.format(time.strftime('%H:%M:%S', time.localtime()))) 保存模型12torch.save(alexnet.state_dict(), 'D:/Workspace/ModelsPath/' + 'Alex_net_model.ckpt')print('Model Test and Save Succeed. ') 结果截图 笔记 nn.Dropout(0.5) 按照一定概率丢掉一些连接 优化函数 SGD, Adam, Momentum, AdaGrad, RMSProp 损失函数 CrossEntropy(包括了softmax activation), MSE nn.Sequential() 将多个有序的模块组合在一起 nn.Conv2d() 二维卷积，参数有：in_channels, out_channels, kernel_size, stride, padding(p = ((W_output - 1) * s + k - W_input) / 2) nn.MaxPool2d() 最大池化函数，参数有：kernel_size, stride, padding torchvision.transforms.Compose() 将某种tansform有序的组合到list中 待完善1）torch 0.3.0 和 0.4.0 的对比2）参数初始化的作用 3）transforms中，图片预处理先调尺寸4）torch.view() 和 reshape() 的区别和用法]]></content>
      <categories>
        <category>经典网络结构</category>
      </categories>
      <tags>
        <tag>标签1</tag>
        <tag>标签2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Faster RCNN]]></title>
    <url>%2F%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%2FFaster%20RCNN%2F</url>
    <content type="text"><![CDATA[Faster RCNN知乎: 一文弄懂 Faster RCNN知乎: rpn的作用CSDN: 感受野可以清晰的看到该网络对于一副任意大小PxQ的图像，首先缩放至固定大小MxN，然后将MxN图像送入网络；而Convlayers中包含了13个conv层+13个relu层+4个pooling层；RPN网络首先经过3x3卷积，再分别生成foregroundanchors与bounding box regression偏移量，然后计算出proposals；而RoiPooling层则利用proposals从feature maps中提取proposalfeature送入后续全连接和softmax网络作classification（即分类proposal到底是什么object） RPN经典的检测方法生成检测框都非常耗时，如OpenCVadaboost使用滑动窗口+图像金字塔生成检测框；或如R-CNN使用SS(Selective Search)方法生成检测框。而FasterRCNN则抛弃了传统的滑动窗口和SS方法，直接使用RPN生成检测框，这也是Faster R-CNN的巨大优势，能极大提升检测框的生成速度。 可以看到RPN网络实际分为2条线:(1) 上面一条通过softmax分类anchors获得foreground和background（检测目标是foreground）(2) 下面一条用于计算对于anchors的bounding box regression偏移量，以获得精确的proposal。而最后的Proposal层则负责综合foregroundanchors和bounding boxregression偏移量获取proposals，同时剔除太小和超出边界的proposals。其实整个网络到了ProposalLayer这里，就完成了相当于目标定位的功能。 多通道卷积 Anchors ( RPN网络 )提到RPN网络，就不能不说anchors。所谓anchors，实际上就是一组由rpn/generate_anchors.py生成的矩形。其中每行的4个值 表矩形左上和右下角点坐标。9个矩形共有3种形状，长宽比为大约为 三种，如图6。实际上通过anchors就引入了检测中常用到的多尺度方法。 Region Proposal有什么作用？作用是代替以往rcnn使用的selective search的方法寻找图片里面可能存在物体的区域。当一张图片输入resnet或者vgg，在最后一层的feature map上面，寻找可能出现物体的位置，这时候分别以这张feature map的每一个点为中心，在原图上画出9个尺寸不一anchor。然后计算anchor与GT（ground truth） box的iou（重叠率），满足一定iou条件的anchor，便认为是这个anchor包含了某个物体 1、COCO数据集上总共只有80类物体，如果不进行Region Proposal，即网络最后的classification是对所有anchor框定的Region进行识别分类，会严重拖累网络的分类性能，难以收敛。原因在于，存在过多的不包含任何有用的类别（80类之外的，例如各种各样的天空、草地、水泥墙、玻璃反射等等）的Region输入分类网络，而这些无用的Region占了所有Region的很大比例。换句话说，这些Region数量庞大，却并不能为softmax分类器带来有用的性能提升（因为无论怎么预测，其类别都是背景，对于主体的80类没有贡献）。2、大量无用的Region都需要单独进入分类网络，而分类网络由几层卷积层和最后一层全连接层组成，参数众多，十分耗费计算时间，Faster R-CNN本来就不能做到实时，这下更慢了。 最后有个小小的说明，针对不了解Anchor的同学们，我在文中始终在说对于感兴趣的区域“框定一个坐标”，这是为了便于理解，其实这样说是不准确的。具体就是：我们整张图像上，所有的框，一开始就由Anchor和网络结构确定了，这些框都有各自初始的坐标（锚点）。所有后续的工作，RPN提取前景和背景，其实就是保留包含前景的框，丢掉包含背景的；包括后续的NMS，也都是丢掉多余的，并非重新新建一个框。我们网络输出的两个Bounding-box regression，都是输出的坐标偏移量，也就是在初始锚点的基础上做的偏移修正和缩放，并非输出一个原图上的绝对坐标。 yolo有类似rpn的机制，那就是最后输出时的confidence值，这个值决定了前景和背景。ssd是将anchor机制融合在了1 stage模型中，原理与本文所述基本一致。 目标检测的思想是，首先在图片中寻找“可能存在物体的位置（regions）”，然后再判断“这个位置里面的物体是什么东西”，所以region proposal就参与了判断物体可能存在位置的过程。region proposal是让模型学会去看哪里有物体，GT box就是给它进行参考，告诉它是不是看错了，该往哪些地方看才对。 对于每个3x3的窗口，作者就计算这个滑动窗口的中心点所对应的原始图片的中心点。然后作者假定，这个3x3窗口，是从原始图片上通过SPP池化得到的，而这个池化的区域的面积以及比例，就是一个个的anchor。换句话说，对于每个3x3窗口，作者假定它来自9种不同原始区域的池化，但是这些池化在原始图片中的中心点，都完全一样。这个中心点，就是刚才提到的，3x3窗口中心点所对应的原始图片中的中心点。如此一来，在每个窗口位置，我们都可以根据9个不同长宽比例、不同面积的anchor，逆向推导出它所对应的原始图片中的一个区域，这个区域的尺寸以及坐标，都是已知的。而这个区域，就是我们想要的 proposal。所以我们通过滑动窗口和anchor，成功得到了 51x39x9 个原始图片的proposal。接下来，每个proposal我们只输出6个参数：每个 proposal 和 ground truth 进行比较得到的前景概率和背景概率(2个参数）（对应图上的 cls_score）；由于每个 proposal 和 ground truth 位置及尺寸上的差异，从 proposal 通过平移放缩得到 ground truth 需要的4个平移放缩参数（对应图上的 bbox_pred）。]]></content>
      <categories>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>Two Stage</tag>
        <tag>RCNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RCNN]]></title>
    <url>%2F%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%2FRCNN%2F</url>
    <content type="text"><![CDATA[RCNN 简介和在大量区域上工作不同，RCNN算法提出在图像中创建多个边界框，检查这些边框中是否含有目标物体。RCNN使用选择性搜索来从一张图片中提取这些边框。首先，让我们明确什么是选择性搜索，以及它是如何辨别不同区域的。组成目标物体通常有四个要素： 变化尺度 颜色 结构（材质） 所占面积 选择性搜索会确定物体在图片中的这些特征，然后基于这些特征突出不同区域。 图解下面是选择搜索的一个简单案例： 首先将一张图片作为输入： 之后，它会生成最初的sub-分割，将图片分成多个区域： 基于颜色、结构、尺寸、形状，将相似的区域合并成更大的区域： 最后，生成最终的目标物体位置（Region of Interest）。 首先，将以下图片作为输入： 之后，我们会用上文中的选择性搜索得到感兴趣区域： 将这些区域输入到CNN中，并经过卷积网络： CNN为每个区域提取特征，利用SVM将这些区域分成不同类别： 最后，用边界框回归预测每个区域的边界框位置： 这就是RCNN检测目标物体的方法。 过程 我们首先取一个预训练卷积神经网络。 根据需要检测的目标类别数量，训练网络的最后一层。 得到每张图片的感兴趣区域（Region of Interest），对这些区域重新改造，以让其符合CNN的输入尺寸要求。 得到这些区域后，我们训练支持向量机（SVM）来辨别目标物体和背景。对每个类别，我们都要训练一个二元SVM。 最后，我们训练一个线性回归模型，为每个辨识到的物体生成更精确的边界框。 详解以下是R-CNN的主要步骤： 候选区域：通过Selective Search从原始图片提取2000个左右区域候选框； 区域大小归一化：把所有侯选框缩放成固定大小（原文采用227×227）； 特征提取：通过CNN网络，提取特征； 分类与回归：在特征层的基础上添加两个全连接层，再用SVM分类来做识别，用线性回归来微调边框位置与大小，其中每个类别单独训练一个边框回归器。 其中目标检测系统的结构如图6所示，注意，图中的第2步对应步骤中的1、2步，即包括候选区域和区域大小归一化。Overfeat可以看做是R-CNN的一个特殊情况，只需要把Selective Search换成多尺度的滑动窗口，每个类别的边框回归器换成统一的边框回归器，SVM换为多层网络即可。但是Overfeat实际比R-CNN快9倍，这主要得益于卷积相关的共享计算。事实上，R-CNN有很多缺点： 重复计算：R-CNN虽然不再是穷举，但依然有两千个左右的候选框，这些候选框都需要进行CNN操作，计算量依然很大，其中有不少其实是重复计算； SVM模型：而且还是线性模型，在标注数据不缺的时候显然不是最好的选择； 训练测试分为多步：候选区域、特征提取、分类、回归都是断开的训练的过程，中间数据还需要单独保存； 训练的空间和时间代价很高：卷积出来的特征需要先存在硬盘上，这些特征需要几百G的存储空间； 慢：前面的缺点最终导致R-CNN出奇的慢，GPU上处理一张图片需要13秒，CPU上则需要53秒[2]。 当然，R-CNN这次是冲着效果来的，其中ILSVRC 2013数据集上的mAP由Overfeat的24.3%提升到了31.4%，第一次有了质的改变。]]></content>
      <categories>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>Two Stage</tag>
        <tag>RCNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R-FCN]]></title>
    <url>%2F%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%2FR-FCN%2F</url>
    <content type="text"><![CDATA[R-FCN前面的目标检测方法都可以细分为两个子网络： 共享的全卷积网络； 不共享计算的ROI相关的子网络（比如全连接网络）。 R-FCN则将最后的全连接层之类换为了一个位置敏感的的卷积网络，从而让所有计算都可以共享。具体来说，先把每个候选区域划分为_k_×_k_个网格，比如R-FCN原文中_k_的取值为3，则对应的九个网格分别表示：左上top-left，上中top-center，……，右下bottom-right，对应图12中的九宫格及图13中的不同颜色的块，每个Grid都有对应的编码，但预测时候会有_C_+1个输出，_C_表示类别数目，+1是因为有背景类别，全部的输出通道数量为_k_×_k_×(_C_+1)。图13 R-FCN需要注意的是，图12、13中不同位置都存在一个九宫格，但是Pooling时候只有一个起作用，比如bottom-right层只有右下角的小块起作用。那么问题来了，这一层其他的8个框有什么作用呢？答案是它们可以作为其他ROI（偏左或偏上一些的ROI）的右下角。R-FCN的步骤为： 候选区域：使用RPN（Region Proposal Network，候选区域网络），RPN本身是全卷积网络结构； 分类与回归：利用和RPN共享的特征进行分类。当做bbox回归时，则将_C_设置为4。]]></content>
      <categories>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ResNet]]></title>
    <url>%2F%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%2FResNet%2F</url>
    <content type="text"><![CDATA[ResNet作者由VGG19设计出了plain 网络和残差网络，如下图中部和右侧网络。然后利用这两种网络进行实验对比。 设计网络的规则： 对于输出feature map大小相同的层，有相同数量的filters，即channel数相同； 当feature map大小减半时（池化），filters数量翻倍。 对于残差网络，维度匹配的shortcut连接为实线，反之为虚线。维度不匹配时，同等映射有两种可选方案： a) 直接通过zero padding 来增加维度（channel）。 b) 乘以W矩阵投影到新的空间。实现是用1x1卷积实现的，直接改变1x1卷积的filters数目。这种会增加参数。 resnet-101 结构示意:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321\\\\ 0-Sequential( (0): Bottleneck( (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace) (downsample): Sequential( (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Bottleneck( (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace) ) (2): Bottleneck( (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace) ))\\\\ 1-Sequential( (0): Bottleneck( (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace) (downsample): Sequential( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Bottleneck( (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace) ) (2): Bottleneck( (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace) ) (3): Bottleneck( (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace) ))\\\\ 2-Sequential( (0): Bottleneck( (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace) (downsample): Sequential( (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace) ) (2): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace) ) (3): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace) ) (4): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace) ) (5): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace) ) (6): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace) ) (7): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace) ) (8): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace) ) (9): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace) ) (10): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace) ) (11): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace) ) (12): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace) ) (13): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace) ) (14): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace) ) (15): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace) ) (16): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace) ) (17): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace) ) (18): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace) ) (19): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace) ) (20): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace) ) (21): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace) ) (22): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace) ))\\\\ 3-Sequential( (0): Bottleneck( (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace) (downsample): Sequential( (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Bottleneck( (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace) ) (2): Bottleneck( (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace) )) resnet代码复现https://www.cnblogs.com/wanghui-garcia/p/10775860.htmlhttps://www.cnblogs.com/Mrzhang3389/p/10127223.html 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305# ! /usr/bin/env python# -*- coding:utf-8 -*-# @author: Edison Jia-hao-Chen# time: 2019-6-13# email: JiahaoChen@whu.edu.cn# A implementation of ResNet (Pytorch)import loggingimport torch.nn as nnfrom torch.nn.modules.batchnorm import _BatchNormfrom mmcv.runner import load_checkpointfrom mmcv.cnn import constant_init, kaiming_initfrom ..registry import BACKBONESimport torch.utils.checkpoint as cpdef conv3x3(in_planes, out_planes, stride=1): return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)class BasicBlock(nn.Module): expansion = 1 def __init__(self, in_channels, out_channels, stride=1, dilation=1, with_cp=False, downsample=None): super(BasicBlock, self).__init__() self.conv1 = conv3x3(in_channels, out_channels, stride) self.bn1 = nn.BatchNorm2d(out_channels) self.relu = nn.ReLU(inplace=True) self.conv2 = conv3x3(out_channels, out_channels) self.bn2 = nn.BatchNorm2d(out_channels) self.downsample = downsample self.stride = stride def forward(self, x): residual = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) if self.downsample is not None: residual = self.downsample(x) out += residual out = self.relu(out) return outclass Bottleneck(nn.Module): expansion = 4 def __init__(self, in_channels, out_channels, stride=1, dilation=1, with_cp=False, downsample=None): super(Bottleneck, self).__init__() self.with_cp = with_cp self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False) self.bn1 = nn.BatchNorm2d(out_channels) self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=dilation, bias=False) self.bn2 = nn.BatchNorm2d(out_channels) self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=1, bias=False) self.bn3 = nn.BatchNorm2d(out_channels * self.expansion) self.relu = nn.ReLU(inplace=True) self.downsample = downsample def forward(self, x): def _inner_forward(x): residual = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) out = self.relu(out) out = self.conv3(out) out = self.bn3(out) if self.downsample is not None: residual = self.downsample(x) out += residual return out if self.with_cp and x.requires_grad: # out = _inner_forward(x) out = cp.checkpoint(_inner_forward, x) else: out = _inner_forward(x) out = _inner_forward(x) # raise Exception('x:&#123;&#125; out:&#123;&#125;'.format(x.size(), out.size())) out = self.relu(out) return out# ResNet By Edison@BACKBONES.register_moduleclass ResNet_CJH(nn.Module): arch_settings = &#123; 18: (BasicBlock, (2, 2, 2, 2)), 34: (BasicBlock, (3, 4, 6, 3)), 50: (Bottleneck, (3, 4, 6, 3)), 101: (Bottleneck, (3, 4, 23, 3)), 152: (Bottleneck, (3, 8, 36, 3)) &#125; def __init__(self, depth, num_stages=4, strides=(1, 2, 2, 2), dilations=(1, 1, 1, 1), style='pytorch', norm_eval=True, frozen_stages=-1, zero_init_residual=True): super(ResNet_CJH, self).__init__() if depth not in self.arch_settings: raise KeyError('invalid depth &#123;&#125; for resnet'.format(depth)) self.depth = depth self.num_stages = num_stages assert 1 &lt;= num_stages &lt;= 4 self.strides = strides self.dilations = dilations assert len(strides) == len(dilations) == num_stages self.style = style self.zero_init_residual = zero_init_residual self.block, stage_blocks = self.arch_settings[depth] self.stage_blocks = stage_blocks[:num_stages] self.in_channels = 64 # ResNet includes: layer_head + layers_res(0, 1, 2, 3) # the first layer: layer_head self.layer_head = nn.Sequential( nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False), nn.BatchNorm2d(64), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2, padding=1) ) # four res layers self.layers_res = [] for i, num_blocks in enumerate(self.stage_blocks): stride = strides[i] dilation = dilations[i] out_channels = 64 * 2**i res_layer = self._make_res_layer( self.block, self.in_channels, out_channels, num_blocks, stride=stride, dilation=dilation ) self.in_channels = out_channels * self.block.expansion layer_name = 'layer&#123;&#125;'.format(i + 1) self.add_module(layer_name, res_layer) self.layers_res.append(layer_name) # self._freeze_stages() # @property # def norm1(self): # return getattr(self, self.norm1_name) # @property # def norm2(self): # return getattr(self, self.norm2_name) # @property # def norm3(self): # return getattr(self, self.norm3_name) # res layer: 0, 1, 2, 3 def _make_res_layer(self, block, in_channels, out_channels, blocks, stride=1, dilation=1, style='pytorch'): downsample = None # print('fuck make layers: stride=&#123;&#125;, inchannels=&#123;&#125;, outchannels=&#123;&#125;'.format(stride, in_channels, out_channels)) if stride != 1 or in_channels != out_channels * block.expansion: downsample = nn.Sequential( nn.Conv2d(in_channels, out_channels * block.expansion, kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d(out_channels * block.expansion), ) # print('fuck downsample:\n&#123;&#125;'.format(downsample)) layers = [] layers.append( block( in_channels=in_channels, out_channels=out_channels, stride=stride, dilation=dilation, downsample=downsample, ) ) in_channels = out_channels * block.expansion for i in range(1, blocks): layers.append( block( in_channels=in_channels, out_channels=out_channels, stride=1, dilation=dilation) ) return nn.Sequential(*layers) def forward(self, x): x = self.layer_head(x) outs = [] for i, layer_name in enumerate(self.layers_res): res_layer = getattr(self, layer_name) x = res_layer(x) if i in range(self.num_stages): outs.append(x) return tuple(outs) def init_weights(self, pretrained=None): if isinstance(pretrained, str): logger = logging.getLogger() load_checkpoint(self, pretrained, strict=False, logger=logger) elif pretrained is None: for m in self.modules(): if isinstance(m, nn.Conv2d): kaiming_init(m) elif isinstance(m, (_BatchNorm, nn.GroupNorm)): constant_init(m, 1) # if self.dcn is not None: # # # raise Exception('Dcn!!!!!!: &#123;&#125;'.format(dcn)) # for m in self.modules(): # if isinstance(m, Bottleneck) and hasattr( # m, 'conv2_offset'): # constant_init(m.conv2_offset, 0) if self.zero_init_residual: # Todo pass # for m in self.modules(): # if isinstance(m, Bottleneck): # constant_init(m.norm3, 0) # elif isinstance(m, BasicBlock): # constant_init(m.norm2, 0) else: raise TypeError('pretrained must be a str or None') # def _freeze_stages(self): # if self.frozen_stages &gt;= 0: # self.norm1.eval() # for m in [self.conv1, self.norm1]: # for param in m.parameters(): # param.requires_grad = False # # for i in range(1, self.frozen_stages + 1): # m = getattr(self, 'layer&#123;&#125;'.format(i)) # m.eval() # for param in m.parameters(): # param.requires_grad = False # def train(self, mode=True): # super(ResNet, self).train(mode) # self._freeze_stages() # if mode and self.norm_eval: # for m in self.modules(): # # trick: eval have effect on BatchNorm only # if isinstance(m, _BatchNorm): # m.eval()]]></content>
      <categories>
        <category>经典网络结构</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>主干网</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SSD 目标检测算法]]></title>
    <url>%2F%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%2FSSD%20%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[SSD 目标检测算法 基础知识The Single-Shot MultiBox Detector 两阶段：RCNN（先通过启发式方法或者 CNN 网络（RPN）产生一系列稀疏候选框，然后通过这些框进行分类、回归） 一阶段：YOLO 、RetinaNet 和 SSD （不同位置进行密集抽样，可以采用不同的尺度和长宽比、然后利用CNN提取特征，进行分类与回归） SSD（参考：https://zhuanlan.zhihu.com/p/33544892）多框预测相比Yolo，SSD采用CNN来直接进行检测，而不是像Yolo那样在全连接层之后做检测。其实采用卷积直接做检测只是SSD相比Yolo的其中一个不同点，另外还有两个重要的改变，一是SSD提取了不同尺度的特征图来做检测，大尺度特征图（较靠前的特征图）可以用来检测小物体，而小尺度特征图（较靠后的特征图）用来检测大物体；二是SSD采用了不同尺度和长宽比的先验框（Prior boxes, Default boxes，在Faster R-CNN中叫做锚，Anchors） 如果检测目标共有 c 个类别，SSD其实需要预测 c+1 个置信度值，其中第一个置信度指的是不含目标或者属于背景的评分。 在预测过程中，置信度最高的那个类别就是边界框所属的类别 对于一个大小 m* n 的特征图，共有 mn 个单元，每个单元设置的先验框数目记为 k ，那么每个单元共需要 (c+4)k 个预测值，所有的单元共需要 (c+4)kmn 个预测值，由于SSD采用卷积做检测，所以就需要 (c+4)k 个卷积核完成这个特征图的检测过程。 SSD采用VGG16作为基础模型，然后在VGG16的基础上新增了卷积层来获得更多的特征图以用于检测。 采用VGG16做基础模型，首先VGG16是在ILSVRC CLS-LOC数据集预训练。然后借鉴了DeepLab-LargeFOV，分别将VGG16的全连接层fc6和fc7转换成 卷积层 conv6和 卷积层conv7，同时将池化层pool5由原来的stride=2的 变成stride=1的 （猜想是不想reduce特征图大小），为了配合这种变化，采用了一种Atrous Algorithm，其实就是conv6采用扩展卷积或带孔卷积（Dilation Conv），其在不增加参数与模型复杂度的条件下指数级扩大卷积的视野，其使用扩张率(dilation rate)参数，来表示扩张的大小，如下图6所示，(a)是普通的 卷积，其视野就是 ，(b)是扩张率为1，此时视野变成 ，(c)扩张率为3时，视野扩大为 ，但是视野的特征更稀疏了。Conv6采用 大小但dilation rate=6的扩展卷积。 将 VGG 的全连接层 fc6、fc7 转换成了 33 的卷积层 conv6 和 11 的卷积层 conv7 将池化层 pool5 （stride=2, kernel_size=2）变成了（stride=1, kernel_size=3） 采用 Atrous 算法：扩张率 dilation rate = 6 移除 Dropout 层和 fc8 层，新增一系列卷积层 其中VGG16中的Conv4_3层将作为用于检测的第一个特征图。conv4_3层特征图大小是 ，但是该层比较靠前，其norm较大，所以在其后面增加了一个L2 Normalization层（参见ParseNet），以保证和后面的检测层差异不是很大，这个和Batch Normalization层不太一样，其仅仅是对每个像素点在channle维度做归一化，而Batch Normalization层是在[batch_size, width, height]三个维度上做归一化。归一化后一般设置一个可训练的放缩变量gamma。 从后面新增的卷积层中提取Conv7，Conv8_2，Conv9_2，Conv10_2，Conv11_2作为检测所用的特征图，加上Conv4_3层，共提取了6个特征图，其大小分别是 ，但是不同特征图设置的先验框数目不同（同一个特征图上每个单元设置的先验框是相同的，这里的数目指的是一个单元的先验框数目）。先验框的设置，包括尺度（或者说大小）和长宽比两个方面。 先要确定训练图片中的ground truth（真实目标）与哪个先验框来进行匹配，与之匹配的先验框所对应的边界框将负责预测它。在Yolo中，ground truth的中心落在哪个单元格，该单元格中与其IOU最大的边界框负责预测它。但是在SSD中却完全不一样，SSD的先验框与ground truth的匹配原则主要有两点两个原则： 对于图片中每个ground truth，找到与其IOU最大的先验框，该先验框与其匹配 对于剩余的未匹配先验框，若某个ground truth的 大于某个阈值（一般是0.5），那么该先验框也与这个ground truth进行匹配 TensorFlow版本就是只实施了第二个原则，但是这里的Pytorch两个原则都实施了。图8为一个匹配示意图，其中绿色的GT是ground truth，红色为先验框，FP表示负样本，TP表示正样本。 尽管一个ground truth可以与多个先验框匹配，但是ground truth相对先验框还是太少了，所以负样本相对正样本会很多。为了保证正负样本尽量平衡，SSD采用了hard negative mining，就是对负样本进行抽样，抽样时按照置信度误差（预测背景的置信度越小，误差越大）进行降序排列，选取误差的较大的top-k作为训练的负样本，以保证正负样本比例接近1:3。 3838 和最后 33、1*1 对应的先验框为：1:1 两种和 1:2、2:1其他的对应6个先验框：1:1 两种和 1:2、2:1、1:3、3:1 代码结构详解 vgg() - 修改后的 VGG-161234567891011121314151617181920212223def vgg(cfg, i, batch_norm=False): layers = [] in_channels = i for v in cfg: if v == 'M': layers += [nn.MaxPool2d(kernel_size=2, stride=2)] elif v == 'C': layers += [nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)] else: conv2d = nn.Conv2d(in_channels=in_channels, out_channels=v, kernel_size=3, padding=1) if batch_norm: layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)] else: layers += [conv2d, nn.ReLU(inplace=True)] in_channels = v pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1) conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6) conv7 = nn.Con2d(1024, 1024, kernel_size=1) layers += [pool5, conv6, nn.ReLU(inplace=True), conv7, nn.ReLU(inplace=True)] return layers add_extras() - 添加层，作用：回归、分类1234567891011def add_extras(cfg, i, batch_norm=False): exts1_1 = nn.Conv2d(in_channels=i, out_channels=256, kernel_size=1) exts1_2 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=2, padding=1) exts2_1 = nn.Conv2d(512, 128, 1, 1, 0) exts2_2 = nn.Conv2d(128, 256, 3, 2, 1) exts3_1 = nn.Conv2d(256, 128, 1, 1, 0) exts3_2 = nn.Conv2d(128, 256, 3, 1, 0) exts4_1 = nn.Conv2d(256, 128, 1, 1, 0) exts4_2 = nn.Conv2d(128, 256, 3, 1, 0) return [exts1_1, exts1_2, exts2_1, exts2_2, exts3_1, exts3_2, exts4_1, exts4_2] multibox() - 定义坐标预测层、分类层12345678910111213141516171819202122232425262728def multibox(vgg, extras, num_classes): loc_layers = [] conf_layers = [] #vgg_source=[21, -2] # 21 denote conv4_3, -2 denote conv7 # 定义6个坐标预测层, 输出的通道数就是每个像素点上会产生的 default box 的数量 loc1 = nn.Conv2d(vgg[21].out_channels, 4*4, 3, 1, 1) # 利用conv4_3的特征图谱, 也就是 vgg 网络 List 中的第 21 个元素的输出(注意不是第21层, 因为这中间还包含了不带参数的池化层). loc2 = nn.Conv2d(vgg[-2].out_channels, 6*4, 3, 1, 1) # Conv7 loc3 = nn.Conv2d(vgg[1].out_channels, 6*4, 3, 1, 1) # exts1_2 loc4 = nn.Conv2d(extras[3].out_channels, 6*4, 3, 1, 1) # exts2_2 loc5 = nn.Conv2d(extras[5].out_channels, 4*4, 3, 1, 1) # exts3_2 loc6 = nn.Conv2d(extras[7].out_channels, 4*4, 3, 1, 1) # exts4_2 loc_layers = [loc1, loc2, loc3, loc4, loc5, loc6] # 定义分类层, 和定位层差不多, 只不过输出的通道数不一样, 因为对于每一个像素点上的每一个default box, # 都需要预测出属于任意一个类的概率, 因此通道数为 default box 的数量乘以类别数. conf1 = nn.Conv2d(vgg[21].out_channels, 4*num_classes, 3, 1, 1) conf2 = nn.Conv2d(vgg[-2].out_channels, 6*num_classes, 3, 1, 1) conf3 = nn.Conv2d(extras[1].out_channels, 6*num_classes, 3, 1, 1) conf4 = nn.Conv2d(extras[3].out_channels, 6*num_classes, 3, 1, 1) conf5 = nn.Conv2d(extras[5].out_channels, 4*num_classes, 3, 1, 1) conf6 = nn.Conv2d(extras[7].out_channels, 4*num_classes, 3, 1, 1) conf_layers = [conf1, conf2, conf3, conf4, conf5, conf6] # loc_layers: [b×w1×h1×4*4, b×w2×h2×6*4, b×w3×h3×6*4, b×w4×h4×6*4, b×w5×h5×4*4, b×w6×h6×4*4] # conf_layers: [b×w1×h1×4*C, b×w2×h2×6*C, b×w3×h3×6*C, b×w4×h4×6*C, b×w5×h5×4*C, b×w6×h6×4*C] C为num_classes # 注意pytorch中卷积层的输入输出维度是:[N×C×H×W], 上面的顺序有点错误, 不过改起来太麻烦 return loc_layers, conf_layers SSD 网络定义123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111# ssd.pyclass SSD(nn.Module): # SSD网络是由 VGG 网络后接 multibox 卷积层 组成的, 每一个 multibox 层会有如下分支: # - 用于class conf scores的卷积层 # - 用于localization predictions的卷积层 # - 与priorbox layer相关联, 产生默认的bounding box # 参数: # phase: test/train # size: 输入图片的尺寸 # base: VGG16的层 # extras: 将输出结果送到multibox loc和conf layers的额外的层 # head: "multibox head", 包含一系列的loc和conf卷积层. def __init__(self, phase, size, base, extras, head, num_classes): # super(SSD, self) 首先找到 SSD 的父类, 然后把类SSD的对象转换为父类的对象 super(SSD, self).__init__() self.phase = phase self.num_classes = num_classes self.cfg = (coco, voc)[num_classes == 21] self.priorbox = PriorBox(self.cfg) # layers/functions/prior_box.py class PriorBox(object) self.priors = Variable(self.priorbox.forward(), volatile=True) # from torch.autograd import Variable self.size = size self.vgg = nn.ModuleList(base) self.L2Norm = L2Norm(512,20) # layers/modules/l2norm.py class L2Norm(nn.Module) self.extras = nn.ModuleList(extras) self.loc = nn.ModuleList(head[0]) # head = (loc_layers, conf_layers) self.conf = nn.ModuleList(head[1]) if phase = "test": self.softmax = nn.Softmax(dim=-1) # 用于囧穿概率 self.detect = Detect(num_classes, 0, 200, 0.01, 0.45) # layers/functions/detection.py class Detect # 用于将预测结果转换成对应的坐标和类别编号形式, 方便可视化. def forward(self, x): # 定义forward函数, 将设计好的layers和ops应用到输入图片 x 上 # 参数: x, 输入的batch 图片, Shape: [batch, 3, 300, 300] # 返回值: 取决于不同阶段 # test: 预测的类别标签, confidence score, 以及相关的location. # Shape: [batch, topk, 7] # train: 关于以下输出的元素组成的列表 # 1: confidence layers, Shape: [batch*num_priors, num_classes] # 2: localization layers, Shape: [batch, num_priors*4] # 3: priorbox layers, Shape: [2, num_priors*4] sources = list() # 这个列表存储的是参与预测的卷积层的输出, 也就是原文中那6个指定的卷积层 loc = list() # 用于存储预测的边框信息 conf = list() # 用于存储预测的类别信息 # 计算vgg直到conv4_3的relu for k in range(23): x = self.vgg[k](x) s = self.L2Norm(x) sources.append(s) # 将 conv4_3 的特征层输出添加到 sources 中, 后面会根据 sources 中的元素进行预测 # 将vgg应用到fc7 for k in range(23, len(self.vgg)): x = self.vgg[k](x) sources.append(x) # 同理, 添加到 sources 列表中 # 计算extras layers, 并且将结果存储到sources列表中 for k, v in enumerate(self.extras): x = F.relu(v(x), inplace=True) # import torch.nn.functional as F if k % 2 = 1: # 在extras_layers中, 第1,3,5,7,9(从第0开始)的卷积层的输出会用于预测box位置和类别, 因此, 将其添加到 sources列表中 sources.append(x) # 应用multibox到source layers上, source layers中的元素均为各个用于预测的特征图谱 # apply multibox to source layers # 注意pytorch中卷积层的输入输出维度是:[N×C×H×W] for (x, l, c) in zip(sources, self.loc, self.conf): # permute重新排列维度顺序, PyTorch维度的默认排列顺序为 (N, C, H, W), # 因此, 这里的排列是将其改为 $(N, H, W, C)$. # contiguous返回内存连续的tensor, 由于在执行permute或者transpose等操作之后, tensor的内存地址可能不是连续的, # 然后 view 操作是基于连续地址的, 因此, 需要调用contiguous语句. loc.append(l(x).permute(0,2,3,1).contiguous()) conf.append(c(x).permute(0,2,3,1).contiguous()) # loc: [b×w1×h1×4*4, b×w2×h2×6*4, b×w3×h3×6*4, b×w4×h4×6*4, b×w5×h5×4*4, b×w6×h6×4*4] # conf: [b×w1×h1×4*C, b×w2×h2×6*C, b×w3×h3×6*C, b×w4×h4×6*C, b×w5×h5×4*C, b×w6×h6×4*C] C为num_classes # cat 是 concatenate 的缩写, view返回一个新的tensor, 具有相同的数据但是不同的size, 类似于numpy的reshape # 在调用view之前, 需要先调用contiguous loc = torch.cat([o.view(o.size(0), -1) for o in loc], 1) # 将除batch以外的其他维度合并, 因此, 对于边框坐标来说, 最终的shape为(两维):[batch, num_boxes*4] conf = torch.cat([o.view(o.size(0), -1) for o in conf], 1) # 同理, 最终的shape为(两维):[batch, num_boxes*num_classes] if self.phase == "test": # 这里用到了 detect 对象, 该对象主要由于接预测出来的结果进行解析, 以获得方便可视化的边框坐标和类别编号, 具体实现会在后文讨论. output = self.detect( loc.view(loc.size(0), -1, 4), # 又将shape转换成: [batch, num_boxes, 4], 即[1, 8732, 4] self.softmax(conf.view(conf.size(0), -1, self.num_classes)), # 同理, shape 为[batch, num_boxes, num_classes], 即 [1, 8732, 21] self.priors.type(type(x.data)) # 利用 PriorBox对象获取特征图谱上的 default box, 该参数的shape为: [8732,4]. 关于生成 default box 的方法实际上很简单, 类似于 anchor box, 详细的代码实现会在后文解析. # 这里的 self.priors.type(type(x.data)) 与 self.priors 就结果而言完全等价(自己试验过了), 但是为什么? ) if self.phase == "train": # 如果是训练阶段, 则无需解析预测结果, 直接返回然后求损失. output = ( loc.view(loc.size(0), -1, 4), conf.view(conf.size(0), -1, self.num_classes), self.priors ) return output def load_weights(self, base_file): # 加载权重文件 other, ext = os.path.splitext(base_file) if ext == ".pkl" or ".pth": print("Loading weights into state dict...") self.load_state_dict(torch.load(base_file, map_location=lambda storage, loc: storage)) print("Finished!") else: print("Sorry only .pth and .pkl files supported") （一）PriorBox 类 - 用于生成 Default boxlayers/functions/prior_box.py(也可以看成是 anchor box) 根据 SSD 的原理, 需要在选定的特征图谱上输出 Default Box, 然后根据这些 Default Box 进行边框回归任务. 首先梳理一下生成 Default Box 的思路. 假如feature maps数量为 m, 那么每一个feature map中的default box的尺寸大小计算如下:上式中, Smin=0.2, Smax=0.9. 对于原文中的设置 m=6(4,6,6,6,4,4), 因此就有 :s={0.2,0.34,0.48,0.62,0.76,0.9}然后, 几个不同的aspect ratio, 用 arar 表示: ar=1,2,3,1/2,1/3, 则每一个default boxes 的width 和height就可以得到( ):对于宽高比为1的 default box, 我们额外添加了一个 scale 为 的 box, 因此 feature map 上的每一个像素点都对应着6个 default boxes (per feature map localtion).每一个default box的中心, 设置为: , 其中, |fk| 是第 k 个feature map的大小 i,j对应了 feature map 上所有可能的像素点.在实际使用中, 可以自己根据数据集的特点来安排不同的 default boxes 参数组合 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677from __future__ import divisionfrom math import sqrt as sqrtfrom itertools import product as productimport torch# Default-Box 生成候选框class PriorBox(object): """Compute priorbox coordinates in center-offset form for each source feature map. """ # 所谓 prior-box 实际上就是网格中每一个 cell 推荐的 box def __init__(self, cfg): # 在 SSD 的 init 中, cfg=(coco, voc)[num_classes=21] # coco, voc 的相关配置都来自于 data/cfg.py 文件 super(PriorBox, self).__init__() self.image_size = cfg['min_dim'] # 300 # number of priors for feature map location (either 4 or 6) self.num_priors = len(cfg['aspect_ratios']) # [[2], [2, 3], [2, 3], [2, 3], [2], [2]] self.variance = cfg['variance'] or [0.1] # [0.1, 0.2] self.feature_maps = cfg['feature_maps'] # [38, 19, 10, 5, 3, 1] self.min_sizes = cfg['min_sizes'] # [21, 45, 99, 153, 207, 261] self.max_sizes = cfg['max_sizes'] # [45, 99, 153, 207, 261, 315] self.steps = cfg['steps'] # [8, 16, 32, 64, 100, 300] self.aspect_ratios = cfg['aspect_ratios'] # self.clip = cfg['clip'] # True self.version = cfg['name'] # 'COCO' for v in self.variance: if v &lt;= 0: raise ValueError('Variances must be greater than 0') def forward(self): mean = [] for k, f in enumerate(self.feature_maps): # 存放的是feature map的尺寸:38,19,10,5,3,1 # from itertools import product as product # product 用于求多个可迭代对象的笛卡尔积 for i, j in product(range(f), repeat=2): # 这里实际上可以用最普通的for循环嵌套来代替, 主要目的是产生anchor的坐标(i,j) # product 用于求多个可迭代对象的笛卡尔积 f_k = self.image_size / self.steps[k] # steps=[8,16,32,64,100,300]. f_k大约为feature map的尺寸 # unit center x,y # 求得center的坐标, 浮点类型. 实际上, 这里也可以直接使用整数类型的 `f`, 计算上没太大差别 cx = (j + 0.5) / f_k cy = (i + 0.5) / f_k # aspect_ratio: 1 # rel size: min_size # aspect_ratios 为1时对应的box s_k = self.min_sizes[k]/self.image_size mean += [cx, cy, s_k, s_k] # aspect_ratio: 1 # rel size: sqrt(s_k * s_(k+1)) # 根据原文, 当 aspect_ratios 为1时, 会有一个额外的 box, 如下: s_k_prime = sqrt(s_k * (self.max_sizes[k]/self.image_size)) mean += [cx, cy, s_k_prime, s_k_prime] # rest of aspect ratios # 其余(2, 或 2,3)的宽高比(aspect ratio) for ar in self.aspect_ratios[k]: mean += [cx, cy, s_k*sqrt(ar), s_k/sqrt(ar)] mean += [cx, cy, s_k/sqrt(ar), s_k*sqrt(ar)] # 综上, 每个卷积特征图谱上每个像素点最终产生的 box 数量要么为4, 要么为6, 根据不同情况可自行修改. # back to torch land output = torch.Tensor(mean).view(-1, 4) if self.clip: output.clamp_(max=1, min=0) # clamp_ 是clamp的原地执行版本 return output # 输出default box坐标(可以理解为anchor box)## 最终, 输出的 ouput 就是一张图片中所有的 default box 的坐标, 对于论文中的默认设置来说产生的 box 数量为: 8732 上面用到了 python 自带的迭代器 itertools。http://funhacks.net/2017/02/13/itertools/ 迭代器的特点是：惰性求值（Lazy evaluation），即只有当迭代至某个值时，它才会被计算，这个特点使得迭代器特别适合于遍历大文件或无限集合等，因为我们不用一次性将它们存储在内存中。 product 用于求多个可迭代对象的笛卡尔积，它跟嵌套的 for 循环等价。它的一般使用形式如下：（其中，repeat 是一个关键字参数，用于指定重复生成序列的次数） 1product(iter1, iter2, ... iterN, [repeat=1]) 关于 enumerate() 函数 （二）L2Norm 类 - 实现了 L2归一化layers/modules/l2norm.py 123456789101112131415161718192021222324import torchimport torch.nn as nnfrom torch.autograd import Functionfrom torch.autograd import Variableimport torch.nn.init as initclass L2Norm(nn.Module): def __init__(self,n_channels, scale): super(L2Norm,self).__init__() self.n_channels = n_channels self.gamma = scale or None self.eps = 1e-10 self.weight = nn.Parameter(torch.Tensor(self.n_channels)) self.reset_parameters() def reset_parameters(self): init.constant(self.weight,self.gamma) def forward(self, x): norm = x.pow(2).sum(dim=1, keepdim=True).sqrt()+self.eps #x /= norm x = torch.div(x,norm) out = self.weight.unsqueeze(0).unsqueeze(2).unsqueeze(3).expand_as(x) * x return out （三）Detect 类 - 用于解析预测结果, 并将其转换成边框坐标和类别编号layers/functions/detection.py在模型中, 我们为了加快训练速度, 促使模型收敛, 因此会将相应的 box 的坐标转换成与图片size成比例的小数形式, 因此, 无法直接将模型产生的预测结果可视化. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162import torchfrom torch.autograd import Functionfrom SSD.layers.box_utils import decode, nmsfrom SSD.data import voc0712 as cfgclass Detect(Function): """At test time, Detect is the final layer of SSD. Decode location preds, apply non-maximum suppression to location predictions based on conf scores and threshold to a top_k number of output predictions for both confidence score and locations. """ def __init__(self, num_classes, bkg_label, top_k, conf_thresh, nms_thresh): self.num_classes = num_classes self.background_label = bkg_label self.top_k = top_k # Parameters used in nms. self.nms_thresh = nms_thresh if nms_thresh &lt;= 0: raise ValueError('nms_threshold must be non negative.') self.conf_thresh = conf_thresh self.variance = cfg['variance'] def forward(self, loc_data, conf_data, prior_data): """ Args: loc_data: (tensor) Loc preds from loc layers Shape: [batch,num_priors*4] conf_data: (tensor) Shape: Conf preds from conf layers Shape: [batch*num_priors,num_classes] prior_data: (tensor) Prior boxes and variances from priorbox layers Shape: [1,num_priors,4] """ num = loc_data.size(0) # batch size num_priors = prior_data.size(0) output = torch.zeros(num, self.num_classes, self.top_k, 5) conf_preds = conf_data.view(num, num_priors, self.num_classes).transpose(2, 1) # Decode predictions into bboxes. for i in range(num): decoded_boxes = decode(loc_data[i], prior_data, self.variance) # For each class, perform nms conf_scores = conf_preds[i].clone() for cl in range(1, self.num_classes): c_mask = conf_scores[cl].gt(self.conf_thresh) scores = conf_scores[cl][c_mask] if scores.dim() == 0: continue l_mask = c_mask.unsqueeze(1).expand_as(decoded_boxes) boxes = decoded_boxes[l_mask].view(-1, 4) # idx of highest scoring and non-overlapping boxes per class ids, count = nms(boxes, scores, self.nms_thresh, self.top_k) output[i, cl, :count] = \ torch.cat((scores[ids[:count]].unsqueeze(1), boxes[ids[:count]]), 1) flt = output.contiguous().view(num, -1, 5) _, idx = flt[:, :, 0].sort(1, descending=True) _, rank = idx.sort(1) flt[(rank &lt; self.top_k).unsqueeze(-1).expand_as(flt)].fill_(0) return output decode() 和 nms() 函数123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384def decode(loc, priors, variances): """Decode locations from predictions using priors to undo the encoding we did for offset regression at train time. Args: loc (tensor): location predictions for loc layers, Shape: [num_priors,4] priors (tensor): Prior boxes in center-offset form. Shape: [num_priors,4]. variances: (list[float]) Variances of priorboxes Return: decoded bounding box predictions """ boxes = torch.cat(( priors[:, :2] + loc[:, :2] * variances[0] * priors[:, 2:], priors[:, 2:] * torch.exp(loc[:, 2:] * variances[1])), 1) boxes[:, :2] -= boxes[:, 2:] / 2 boxes[:, 2:] += boxes[:, :2] return boxesdef nms(boxes, scores, overlap=0.5, top_k=200): """Apply non-maximum suppression at test time to avoid detecting too many overlapping bounding boxes for a given object. Args: boxes: (tensor) The location preds for the img, Shape: [num_priors,4]. scores: (tensor) The class predscores for the img, Shape:[num_priors]. overlap: (float) The overlap thresh for suppressing unnecessary boxes. top_k: (int) The Maximum number of box preds to consider. Return: The indices of the kept boxes with respect to num_priors. """ keep = scores.new(scores.size(0)).zero_().long() if boxes.numel() == 0: return keep x1 = boxes[:, 0] y1 = boxes[:, 1] x2 = boxes[:, 2] y2 = boxes[:, 3] area = torch.mul(x2 - x1, y2 - y1) v, idx = scores.sort(0) # sort in ascending order # I = I[v &gt;= 0.01] idx = idx[-top_k:] # indices of the top-k largest vals xx1 = boxes.new() yy1 = boxes.new() xx2 = boxes.new() yy2 = boxes.new() w = boxes.new() h = boxes.new() # keep = torch.Tensor() count = 0 while idx.numel() &gt; 0: i = idx[-1] # index of current largest val # keep.append(i) keep[count] = i count += 1 if idx.size(0) == 1: break idx = idx[:-1] # remove kept element from view # load bboxes of next highest vals torch.index_select(x1, 0, idx, out=xx1) torch.index_select(y1, 0, idx, out=yy1) torch.index_select(x2, 0, idx, out=xx2) torch.index_select(y2, 0, idx, out=yy2) # store element-wise max with next highest score xx1 = torch.clamp(xx1, min=x1[i]) yy1 = torch.clamp(yy1, min=y1[i]) xx2 = torch.clamp(xx2, max=x2[i]) yy2 = torch.clamp(yy2, max=y2[i]) w.resize_as_(xx2) h.resize_as_(yy2) w = xx2 - xx1 h = yy2 - yy1 # check sizes of xx1 and xx2.. after each iteration w = torch.clamp(w, min=0.0) h = torch.clamp(h, min=0.0) inter = w*h # IoU = i / (area(a) + area(b) - i) rem_areas = torch.index_select(area, 0, idx) # load remaining areas) union = (rem_areas - inter) + area[i] IoU = inter/union # store result in iou # keep only elements with an IoU &lt;= overlap idx = idx[IoU.le(overlap)] return keep, count 构建 SSD 网络123456789101112131415161718192021222324252627282930313233343536class SSD(nn.Module): # ...自定义SSD网络def vgg(cfg, i, batch_norm=False): # ... 搭建vgg网络def add_extras(cfg, i, batch_norm=False): # ... 向VGG网络中添加额外的层用于feature scalingdef multibox(vgg, extra_layers, cfg, num_classes): # ... 构建multibox结构 base = &#123; # vgg 网络结构参数 '300': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'C', 512, 512, 512, 'M', 512, 512, 512], '500': []&#125;extras = &#123; # extras 层参数 '300': [256, 'S', 512, 128, 'S', 256, 128, 256, 128, 256], '500': []&#125;mbox = &#123; # multibox 相关参数 '300': [4, 6, 6, 6, 4, 4], '500': []&#125;def build_ssd(phase, size=300, num_classes=21): # 构建模型函数, 调用上面的函数进行构建 if phase != "test" and phase != "train": # 只能是训练或者预测阶段 print("ERROR: Phase: " + phase + " not recognized") return if size != 300: print("ERROR: You specified size " + repr(size) + ". However, "+ "currently only SSD300 is supported!") # 仅仅支持300size的SSD return base_, extras_, head_ = multibox(vgg(base[str(size)], 3), add_extras(extras[str(size), 1024), mbox[str(size)], num_classes ) return SSD(phase, size, base_, extras_, head_, num_classes) 损失函数 MultiBox 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112# layers/modules/multibox_loss.pyclass MultiBoxLoss(nn.Module): # 计算目标: # 输出那些与真实框的iou大于一定阈值的框的下标. # 根据与真实框的偏移量输出localization目标 # 用难样例挖掘算法去除大量负样本(默认正负样本比例为1:3) # 目标损失: # L(x,c,l,g) = (Lconf(x,c) + αLloc(x,l,g)) / N # 参数: # c: 类别置信度(class confidences) # l: 预测的框(predicted boxes) # g: 真实框(ground truth boxes) # N: 匹配到的框的数量(number of matched default boxes) def __init__(self, num_classes, overlap_thresh, prior_for_matching, bkg_label, neg_mining, neg_pos, neg_overlap, encode_target, use_gpu=True): super(MultiBoxLoss, self).__init__() self.use_gpu = use_gpu self.num_classes= num_classes # 列表数 self.threshold = overlap_thresh # 交并比阈值, 0.5 self.background_label = bkg_label # 背景标签, 0 self.use_prior_for_matching = prior_for_matching # True 没卵用 self.do_neg_mining = neg_mining # True, 没卵用 self.negpos_ratio = neg_pos # 负样本和正样本的比例, 3:1 self.neg_overlap = neg_overlap # 0.5 判定负样本的阈值. self.encode_target = encode_target # False 没卵用 self.variance = cfg["variance"] def forward(self, predictions, targets): loc_data, conf_data, priors = predictions # loc_data: [batch_size, 8732, 4] # conf_data: [batch_size, 8732, 21] # priors: [8732, 4] default box 对于任意的图片, 都是相同的, 因此无需带有 batch 维度 num = loc_data.size(0) # num = batch_size priors = priors[:loc_data.size(1), :] # loc_data.size(1) = 8732, 因此 priors 维持不变 num_priors = (priors.size(0)) # num_priors = 8732 num_classes = self.num_classes # num_classes = 21 (默认为voc数据集) # 将priors(default boxes)和ground truth boxes匹配 loc_t = torch.Tensor(num, num_priors, 4) # shape:[batch_size, 8732, 4] conf_t = torch.LongTensor(num, num_priors) # shape:[batch_size, 8732] for idx in range(num): # targets是列表, 列表的长度为batch_size, 列表中每个元素为一个 tensor, # 其 shape 为 [num_objs, 5], 其中 num_objs 为当前图片中物体的数量, 第二维前4个元素为边框坐标, 最后一个元素为类别编号(1~20) truths = targets[idx][:, :-1].data # [num_objs, 4] labels = targets[idx][:, -1].data # [num_objs] 使用的是 -1, 而不是 -1:, 因此, 返回的维度变少了 defaults = priors.data # [8732, 4] # from ..box_utils import match # 关键函数, 实现候选框与真实框之间的匹配, 注意是候选框而不是预测结果框! 这个函数实现较为复杂, 会在后面着重讲解 match(self.threshold, truths, defaults, self.variance, labels, loc_t, conf_t, idx) # 注意! 要清楚 Python 中的参数传递机制, 此处在函数内部会改变 loc_t, conf_t 的值, 关于 match 的详细讲解可以看后面的代码解析 if self.use_gpu: loc_t = loc_t.cuda() conf_t = conf_t.cuda() # 用Variable封装loc_t, 新版本的 PyTorch 无需这么做, 只需要将 requires_grad 属性设置为 True 就行了 loc_t = Variable(loc_t, requires_grad=False) conf_t = Variable(conf_t, requires_grad=False) pos = conf_t &gt; 0 # 筛选出 &gt;0 的box下标(大部分都是=0的) num_pos = pos.sum(dim=1, keepdim=True) # 求和, 取得满足条件的box的数量, [batch_size, num_gt_threshold] # 位置(localization)损失函数, 使用 Smooth L1 函数求损失 # loc_data:[batch, num_priors, 4] # pos: [batch, num_priors] # pos_idx: [batch, num_priors, 4], 复制下标成坐标格式, 以便获取坐标值 pos_idx = pos.unsqueeze(pos.dim()).expand_as(loc_data) loc_p = loc_data[pos_idx].view(-1, 4)# 获取预测结果值 loc_t = loc_t[pos_idx].view(-1, 4) # 获取gt值 loss_l = F.smooth_l1_loss(loc_p, loc_t, size_average=False) # 计算损失 # 计算最大的置信度, 以进行难负样本挖掘 # conf_data: [batch, num_priors, num_classes] # batch_conf: [batch, num_priors, num_classes] batch_conf = conf_data.view(-1, self.num_classes) # reshape # conf_t: [batch, num_priors] # loss_c: [batch*num_priors, 1], 计算每个priorbox预测后的损失 loss_c = log_sum_exp(batch_conf) - batch_conf.gather(1, conf_t.view(-1,1)) # 难负样本挖掘, 按照loss进行排序, 取loss最大的负样本参与更新 loss_c[pos.view(-1, 1)] = 0 # 将所有的pos下标的box的loss置为0(pos指示的是正样本的下标) # 将 loss_c 的shape 从 [batch*num_priors, 1] 转换成 [batch, num_priors] loss_c = loss_c.view(num, -1) # reshape # 进行降序排序, 并获取到排序的下标 _, loss_idx = loss_c.sort(1, descending=True) # 将下标进行升序排序, 并获取到下标的下标 _, idx_rank = loss_idx.sort(1) # num_pos: [batch, 1], 统计每个样本中的obj个数 num_pos = pos.long().sum(1, keepdim=True) # 根据obj的个数, 确定负样本的个数(正样本的3倍) num_neg = torch.clamp(self.negpos_ratio*num_pos, max=pos.size(1)-1) # 获取到负样本的下标 neg = idx_rank &lt; num_neg.expand_as(idx_rank) # 计算包括正样本和负样本的置信度损失 # pos: [batch, num_priors] # pos_idx: [batch, num_priors, num_classes] pos_idx = pos.unsqueeze(2).expand_as(conf_data) # neg: [batch, num_priors] # neg_idx: [batch, num_priors, num_classes] neg_idx = neg.unsqueeze(2).expand_as(conf_data) # 按照pos_idx和neg_idx指示的下标筛选参与计算损失的预测数据 conf_p = conf_data[(pos_idx+neg_idx).gt(0)].view(-1, self.num_classes) # 按照pos_idx和neg_idx筛选目标数据 targets_weighted = conf_t[(pos+neg).gt(0)] # 计算二者的交叉熵 loss_c = F.cross_entropy(conf_p, targets_weighted, size_average=False) # 将损失函数归一化后返回 N = num_pos.data.sum() loss_l = loss_l / N loss_c = loss_c / N return loss_l, loss_c GT Box 与 Default Box 的匹配在上面的代码中, 有一个很重要的函数, 即 match() 函数, 因为我们知道, 当根据特征图谱求出这些 prior box(default box, 8732个)以后, 我们仅仅知道这些 box 的 scale 和 aspect_ratios 信息, 但是如果要计算损失函数, 我们就必须知道与每个 prior box 相对应的 ground truth box 是哪一个, 因此, 我们需要根据交并比来求得这些 box 之间的匹配关系. 匹配算法的核心思想如下: 首先将找到与每个 gtbox 交并比最高的 defaultbox, 记录其下标 然后找到与每个 defaultbox 交并比最高的 gtbox. 注意, 这两步不是一个相互的过程, 假想一种极端情况, 所有的priorbox与某个gtbox(标记为G)的交并比为1, 而其他gtbox分别有一个交并比最高的priorbox, 但是肯定小于1(因为其他的gtbox与G的交并比肯定小于1), 这样一来, 就会使得所有的priorbox都与G匹配. 为了防止上面的情况, 我们将那些对于gtbox来说, 交并比最高的priorbox, 强制进行互相匹配, 即令 best_truth_idx[best_prior_idx[j]] = j, 详细见下面的for循环. 根据下标获取每个priorbox对应的gtbox的坐标, 然后对坐标进行相应编码, 并存储起来, 同时将gt类别也存储起来, 到此, 匹配完成. （一）point_form() 函数 - 将 boxes 的坐标信息转换成左上角和右下角的形式（二）intersect() 函数 - 返回 box_a 与 box_b 集合中元素的交集（三）jaccard() 函数 - 返回 box_a 与 box_b 集合中元素的交并比（四）encode() 函数 - 将 box 信息编码成小数形式, 方便网络训练（五）match() 函数 - 匹配算法, 通过调用上述函数实现匹配功能 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103# ./layers/box_utils.pydef point_form(boxes): # 将(cx, cy, w, h) 形式的box坐标转换成 (xmin, ymin, xmax, ymax) 形式 return torch.cat( (boxes[:2] - boxes[2:]/2), # xmin, ymin (boxes[:2] + boxes[2:]/2), 1) # xmax, ymaxdef intersect(box_a, box_b): # box_a: (truths), (tensor:[num_obj, 4]) # box_b: (priors), (tensor:[num_priors, 4], 即[8732, 4]) # return: (tensor:[num_obj, num_priors]) box_a 与 box_b 两个集合中任意两个 box 的交集, 其中res[i][j]代表box_a中第i个box与box_b中第j个box的交集.(非对称矩阵) # 思路: 先将两个box的维度扩展至相同维度: [num_obj, num_priors, 4], 然后计算面积的交集 # 两个box的交集可以看成是一个新的box, 该box的左上角坐标是box_a和box_b左上角坐标的较大值, 右下角坐标是box_a和box_b的右下角坐标的较小值 A = box_a.size(0) B = box_b.size(0) # box_a 左上角/右下角坐标 expand以后, 维度会变成(A,B,2), 其中, 具体可看 expand 的相关原理. box_b也是同理, 这样做是为了得到a中某个box与b中某个box的左上角(min_xy)的较大者(max) # unsqueeze 为增加维度的数量, expand 为扩展维度的大小 min_xy = torch.max(box_a[:, :2].unsqueeze(1).expand(A,B,2), box_b[:, :2].unsqueeze(0).expand(A,B,2)) # 在box_a的 A 和 2 之间增加一个维度, 并将维度扩展到 B. box_b 同理 # 求右下角(max_xy)的较小者(min) max_xy = torch.min(box_a[:, 2:].unsqueeze(1).expand(A,B,2), box_b[:, 2:].unsqueeze(0).expand(A,B,2)) inter = torch.clamp((max_xy, min_xy), min=0) # 右下角减去左上角, 如果为负值, 说明没有交集, 置为0 return inter[:, :, 0] * inter[:, :, 0] # 高×宽, 返回交集的面积, shape 刚好为 [A, B]def jaccard(box_a, box_b): # A ∩ B / A ∪ B = A ∩ B / (area(A) + area(B) - A ∩ B) # box_a: (truths), (tensor:[num_obj, 4]) # box_b: (priors), (tensor:[num_priors, 4], 即[8732, 4]) # return: (tensor:[num_obj, num_priors]), 代表了 box_a 和 box_b 两个集合中任意两个 box之间的交并比 inter = intersect(box_a, box_b) # 求任意两个box的交集面积, shape为[A, B], 即[num_obj, num_priors] area_a = ((box_a[:,2]-box_a[:,0]) * (box_a[:,3]-box_a[:,1])).unsqueeze(1).expand_as(inter) # [A,B] area_b = ((box_b[:,2]-box_b[:,0]) * (box_b[:,3]-box_b[:,1])).unsqueeze(0).expand_as(inter) # [A,B], 这里会将A中的元素复制B次 union = area_a + area_b - inter return inter / union # [A, B], 返回任意两个box之间的交并比, res[i][j] 代表box_a中的第i个box与box_b中的第j个box之间的交并比.def encode(matched, priors, variances): # 对边框坐标进行编码, 需要宽度方差和高度方差两个参数, 具体公式可以参见原文公式(2) # matched: [num_priors,4] 存储的是与priorbox匹配的gtbox的坐标. 形式为(xmin, ymin, xmax, ymax) # priors: [num_priors, 4] 存储的是priorbox的坐标. 形式为(cx, cy, w, h) # return : encoded boxes: [num_priors, 4] g_cxy = (matched[:, :2] + matched[:, 2:])/2 - priors[:, :2] # 用互相匹配的gtbox的中心坐标减去priorbox的中心坐标, 获得中心坐标的偏移量 g_cxy /= (variances[0]*priors[:, 2:]) # 令中心坐标分别除以 d_i^w 和 d_i^h, 正如原文公式所示 #variances[0]为0.1, 令其分别乘以w和h, 得到d_i^w 和 d_i^h g_wh = (matched[:, 2:] - matched[:, :2]) / priors[:, 2:] # 令互相匹配的gtbox的宽高除以priorbox的宽高. g_wh = torch.log(g_wh) / variances[1] # 这里这个variances[1]=0.2 不太懂是为什么. return torch.cat([g_cxy, g_wh], 1) # 将编码后的中心坐标和宽高``连接起来, 返回 [num_priors, 4]def match(threshold, truths, priors, variances, labels, loc_t, conf_t, idx): # threshold: (float) 确定是否匹配的交并比阈值 # truths: (tensor: [num_obj, 4]) 存储真实 box 的边框坐标 # priors: (tensor: [num_priors, 4], 即[8732, 4]), 存储推荐框的坐标, 注意, 此时的框是 default box, 而不是 SSD 网络预测出来的框的坐标, 预测的结果存储在 loc_data中, 其 shape 为[num_obj, 8732, 4]. # variances: cfg['variance'], [0.1, 0.2], 用于将坐标转换成方便训练的形式(参考RCNN系列对边框坐标的处理) # labels: (tensor: [num_obj]), 代表了每个真实 box 对应的类别的编号 # loc_t: (tensor: [batches, 8732, 4]), # conf_t: (tensor: [batches, 8732]), # idx: batches 中图片的序号, 标识当前正在处理的 image 在 batches 中的序号 overlaps = jaccard(truths, point_form(priors)) # [A, B], 返回任意两个box之间的交并比, overlaps[i][j] 代表box_a中的第i个box与box_b中的第j个box之间的交并比. # 二部图匹配(Bipartite Matching) # [num_objs,1], 得到对于每个 gt box 来说的匹配度最高的 prior box, 前者存储交并比, 后者存储prior box在num_priors中的位置 best_prior_overlap, best_prior_idx = overlaps.max(1, keepdim=True) # keepdim=True, 因此shape为[num_objs,1] # [1, num_priors], 即[1,8732], 同理, 得到对于每个 prior box 来说的匹配度最高的 gt box best_truth_overlap, best_truth_idx = overlaps.max(0, keepdim=True) best_prior_idx.squeeze_(1) # 上面特意保留了维度(keepdim=True), 这里又都把维度 squeeze/reduce 了, 实际上只需用默认的 keepdim=False 就可以自动 squeeze/reduce 维度. best_prior_overlap.squeeze_(1) best_truth_idx.squeeze_(0) best_truth_overlap.squeeze_(0) best_truth_overlap.index_fill_(0, best_prior_idx, 2) # 维度压缩后变为[num_priors], best_prior_idx 维度为[num_objs], # 该语句会将与gt box匹配度最好的prior box 的交并比置为 2, 确保其最大, 以免防止某些 gtbox 没有匹配的 priorbox. # 假想一种极端情况, 所有的priorbox与某个gtbox(标记为G)的交并比为1, 而其他gtbox分别有一个交并比 # 最高的priorbox, 但是肯定小于1(因为其他的gtbox与G的交并比肯定小于1), 这样一来, 就会使得所有 # 的priorbox都与G匹配, 为了防止这种情况, 我们将那些对gtbox来说, 具有最高交并比的priorbox, # 强制进行互相匹配, 即令best_truth_idx[best_prior_idx[j]] = j, 详细见下面的for循环 # 注意!!: 因为 gt box 的数量要远远少于 prior box 的数量, 因此, 同一个 gt box 会与多个 prior box 匹配. for j in range(best_prior_idx.size(0)): # range:0~num_obj-1 best_truth_idx[best_prior_idx[j]] = j # best_prior_idx[j] 代表与box_a的第j个box交并比最高的 prior box 的下标, 将与该 gtbox # 匹配度最好的 prior box 的下标改为j, 由此,完成了该 gtbox 与第j个 prior box 的匹配. # 这里的循环只会进行num_obj次, 剩余的匹配为 best_truth_idx 中原本的值. # 这里处理的情况是, priorbox中第i个box与gtbox中第k个box的交并比最高, # 即 best_truth_idx[i]= k # 但是对于best_prior_idx[k]来说, 它却与priorbox的第l个box有着最高的交并比, # 即best_prior_idx[k]=l # 而对于gtbox的另一个边框gtbox[j]来说, 它与priorbox[i]的交并比最大, # 即但是对于best_prior_idx[j] = i. # 那么, 此时, 我们就应该将best_truth_idx[i]= k 修改成 best_truth_idx[i]= j. # 即令 priorbox[i] 与 gtbox[j]对应. # 这样做的原因: 防止某个gtbox没有匹配的 prior box. mathes = truths[best_truth_idx] # truths 的shape 为[num_objs, 4], 而best_truth_idx是一个指示下标的列表, 列表长度为 8732, # 列表中的下标范围为0~num_objs-1, 代表的是与每个priorbox匹配的gtbox的下标 # 上面的表达式会返回一个shape为 [num_priors, 4], 即 [8732, 4] 的tensor, 代表的就是与每个priorbox匹配的gtbox的坐标值. conf = labels[best_truth_idx]+1 # 与上面的语句道理差不多, 这里得到的是每个prior box匹配的类别编号, shape 为[8732] conf[best_truth_overlap &lt; threshold] = 0 # 将与gtbox的交并比小于阈值的置为0 , 即认为是非物体框 loc = encode(matches, priors, variances) # 返回编码后的中心坐标和宽高. loc_t[idx] = loc # 设置第idx张图片的gt编码坐标信息 conf_t[idx] = conf # 设置第idx张图片的编号信息.(大于0即为物体编号, 认为有物体, 小于0认为是背景) 训练 验证 Tricks]]></content>
      <categories>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>One Stage</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AlexNet 实现]]></title>
    <url>%2F%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%2FVGG%20%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[VGG 实现 2014年ILSVRC14比赛的第2。深度神经网络中的经典网络VGG(Oxford Visual Geometry Group)VGG16的图解VGG网络详解及代码实现简单易懂Pytorch实战实例VGG深度网络用PyTorch实现经典VGG网络：https://hellozhaozheng.github.io/z_post/PyTorch-VGG/https://blog.csdn.net/qq_16234613/article/details/79818370 VGG-16 分层详解https://blog.csdn.net/roguesir/article/details/77051250 综述 作者：牛津大学视觉几何组 时间：2014 输入大小：2242243 源码参考 网络结构 16层：13个conv，3个fc 19层：16个conv，3个fc 参数量 16层：138M 19层：144M 评价 2014年ILSVRC定位第1，分类第2（TOP-5错误率7.3%）（第一名是GoogLeNet） VGG模型在多个迁移学习任务中的表现要优于googLeNet 从图像中提取CNN特征，VGG模型是首选算法 优缺点 优点 VGG的结构非常简洁，整个网络都使用了同样大小的卷积核尺寸3x3和最大池化尺寸2x2 几个小滤波器组合的卷积层比一个大滤波器卷积层好 加深网络层数，可以提高网络性能 vgg网络由于其具有较强的特征提取能力 缺点 参数量有140M之多，需要更大的存储空间,计算量大模型结构模型特点 小卷积核。整个网络都使用了同样大小尺寸（3x3）的卷积核 原因： 1.3个3x3的卷积层串联的效果相当于1个7x7的卷积层； 2.3个串联的3x3的卷积层，拥有比1个7x7的卷积层更少的参数量，（3x3x3）/（7x7）=55% 3.3个3x3的卷积层拥有比1个7x7的卷积层更多的非线性变换，使得CNN对特征的学习能力更强。 4.在保证具有相同感知野的条件下，提升了网络的深度和神经网络的效果 （极少用了1x1），1x1卷积的意义主要是在于线性变换，而输入通道数和输出通道数不变，没有发生降维 小池化核。相比AlexNet的3x3的池化核，VGG全部为2x2的池化核实验结果 A-LRN比A几乎没有提升，说明采用LRN后，并未对网络效果进行改善 E比D、B比A效果有提升，说明分类误差随着网络深度的增加而减少 C比A效果有提升，说明增加1x1的非线性卷积，对提高分类效果有一定作用 D比C效果好，说明增大感受野到3x3具有更好的效果 作者通过分析，认为由于卷积神经网络的特性，3x3大小的卷积核足以捕捉到横、竖以及斜对角像素的变化。源码解析 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697class VGG(nn.Module): def __init__(self, features, num_classes=1000): super(VGG, self).__init__() self.features = features self.classifier = nn.Sequential( nn.Linear(512 * 7 * 7, 4096), nn.ReLU(True), nn.Dropout(), nn.Linear(4096, 4096), nn.ReLU(True), nn.Dropout(), # nn.Linear(4096, num_classes), ) def make_layers(cfg, batch_norm=False): layers = [] in_channels = 3 for v in cfg: if v == 'M': layers += [nn.MaxPool2d(kernel_size=2, stride=2)] else: conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1) if batch_norm: layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)] else: layers += [conv2d, nn.ReLU(inplace=True)] in_channels = v return nn.Sequential(*layers)vgg_cfg = &#123; 'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512],&#125;class VGG16(Network): def __init__(self, feat_strdie=(16, ), anchor_scales=(8, 16, 32), anchor_ratios=(0.5, 1, 2)): Network.__init__(self) self._channels['head'] = 512 self._channels['tail'] = 4096 self._feat_stride = feat_strdie self._anchor_scales = anchor_scales self._anchor_ratios = anchor_ratios self._num_anchors = len(anchor_scales)*len(anchor_ratios) # self._init_network()def _init_network(self, bn=False): self._vgg = VGG(make_layers(vgg_cfg['D'])) # Fix the layers before conv3: for layer in range(10): for p in self._vgg.features[layer].parameters(): p.requires_grad = False self._layers['head'] = self._vgg.features self._layers['tail'] = self._vgg.classifierdef _image_to_head(self, input): return self._layers['head'](input)def _head_to_tail(self, pool5): x = pool5.view(pool5.size()[0], -1) return self._layers['tail'](x)def _load_pre_trained_model(self, pre_trained_model): pre_model = torch.load(pre_trained_model) state_dict = self._vgg.state_dict() pre_model_dict = &#123;k: v for k, v in pre_model.items() if k in state_dict&#125; # # imagenet pre-trained modal is RGB order, but this project is BGR order for key in state_dict.keys(): if 'classifier' in key: key_split = key.split('.') new_key = [key_split[0]]+[str(int(key_split[1])+1)]+[key_split[2]] pre_model_dict[key] = pre_model['.'.join(new_key)] else: pass state_dict.update(pre_model_dict) self._vgg.load_state_dict(pre_model_dict)if __name__ == '__main__': pre_trained_model = '../../data/pretrained_model/vgg16_caffe.pth' pre_model = torch.load(pre_trained_model) for key in pre_model.keys(): print(key) # if 'features.0.weight' in key: # print(key) # print(pre_model[key][0,0:5,0,0]) # print(flip(pre_model[key], 1)[0,-5:, 0, 0]) # print(pre_model[key][0,0:5,0,0]) vgg = VGG16() vgg._init_network() for k in vgg.state_dict().keys(): print(k) # for layer in range(10): # print(vgg._vgg.features[layer]) 创建VGG16()模型，调用_init_network()方法 调用make_layers()方法，根据配置文件vgg_cfg[‘D’]，创建Conv2d,Relu,MaxPool,BatchNorm层 返回features层，调用VGG16的init方法，添加classifier层，返回完整模型self._vgg 给每层的参数添加不求梯度的属性；p.requires_grad = False 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227import torchimport torch.nn as nnfrom torch.autograd import Variableimport torch.nn.functional as F# importfrom torch.utils.data import DataLoaderimport torchvisionimport torchvision.transforms as transformsimport time# ================================================================================================================''' Set Hyper-parameters '''num_epochs = 100learning_rate = 0.1batch_size = 128use_gpu = torch.cuda.is_available()DOWNLOAD = Falseprint('【Preparing Data】---', end='')print('[Time: &#123;:&#125;]'.format(time.strftime('%H:%M:%S', time.localtime())))''' Pre-processing '''# transforms = transforms.Compose([# transforms.Resize(96),# transforms.ToTensor(),# transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))# # channel = (channel - mean) / std# ])# 图像预处理和增强transform_train = transforms.Compose([ transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip(), transforms.ToTensor(), # 将 PIL.Image、Numpy.ndarray 转化成 torch.FloatTensor; 并归一化到[0, 1, 0] transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)), # 怎么确定的？])transform_test = transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),])''' Download DataSet '''train_set = torchvision.datasets.CIFAR10( root='./data', train=True, transform=transform_train, download=DOWNLOAD)test_set = torchvision.datasets.CIFAR10( root='./data', train=False, transform=transform_test)''' Use DataLoader '''train_loader = DataLoader( dataset=train_set, batch_size=batch_size, shuffle=True, # num_workers=2)test_loader = DataLoader( dataset=test_set, batch_size=batch_size, # 100 shuffle=False, # num_workers=2)# ================================================================================================================''' Different Shape of VGG Net '''cfg = &#123; 'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'], 'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'], 'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'], 'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],&#125;classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')''' Define Network '''# DataSet: CIFAR-10class VGG(nn.Module): def __init__(self, vgg_name): super(VGG, self).__init__() self.features = self._make_layers(cfg[vgg_name]) self.classifier = nn.Linear(512, 10) def forward(self, x): out = self.features(x) out = out.view(out.size(0), -1) out = self.classifier(out) return out # _make_layers def _make_layers(self, cfg): layers = [] in_channels = 3 for x in cfg: if x == 'M': layers += [nn.MaxPool2d(kernel_size=2, stride=2)] else: layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1), nn.BatchNorm2d(x), nn.ReLU(inplace=True)] in_channels = x layers += [nn.AvgPool2d(kernel_size=1, stride=1)] return nn.Sequential(*layers)net = VGG('VGG16')# x = torch.randn(2, 3, 32, 32)# print(net(Variable(x)))if use_gpu: net = net.cuda()# print(net)# ================================================================================================================print('【Start Training】---', end='')print('[Time: &#123;:&#125;]'.format(time.strftime('%H:%M:%S', time.localtime())))''' Define Loss And Optimizer '''criterion = nn.CrossEntropyLoss()optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate)net.train()for epoch in range(num_epochs): train_loss = 0 correct = 0 total = 0 for batch_idx, (inputs, targets) in enumerate(train_loader): if use_gpu: inputs, targets = inputs.cuda(), targets.cuda() # Forward optimizer.zero_grad() out = net(inputs) # Calculate The Loss loss = criterion(out, targets) # Backward loss.backward() # Update Param optimizer.step() train_loss += loss.item() _, pred = torch.max(out.data, 1) total += targets.size(0) correct += (pred == targets).sum().item() print(batch_idx, len(train_loader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)' % (train_loss / (batch_idx + 1), 100. * correct / total, correct, total)) # # Lower LearningRate if (epoch + 1) % 10 == 0: learning_rate /= 5 optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate) print('======================= 【Training Acc: &#123;:&#125;%】====================='.format(100 * correct / total), end='') print('[Time: &#123;:&#125;]'.format(time.strftime('%H:%M:%S', time.localtime())))# ================================================================================================================print('【Start Testing】---', end='')print('[Time: &#123;:&#125;]'.format(time.strftime('%H:%M:%S', time.localtime())))net.eval()for epoch in range(num_epochs): test_loss = 0 correct = 0 total = 0 for batch_idx, (inputs, targets) in enumerate(test_loader): if use_gpu: inputs, targets = inputs.cuda(), targets.cuda() # Forward optimizer.zero_grad() out = net(inputs) # Calculate The Loss loss = criterion(out, targets) # Backward loss.backward() # Update Param optimizer.step() test_loss += loss.item() _, pred = torch.max(out.data, 1) total += targets.size(0) correct += (pred == targets).sum().item() print(batch_idx, len(train_loader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)' % (test_loss / (batch_idx + 1), 100. * correct / total, correct, total)) print('======================= 【Testing Acc: &#123;:&#125;%】====================='.format(100 * correct / total), end='') print('[Time: &#123;:&#125;]'.format(time.strftime('%H:%M:%S', time.localtime())))# ================================================================================================================print('【Saving Model】---', end='')print('[Time: &#123;:&#125;]'.format(time.strftime('%H:%M:%S', time.localtime())))torch.save(net.state_dict(), 'D:/Workspace/ModelsPath/' + 'vgg_cifar10_model.ckpt')print('【Saved】')]]></content>
      <categories>
        <category>经典网络结构</category>
      </categories>
      <tags>
        <tag>标签1</tag>
        <tag>标签2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[YOLO系列深度解读]]></title>
    <url>%2F%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%2FYOLO%E7%B3%BB%E5%88%97%E6%B7%B1%E5%BA%A6%E8%A7%A3%E8%AF%BB%2F</url>
    <content type="text"><![CDATA[YOLO系列深度解读 YOLO（You only look once）是基于深度学习的端到端的目标检测算法。与大部分目标检测与识别方法（比如Fast R-CNN）将目标识别任务分类目标区域预测和类别预测等多个流程不同，YOLO将目标区域预测和目标类别预测整合于单个神经网络模型中，实现在准确率较高的情况下实时快速目标检测与识别，其增强版本GPU中能跑45fps，简化版本155fps YOLO v1简介 时间：CVPR 2016(YOLO v1) 作者：Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi 论文：You Only Look Once: Unified, Real-Time Object Detection 代码：yolo_tensorflow 源码分析参考：YOLO源码解析 博客参考：图解YOLO 网络设计 输入shape：448×448×3 输出shape：7×7×30 卷积层个数：24 全连接层个数：2 YOLO核心思想 YOLO的核心思想就是将整张图作为网络的输入，直接在输出层回归bounding box的位置和bounding box的类别 YOLO网络结构设计 网络结构借鉴了 GoogLeNet 。24个卷积层，2个全链接层。（用1×1 reduction layers 紧跟 3×3 convolutional layers 取代Goolenet的 inception modules ） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960def build_network(self, images, num_outputs, alpha, keep_prob=0.5, is_training=True, scope='yolo'): with tf.variable_scope(scope): with slim.arg_scope( [slim.conv2d, slim.fully_connected], activation_fn=leaky_relu(alpha), weights_regularizer=slim.l2_regularizer(0.0005), weights_initializer=tf.truncated_normal_initializer(0.0, 0.01) ): net = tf.pad( images, np.array([[0, 0], [3, 3], [3, 3], [0, 0]]), name='pad_1') net = slim.conv2d( net, 64, 7, 2, padding='VALID', scope='conv_2') net = slim.max_pool2d(net, 2, padding='SAME', scope='pool_3') net = slim.conv2d(net, 192, 3, scope='conv_4') net = slim.max_pool2d(net, 2, padding='SAME', scope='pool_5') net = slim.conv2d(net, 128, 1, scope='conv_6') net = slim.conv2d(net, 256, 3, scope='conv_7') net = slim.conv2d(net, 256, 1, scope='conv_8') net = slim.conv2d(net, 512, 3, scope='conv_9') net = slim.max_pool2d(net, 2, padding='SAME', scope='pool_10') net = slim.conv2d(net, 256, 1, scope='conv_11') net = slim.conv2d(net, 512, 3, scope='conv_12') net = slim.conv2d(net, 256, 1, scope='conv_13') net = slim.conv2d(net, 512, 3, scope='conv_14') net = slim.conv2d(net, 256, 1, scope='conv_15') net = slim.conv2d(net, 512, 3, scope='conv_16') net = slim.conv2d(net, 256, 1, scope='conv_17') net = slim.conv2d(net, 512, 3, scope='conv_18') net = slim.conv2d(net, 512, 1, scope='conv_19') net = slim.conv2d(net, 1024, 3, scope='conv_20') net = slim.max_pool2d(net, 2, padding='SAME', scope='pool_21') net = slim.conv2d(net, 512, 1, scope='conv_22') net = slim.conv2d(net, 1024, 3, scope='conv_23') net = slim.conv2d(net, 512, 1, scope='conv_24') net = slim.conv2d(net, 1024, 3, scope='conv_25') net = slim.conv2d(net, 1024, 3, scope='conv_26') net = tf.pad( net, np.array([[0, 0], [1, 1], [1, 1], [0, 0]]), name='pad_27') net = slim.conv2d( net, 1024, 3, 2, padding='VALID', scope='conv_28') net = slim.conv2d(net, 1024, 3, scope='conv_29') net = slim.conv2d(net, 1024, 3, scope='conv_30') net = tf.transpose(net, [0, 3, 1, 2], name='trans_31') net = slim.flatten(net, scope='flat_32') net = slim.fully_connected(net, 512, scope='fc_33') net = slim.fully_connected(net, 4096, scope='fc_34') net = slim.dropout( net, keep_prob=keep_prob, is_training=is_training, scope='dropout_35') net = slim.fully_connected( net, num_outputs, activation_fn=None, scope='fc_36') return net YOLO实现规则 将一幅图像分成S×S个网络(grid cell)，如果某个object的中心落在这个网格中，则这个网格就负责预测这个object (5×B+C) 每个网格要预测B个bounding box，预测C个类别的概率 每个bounding box要预测(x, y, w, h)和confidence共5个值 confidence代表了(1.所有预测的box含有object的置信度)和(2.box预测的准确度)。其公式为Pr(Object)∗IOUtruthprePr(Object)∗IOUpretruth 第一项：其中如果有object落在一个grid cell里，Pr(Object)=1，否则Pr(Object)=0；第二项：预测bounding box 和 gt_boxes之间的IOU值 S×S个网格(grid cell)的输出维度为 S×S×(5×B+C) 举例说明: 在PASCAL VOC中，图像输入为448x448，取S=7，B=2，一共有20个类别(C=20)。则输出就是7x7x30 训练过程 imame Resize成448448，图片分割得到77网格(cell) CNN提取特征和预测：卷积负责提特征，全链接部分负责预测： a) 7×7×2=98个bounding box(bbox) 的坐标xcenterxcenter,ycenterycenter,w,h 和grid cell是否有物体的conﬁdence 。 b) 7×7=49个cell所属20个物体的概率。 c) 得到每个box的分类置信度得分(class-specific confidence score) 每个网格预测C个类别的概率和bounding box预测的confidence相乘，得到每个boundind box的分类置信度得分(class-specific confidence score),即属于某一类的概率 Pr(Classi|Object)∗Pr(Object)∗IOUtruthpre=Pr(Classi)∗IOUtruthprePr(Classi|Object)∗Pr(Object)∗IOUpretruth=Pr(Classi)∗IOUpretruth 过滤boxes 设置阈值，滤掉分类置信度得分(class-specific confidence score)得分低的boxes 对boxes进行NMS处理，就得到最终的检测结果。损失函数 损失函数的设计目标就是让坐标（x,y,w,h），confidence，classification 这个三个方面达到很好的平衡。 简单的全部采用了平方误差损失(sum-squared error loss)来做这件事会有以下不足： a) 8维的localization error和20维的classification error同等重要显然是不合理的； b) 如果一个网格中没有object（一幅图中这种网格很多），那么就会将这些网格中的box的confidence push到0，相比于较少的有object的网格，这种做法是overpowering的，这会导致网络不稳定甚至发散。 c) 对不同大小的bbox预测中，相比于大bbox预测偏一点，小box预测偏一点更不能忍受。而平方误差损失(sum-squared error loss)中对同样的偏移loss是一样。 d) 一个网格预测多个bounding box，在训练时我们希望每个object（ground true box）只有一个bounding box专门负责（一个object 一个bbox） 解决方案如下： a) 更重视8维的坐标预测，给这些损失前面赋予更大的loss weight, 记为 λcoordλcoord ,在pascal VOC训练中取5,对没有object的bbox的confidence loss，赋予小的loss weight，记为 λnoobjλnoobj ，在pascal VOC训练中取0.5 b) 有object的bbox的confidence loss 和类别的loss 的loss weight正常取1 c) 作者用了一个比较取巧的办法，就是将box的width和height取平方根代替原本的height和width。 因为small bbox的横轴值较小，发生偏移时，反应到y轴上的loss 比 big box 要大 d) 具体做法是与ground true box（object）的IOU最大的bounding box 负责该ground true box(object)的预测。（个人理解：通过过滤和NMS，得到的IOU最大者偏移会更少一些，可以更快速的学习到正确位置）缺点 YOLO对相互靠的很近的物体，还有很小的群体,检测效果不好，这是因为一个网格中只预测了两个框，并且只属于一类。 对测试图像中，同一类物体出现的新的不常见的长宽比和其他情况下泛化能力偏弱。 由于损失函数的问题，定位误差是影响检测效果的主要原因。尤其是大小物体的处理上，还有待加强。YOLO v2简介 [YOLO9000：Better，Faster，Stronger]，YOLOv2是Joseph Redmon提出的针对YOLO算法不足的改进版本，作者使用了一系列的方法对原来的YOLO多目标检测框架进行了改进，在保持原有速度的优势之下，对小物体检测精度上得以提升，此外作者提出了一种目标分类与检测的联合训练方法，通过这种方法YOLO9000可以同时在COCO和ImageNet数据集中进行训练，训练后的模型可以实现多达9000种物体的实时检测。 时间：CVPR 2017(YOLO v2),Arxiv 2018(YOLO v3) 作者：Joseph Redmon, Ali Farhadi 论文：YOLO9000: Better, Faster, Stronger 代码：yolo_tensorflow 源码分析参考：YOLOv2源码分析(c版) 博客参考 YOLO升级版：YOLOv2和YOLO9000解析 在Python 3中使用YOLOv2 基本思路12- 神经网络：将416 × 416 × 3的图片计算为一个13 × 13 × 125的向量，该向量包含了预测的物品位置和类别信息- 检测器：将神经网络输出的向量进行“解码”操作，输出物品的分类和位置信息。 神经网络设计 输入shape：416 × 416 × 3 输出shape：13 × 13 × 125 YOLOv2的神经网络部分使用了一个带跳层的神经网络，具体结构如下所示： 输入尺寸变为416 × 416 × 3，识别更高分辨率的图片。 每个卷积层后添加了批标准化层，加速了网络的收敛。 (跳层)在第16层开始分为两条路径，将低层的特征直接连接到高层，可提高模型性能。 移除全连接层，使用卷积层预测框的偏移量，最终的输出向量中保存了原来的位置信息。 检测规则 YOLOv2使用了Anchor Box的方法，神经网络输出的向量尺寸是13 × 13 × 125，其中13 × 13是将图片划分为13行和13列共169个cell，每个cell预测125个数据。对于每个cell的125个数据，分解为125 = 5 × (5+20)，即每个cell包括5个anchor box(预测框)，每个anchor box(预测框)包括25个数据，分别为物体存在置信度，物体中心位置(x,y)，物体尺寸(w,h)和类别信息（20个）。如下图所示： 对于每个cell包括5个anchor box信息，每个anchor box包括25个数据，分别： 为是否有物品（1个）：表示位于第i,j的cell中第K个anchor box中有物品的置信度(标签值为cell预测框与gtbox进行IOU的值) 物品位置（4个）：物品位置(x,y,w,h)与物品位置中心点和尺寸的关系：bx=f(xijk)+cx by=f(yijk)+cy bw=pwewijk bh=phehijkbx=f(xijk)+cx by=f(yijk)+cy bw=pwewijk bh=phehijk 其中，xijk,yijk,wijk,hijkxijk,yijk,wijk,hijk为网络为边界框anchor box预测4个坐标，cx,cycx,cy为单元格从图像的左上角偏移,pw,bhpw,bh为之前的边界框anchor box具有宽度和高度 物体种类（20个）：softmax计算每一种物体的概率值 每个cell预测5个anchor box，这5个anchor box有不同的预设尺寸，该预设尺寸可以手动指定也可以在训练集上训练获得。在YOLOv2中，预设尺寸是通过在测试集上进行类聚获得的。模型训练 神经网络部分基于模型Darknet-19，该模型的训练部分分为两个部分：预训练和训练部分 预训练：预训练是在ImageNet上按分类的方式进行预训练160轮，使用SGD优化方法，初始学习率0.1，每次下降4倍，到0.0005时终止。除了训练224x224尺寸的图像外，还是用448x448尺寸的图片。 训练：去除Darknet的最后一个卷积层，并将网络结构修改为YOLOv2的网络，在VOC数据集上进行训练。训练使用的代价函数是MSE代价函数。 在训练过程中，还引入了多尺寸训练，由于网络删除了全连接层，所以该网络并不关心图片的具体大小，训练时使用320~608尺寸的图像{320,352，….，608}。YOLO v3简介 YOLOv3在YOLOv2的基础进行了一些改进，这些更改使其效果变得更好。 在320×320的图像上，YOLOv3运行速度达到了22.2毫秒，mAP为28.2。其与SSD一样准确，但速度快了三倍 时间：Arxiv 2018(YOLO v3) 作者：Joseph Redmon, Ali Farhadi 论文：YOLOv3: An Incremental Improvement 代码： darknet keras-yolo3 tensorflow-yolov3 官网：darknet 源码分析参考：YOLO v3源码解读 博客参考 YOLOv3论文中文解读 yolo系列之yolo v3【深度解析】 相关博客 知乎-计算机视觉论文速递 YOLO-LITE 网络结构图 主要改进特性 网络结构改变：网络的结构由Darknet-19变为Darknet-53，跳层的现象越来越普遍。 多尺度预测：输出3层，每层 S × S个网格，分别为 13×13 ，26 ×26 ，52×52 小尺度：（13×13的feature map）网络接收一张（416×416）的图，经过5个步长为2的卷积来进行降采样(416 / 2ˆ5 = 13),输出（13×13×512），再经过7个卷积得到第一个特征图谱，在这个特征图谱上做第一次预测。 中尺度: （26×26的feature map）从小尺度中从后向前获得倒数第3个卷积层的输出，进行一次卷积一次x2上采样，将上采样特征(26×26×256)与第43个卷积特征(26×26×512)连接，输出(26×26×728),经过7个卷积得到第二个特征图谱(26×26×255)，在这个特征图谱上做第二次预测。 大尺度：（52×52的feature map）操作同中尺度,从后向前获得倒数第3个卷积层的输出，进行一次卷积一次x2上采样，将上采样特征与第26个卷积特征连接，经过7个卷积得到第三个特征图谱，在这个特征图谱上做第三次预测。输出（52×52×255） 好处：让网络同时学习到深层和浅层的特征，通过叠加浅层特征图特征到相邻通道，类似于FPN中的umsample+concat。这个方法把26x26x512的特征图叠加13x13x256的特征图，使模型有了细粒度特征,增加对小目标的识别能力 anchor box:yolov3 anchor box一共有9个，由k-means聚类得到。在COCO数据集上，9个聚类是：（10×13）;（16×30）;（33×23）;（30×61）;（62×45）; （59×119）; （116×90）; （156×198）; （373×326）。不同尺寸特征图对应不同大小的先验框。 13×13尺度的anchor box【（116×90），（156×198），（373×326）】 26×26尺度的anchor box【（30×61），（62×45），（59×119）】 52×52尺度的anchor box【（10×13），（16×30），（33×23）】 原因：(越精细的grid cell就可以检测出越精细的物体)尺度越大，感受野越小，对小物体越敏感，所以选择小的anchor box 边框预测：预测tx ty tw th，使用sigmoid对Objectness和Classes confidence进行sigmoid得到0~1的概率，之所以用sigmoid取代之前版本的softmax，原因是softmax会扩大最大类别概率值而抑制其他类别概率值 (tx,ty):目标中心点相对于该点所在网格左上角的偏移量，经过sigmoid归一化。即值属于【0,1】。如图约（0.3 , 0.4） (cx,cy):该点所在网格的左上角距离最左上角相差的格子数。如图（1,1） (pw,ph):anchor box 的边长 (tw,th):预测边框的宽和高 PS：最终得到的边框坐标值是bx,by,bw,bh.而网络学习目标是tx,ty,tw,th 损失函数LOSS：YOLO V3把YOLOV2中的Softmax loss变成Logistic loss YOLO DeepLearning 目标检测 YOLO]]></content>
      <categories>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>One Stage</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SSP Net]]></title>
    <url>%2F%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%2FSSP%20Net%2F</url>
    <content type="text"><![CDATA[SSP NetSPP-net[4,19]是MSRA何恺明等人提出的，其主要思想是去掉了原始图像上的crop/warp等操作，换成了在卷积特征上的空间金字塔池化层（Spatial Pyramid Pooling，SPP），如图7所示。为何要引入SPP层，主要原因是CNN的全连接层要求输入图片是大小一致的，而实际中的输入图片往往大小不一，如果直接缩放到同一尺寸，很可能有的物体会充满整个图片，而有的物体可能只能占到图片的一角。传统的解决方案是进行不同位置的裁剪，但是这些裁剪技术都可能会导致一些问题出现，比如图7中的crop会导致物体不全，warp导致物体被拉伸后形变严重，SPP就是为了解决这种问题的。SPP对整图提取固定维度的特征，再把图片均分成4份，每份提取相同维度的特征，再把图片均分为16份，以此类推。可以看出，无论图片大小如何，提取出来的维度数据都是一致的，这样就可以统一送至全连接层了。SPP思想在后来的R-CNN模型中也被广泛用到。SPP-net的网络结构如图8所示，实质是最后一层卷积层后加了一个SPP层，将维度不一的卷积特征转换为维度一致的全连接输入。SPP-net做目标检测的主要步骤为： 候选区域：用Selective Search从原图中生成2000个左右的候选窗口； 区域大小缩放：SPP-net不再做区域大小归一化，而是缩放到min(_w_, _h_)=_s_，即统一长宽的最短边长度，_s_选自{480,576,688,864,1200}中的一个，选择的标准是使得缩放后的候选框大小与224×224最接近； 特征提取：利用SPP-net网络结构提取特征； 分类与回归：类似R-CNN，利用SVM基于上面的特征训练分类器模型，用边框回归来微调候选框的位置。 SPP-net解决了R-CNN候选区域时crop/warp带来的偏差问题，提出了SPP层，使得输入的候选框可大可小，但其他方面依然和R-CNN一样，因而依然存在不少问题，这就有了后面的Fast R-CNN。]]></content>
      <categories>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>Two Stage</tag>
        <tag>RCNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cuda]]></title>
    <url>%2F%E7%A1%AC%E4%BB%B6%E5%8A%A0%E9%80%9F%2Fcuda%2F</url>
    <content type="text"><![CDATA[123456789int dev = 0;cudaDeviceProp devProp;(cudaGetDeviceProperties(&amp;devProp, dev));//CHECKstd::cout &lt;&lt; &quot;使用GPU device &quot; &lt;&lt; dev &lt;&lt; &quot;: &quot; &lt;&lt; devProp.name &lt;&lt; std::endl;std::cout &lt;&lt; &quot;SM的数量：&quot; &lt;&lt; devProp.multiProcessorCount &lt;&lt; std::endl;std::cout &lt;&lt; &quot;每个线程块的共享内存大小：&quot; &lt;&lt; devProp.sharedMemPerBlock / 1024.0 &lt;&lt; &quot; KB&quot; &lt;&lt; std::endl;std::cout &lt;&lt; &quot;每个线程块的最大线程数：&quot; &lt;&lt; devProp.maxThreadsPerBlock &lt;&lt; std::endl;std::cout &lt;&lt; &quot;每个EM的最大线程数：&quot; &lt;&lt; devProp.maxThreadsPerMultiProcessor &lt;&lt; std::endl;std::cout &lt;&lt; &quot;每个EM的最大线程束数：&quot; &lt;&lt; devProp.maxThreadsPerMultiProcessor / 32 &lt;&lt; std::endl; 123456789101112131415161718192021222324__global__ void add(int a, int b, int *c) &#123; *c = a + b;&#125;int main() &#123; int c; int *dev_c; cudaMalloc((void**)&amp;dev_c, sizeof(int)); add &lt;&lt; &lt;1, 1 &gt;&gt; &gt;(2, 7, dev_c); cudaMemcpy(&amp;c, dev_c, sizeof(int), cudaMemcpyDeviceToHost); printf(&quot;2 + 7 = %d&quot;, c); return 0;&#125;__global__ void kernel() &#123; printf(&quot;hello world&quot;);&#125;int main() &#123; kernel &lt;&lt; &lt;1, 1 &gt;&gt; &gt;(); cudaDeviceSynchronize(); return 0;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121// 两个向量加法kernel，grid和block均为一维__global__ void add(float* x, float * y, float* z, int n)&#123; // 获取全局索引 int index = threadIdx.x + blockIdx.x * blockDim.x; // 步长 int stride = blockDim.x * gridDim.x; for (int i = index; i &lt; n; i += stride) &#123; z[i] = x[i] + y[i]; &#125;&#125;//int main() &#123;// int N = pow(2, 20);// int V = 1 &lt;&lt; 20;//// std::cout &lt;&lt; N &lt;&lt; &quot;,&quot; &lt;&lt; V &lt;&lt; std::endl;//&#125;int main()&#123; int N = 1 &lt;&lt; 20; int nBytes = N * sizeof(float); // 申请host内存 float *x, *y, *z; x = (float*)malloc(nBytes); y = (float*)malloc(nBytes); z = (float*)malloc(nBytes); // 初始化数据 for (int i = 0; i &lt; N; ++i) &#123; x[i] = 10.0; y[i] = 20.0; &#125; // 申请device内存 float *d_x, *d_y, *d_z; cudaMalloc((void**)&amp;d_x, nBytes); cudaMalloc((void**)&amp;d_y, nBytes); cudaMalloc((void**)&amp;d_z, nBytes); // 将host数据拷贝到device cudaMemcpy((void*)d_x, (void*)x, nBytes, cudaMemcpyHostToDevice); cudaMemcpy((void*)d_y, (void*)y, nBytes, cudaMemcpyHostToDevice); // 定义kernel的执行配置 dim3 blockSize(256); dim3 gridSize((N + blockSize.x - 1) / blockSize.x); // 执行kernel add &lt;&lt; &lt; gridSize, blockSize &gt;&gt; &gt;(d_x, d_y, d_z, N); // 将device得到的结果拷贝到host cudaMemcpy((void*)z, (void*)d_z, nBytes, cudaMemcpyHostToDevice); // 检查执行结果 float maxError = 0.0; for (int i = 0; i &lt; N; i++) maxError = fmax(maxError, fabs(z[i] - 30.0)); std::cout &lt;&lt; &quot;最大误差: &quot; &lt;&lt; maxError &lt;&lt; std::endl; // 释放device内存 cudaFree(d_x); cudaFree(d_y); cudaFree(d_z); // 释放host内存 free(x); free(y); free(z); return 0;&#125;//=============================================================================================#include &quot;cuda_runtime.h&quot;#include &quot;device_launch_parameters.h&quot;#include &lt;stdlib.h&gt;#include &lt;iostream&gt;using namespace std;//将数组A与B中的元素相加存入数组C中__global__ void arrayAdd(int *A, int *B, int *C)&#123; int i = threadIdx.x; C[i] = A[i] + B[i];&#125;int main()&#123; //简单的示例，A、B、C均为长度为4的整型数组 //定义主机端数组A、B、C int A[4] = &#123; 1, 2, 3, 4 &#125;; int B[4] = &#123; 2, 3, 4, 5 &#125;; int C[4]; //定义设备端数组d_a、d_b、d_c int *d_a; int *d_b; int *d_c; //定义他们在设备端的空间大小 int size = 4 * sizeof(int); //在GPU中为他们开辟分配对应的显存空间 cudaMalloc((void**)&amp;d_a, size); cudaMalloc((void**)&amp;d_b, size); cudaMalloc((void**)&amp;d_c, size); //利用cudaMemcpy函数在CPU端A,B的值复制到对应的GPU内存中 cudaMemcpy(d_a, A, size, cudaMemcpyHostToDevice); cudaMemcpy(d_b, B, size, cudaMemcpyHostToDevice); //调用编写好的内核程序，实现数组相加功能 arrayAdd &lt;&lt; &lt;1, 4 &gt;&gt; &gt;(d_a, d_b, d_c); //利用cudaMemcpy函数将GPU端计算结果复制到CPU端 cudaMemcpy(C, d_c, size, cudaMemcpyDeviceToHost); //释放所有的GPU内存（切记有借有还，不然后果自负。。。） cudaFree(d_a); cudaFree(d_b); cudaFree(d_c); system(&quot;Pause&quot;); return 0;&#125; 使用nvprof工具可以分析kernel运行情况，结果如下所示，可以看到kernel函数费时约1.5ms。 12345678nvprof cuda9.exe==7244== NVPROF is profiling process 7244, command: cuda9.exe最大误差: 4.31602e+008==7244== Profiling application: cuda9.exe==7244== Profiling result: Type Time(%) Time Calls Avg Min Max Name GPU activities: 67.57% 3.2256ms 2 1.6128ms 1.6017ms 1.6239ms [CUDA memcpy HtoD] 32.43% 1.5478ms 1 1.5478ms 1.5478ms 1.5478ms add(float*, float*, float*, int) 在上面的实现中，我们需要单独在host和device上进行内存分配，并且要进行数据拷贝，这是很容易出错的。好在CUDA 6.0引入统一内存（Unified Memory）来避免这种麻烦，简单来说就是统一内存使用一个托管内存来共同管理host和device中的内存，并且自动在host和device中进行数据传输。CUDA中使用cudaMallocManaged函数分配托管内存： 1cudaError_t cudaMallocManaged(void **devPtr, size_t size, unsigned int flag=0); 123456789101112131415161718192021222324252627282930313233343536373839int main()&#123; int N = 1 &lt;&lt; 20; int nBytes = N * sizeof(float); // 申请托管内存 float *x, *y, *z; cudaMallocManaged((void**)&amp;x, nBytes); cudaMallocManaged((void**)&amp;y, nBytes); cudaMallocManaged((void**)&amp;z, nBytes); // 初始化数据 for (int i = 0; i &lt; N; ++i) &#123; x[i] = 10.0; y[i] = 20.0; &#125; // 定义kernel的执行配置 dim3 blockSize(256); dim3 gridSize((N + blockSize.x - 1) / blockSize.x); // 执行kernel add &lt;&lt; &lt; gridSize, blockSize &gt;&gt; &gt;(x, y, z, N); // 同步device 保证结果能正确访问 cudaDeviceSynchronize(); // 检查执行结果 float maxError = 0.0; for (int i = 0; i &lt; N; i++) maxError = fmax(maxError, fabs(z[i] - 30.0)); std::cout &lt;&lt; &quot;最大误差: &quot; &lt;&lt; maxError &lt;&lt; std::endl; // 释放内存 cudaFree(x); cudaFree(y); cudaFree(z); return 0;&#125; https://bbs.csdn.net/topics/390798229https://blog.csdn.net/fishseeker/article/details/75093166https://blog.csdn.net/xiaohu2022/article/details/79599947]]></content>
      <categories>
        <category>硬件加速</category>
      </categories>
      <tags>
        <tag>标签1</tag>
        <tag>标签2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MASK RCNN]]></title>
    <url>%2F%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%2Fmaskrcnn%20%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB%2F</url>
    <content type="text"><![CDATA[maskrcnn 源码解读 maskrcnn-benchmark 训练/测试123python3 tools/train_net.py --config-file "configs/e2e_mask_rcnn_R_101_FPN_1x.yaml" --skip-testpython3 tools/test_net.py --config-file "configs/e2e_mask_rcnn_R_101_FPN_1x.yaml" 使用 tensorboard mmdetection 训练/测试1234python3 tools/train.py configs/mask_rcnn_r101_fpn_1x_backup0603.py --work_dir out_63_1702python3 tools/test.py configs/mask_rcnn_r101_fpn_1x_backup0603.py out_63_1702/latest.pth --out out_63_1702/result.pkl --eval bboxpython3 tools/test.py configs/mask_rcnn_r101_fpn_1x_backup0603.py OutPutDir/xian_train6_14_epoch100_batchsize2/latest.pth --out OutPutDir/xian_train6_14_epoch100_batchsize2/result.pkl --eval bbox mmdetection 配置文件详解 配置 mmdetection:123456cd ~/tools/github_src/mmdetection/mmdetectionsource activate mmdetectioncd ~/tools/github_src/mmdetection/mmdetection &amp;&amp; source activate mmdetection./compile.shpython3 setup.py install 配置文件查看gpu占用watch -n 0.1 nvidia-smi ### 源码阅读tools/ train.py1model = build_detector(cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg) mmdet/ models/ builder.py12def build_detector(cfg, train_cfg=None, test_cfg=None): return build(cfg, DETECTORS, dict(train_cfg=train_cfg, test_cfg=test_cfg)) 123456def build(cfg, registry, default_args=None): if isinstance(cfg, list): modules = [_build_module(cfg_, registry, default_args) for cfg_ in cfg] return nn.Sequential(*modules) else: return _build_module(cfg, registry, default_args) 1234567891011121314151617def _build_module(cfg, registry, default_args): assert isinstance(cfg, dict) and 'type' in cfg assert isinstance(default_args, dict) or default_args is None args = cfg.copy() obj_type = args.pop('type') if mmcv.is_str(obj_type): if obj_type not in registry.module_dict: raise KeyError('&#123;&#125; is not in the &#123;&#125; registry'.format( obj_type, registry.name)) obj_type = registry.module_dict[obj_type] elif not isinstance(obj_type, type): raise TypeError('type must be a str or valid type, but got &#123;&#125;'.format( type(obj_type))) if default_args is not None: for name, value in default_args.items(): args.setdefault(name, value) return obj_type(**args) arch_settings = {&lt;br /&gt; 18: (BasicBlock, (2, 2, 2, 2)),&lt;br /&gt; 34: (BasicBlock, (3, 4, 6, 3)),&lt;br /&gt; 50: (Bottleneck, (3, 4, 6, 3)),&lt;br /&gt; 101: (Bottleneck, (3, 4, 23, 3)),&lt;br /&gt; 152: (Bottleneck, (3, 8, 36, 3))&lt;br /&gt; } densenet.pyhttps://github.com/bamos/densenet.pytorch/blob/master/train.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176# ! /usr/bin/env python# -*- coding:utf-8 -*-# @author: Edison Jia-hao-Chen# time: 2019-6-10# email: JiahaoChen@whu.edu.cn# A implementation of DenseNet (Pytorch)# https://github.com/bamos/densenet.pytorchimport loggingimport torchimport torch.nn as nn# import torch.optim as optimimport torch.nn.functional as F# from torch.autograd import Variable# import torchvision.datasets as dset# import torchvision.transforms as transforms# from torch.utils.data import DataLoader# import torchvision.models as models# import sysimport mathfrom ..registry import BACKBONESimport torch.utils.checkpoint as cpfrom mmcv.runner import load_checkpointfrom torch.nn.modules.batchnorm import _BatchNormfrom mmcv.cnn import constant_init, kaiming_initclass Bottleneck(nn.Module): def __init__(self, nChannels, growthRate): super(Bottleneck, self).__init__() interChannels = 4 * growthRate self.bn1 = nn.BatchNorm2d(nChannels) self.conv1 = nn.Conv2d(nChannels, interChannels, kernel_size=1, bias=False) self.bn2 = nn.BatchNorm2d(interChannels) self.conv2 = nn.Conv2d(interChannels, growthRate, kernel_size=3, padding=1, bias=False) def forward(self, x): out = self.conv1(F.relu(self.bn1(x))) out = self.conv2(F.relu(self.bn2(out))) out = torch.cat((x, out), 1) return outclass SingleLayer(nn.Module): def __init__(self, nChannels, growthRate): super(SingleLayer, self).__init__() self.bn1 = nn.BatchNorm2d(nChannels) self.conv1 = nn.Conv2d(nChannels, growthRate, kernel_size=3, padding=1, bias=False) def forward(self, x): out = self.conv1(F.relu(self.bn1(x))) out = torch.cat((x, out), 1) return outclass Transition(nn.Module): def __init__(self, nChannels, nOutChannels): super(Transition, self).__init__() self.bn1 = nn.BatchNorm2d(nChannels) self.conv1 = nn.Conv2d(nChannels, nOutChannels, kernel_size=1, bias=False) def forward(self, x): out = self.conv1(F.relu(self.bn1(x))) out = F.avg_pool2d(out, 2) return out@BACKBONES.register_moduleclass DenseNet(nn.Module): def __init__(self, growthRate, depth, reduction, nClasses, bottleneck): super(DenseNet, self).__init__() nDenseBlocks = (depth - 4) // 3 if bottleneck: nDenseBlocks //= 2 # 16 nChannels = 2 * growthRate # 24 self.conv1 = nn.Conv2d(3, nChannels, kernel_size=3, padding=1, bias=False) self.dense1 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck) nChannels += nDenseBlocks * growthRate # 216 nOutChannels = int(math.floor(nChannels * reduction)) # 108 self.trans1 = Transition(nChannels, nOutChannels) print('nOutChannels=&#123;&#125;'.format(nOutChannels)) nChannels = nOutChannels self.dense2 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck) nChannels += nDenseBlocks * growthRate # 300 nOutChannels = int(math.floor(nChannels * reduction)) # 150 self.trans2 = Transition(nChannels, nOutChannels) nChannels = nOutChannels self.dense3 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck) nChannels += nDenseBlocks * growthRate # self.bn1 = nn.BatchNorm2d(nChannels) self.fc = nn.Linear(nChannels, nClasses) # raise Exception('fc channels [&#123;&#125;, &#123;&#125;]'.format(nChannels, nClasses)) for m in self.modules(): if isinstance(m, nn.Conv2d): n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels m.weight.data.normal_(0, math.sqrt(2. / n)) elif isinstance(m, nn.BatchNorm2d): m.weight.data.fill_(1) m.bias.data.zero_() elif isinstance(m, nn.Linear): m.bias.data.zero_() def init_weights(self, pretrained=None): # if isinstance(pretrained, str): # logger = logging.getLogger() # load_checkpoint(self, pretrained, strict=False, logger=logger) # elif pretrained is None: # for m in self.modules(): # if isinstance(m, nn.Conv2d): # kaiming_init(m) # elif isinstance(m, (_BatchNorm, nn.GroupNorm)): # constant_init(m, 1) # if self.dcn is not None: # for m in self.modules(): # if isinstance(m, Bottleneck) and hasattr( # m, 'conv2_offset'): # constant_init(m.conv2_offset, 0) # if self.zero_init_residual: # for m in self.modules(): # if isinstance(m, Bottleneck): # constant_init(m.norm3, 0) # elif isinstance(m, BasicBlock): # constant_init(m.norm2, 0) # else: # raise TypeError('pretrained must be a str or None') print('|||========== no pretrained... =============|||') def _make_dense(self, nChannels, growthRate, nDenseBlocks, bottleneck): layers = [] for i in range(int(nDenseBlocks)): if bottleneck: layers.append(Bottleneck(nChannels, growthRate)) else: layers.append(SingleLayer(nChannels, growthRate)) nChannels += growthRate return nn.Sequential(*layers) def forward(self, x): outs = [] out = self.conv1(x) out = self.trans1(self.dense1(out)) out = self.trans2(self.dense2(out)) out = self.dense3(out) out = torch.squeeze(F.avg_pool2d(F.relu(self.bn1(out)), 8)) out = F.log_softmax(self.fc(out)) outs.append(out) return tuple(outs) https://github.com/pytorch/vision/blob/master/torchvision/models/utils.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239import reimport torchimport torch.nn as nnimport torch.nn.functional as Fimport torch.utils.checkpoint as cpfrom collections import OrderedDictfrom .densenet_util import load_state_dict_from_urlfrom ..registry import BACKBONES__all__ = ['DenseNet', 'densenet121', 'densenet169', 'densenet201', 'densenet161']model_urls = &#123; 'densenet121': 'https://download.pytorch.org/models/densenet121-a639ec97.pth', 'densenet169': 'https://download.pytorch.org/models/densenet169-b2777c0a.pth', 'densenet201': 'https://download.pytorch.org/models/densenet201-c1103571.pth', 'densenet161': 'https://download.pytorch.org/models/densenet161-8d451a50.pth',&#125;def _bn_function_factory(norm, relu, conv): def bn_function(*inputs): concated_features = torch.cat(inputs, 1) bottleneck_output = conv(relu(norm(concated_features))) return bottleneck_output return bn_functionclass _DenseLayer(nn.Sequential): def __init__(self, num_input_features, growth_rate, bn_size, drop_rate, memory_efficient=False): super(_DenseLayer, self).__init__() self.add_module('norm1', nn.BatchNorm2d(num_input_features)), self.add_module('relu1', nn.ReLU(inplace=True)), self.add_module('conv1', nn.Conv2d(num_input_features, bn_size * growth_rate, kernel_size=1, stride=1, bias=False)), self.add_module('norm2', nn.BatchNorm2d(bn_size * growth_rate)), self.add_module('relu2', nn.ReLU(inplace=True)), self.add_module('conv2', nn.Conv2d(bn_size * growth_rate, growth_rate, kernel_size=3, stride=1, padding=1, bias=False)), self.drop_rate = drop_rate self.memory_efficient = memory_efficient def forward(self, *prev_features): bn_function = _bn_function_factory(self.norm1, self.relu1, self.conv1) if self.memory_efficient and any(prev_feature.requires_grad for prev_feature in prev_features): bottleneck_output = cp.checkpoint(bn_function, *prev_features) else: bottleneck_output = bn_function(*prev_features) new_features = self.conv2(self.relu2(self.norm2(bottleneck_output))) if self.drop_rate &gt; 0: new_features = F.dropout(new_features, p=self.drop_rate, training=self.training) return new_featuresclass _DenseBlock(nn.Module): def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate, memory_efficient=False): super(_DenseBlock, self).__init__() for i in range(num_layers): layer = _DenseLayer( num_input_features + i * growth_rate, growth_rate=growth_rate, bn_size=bn_size, drop_rate=drop_rate, memory_efficient=memory_efficient, ) self.add_module('denselayer%d' % (i + 1), layer) def forward(self, init_features): features = [init_features] for name, layer in self.named_children(): new_features = layer(*features) features.append(new_features) return torch.cat(features, 1)class _Transition(nn.Sequential): def __init__(self, num_input_features, num_output_features): super(_Transition, self).__init__() self.add_module('norm', nn.BatchNorm2d(num_input_features)) self.add_module('relu', nn.ReLU(inplace=True)) self.add_module('conv', nn.Conv2d(num_input_features, num_output_features, kernel_size=1, stride=1, bias=False)) self.add_module('pool', nn.AvgPool2d(kernel_size=2, stride=2))@BACKBONES.register_moduleclass DenseNet(nn.Module): r"""Densenet-BC model class, based on `"Densely Connected Convolutional Networks" &lt;https://arxiv.org/pdf/1608.06993.pdf&gt;`_ Args: growth_rate (int) - how many filters to add each layer (`k` in paper) block_config (list of 4 ints) - how many layers in each pooling block num_init_features (int) - the number of filters to learn in the first convolution layer bn_size (int) - multiplicative factor for number of bottle neck layers (i.e. bn_size * k features in the bottleneck layer) drop_rate (float) - dropout rate after each dense layer num_classes (int) - number of classification classes memory_efficient (bool) - set to True to use checkpointing. Much more memory efficient, but slower. Default: *False* """ def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16), num_init_features=64, bn_size=4, drop_rate=0, num_classes=256, memory_efficient=False): # def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16), # num_init_features=64, bn_size=4, drop_rate=0, num_classes=1000, memory_efficient=False): super(DenseNet, self).__init__() # First convolution self.features = nn.Sequential(OrderedDict([ ('conv0', nn.Conv2d(3, num_init_features, kernel_size=7, stride=2, padding=3, bias=False)), ('norm0', nn.BatchNorm2d(num_init_features)), ('relu0', nn.ReLU(inplace=True)), ('pool0', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)), ])) # Each denseblock num_features = num_init_features for i, num_layers in enumerate(block_config): block = _DenseBlock( num_layers=num_layers, num_input_features=num_features, bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate, memory_efficient=memory_efficient ) self.features.add_module('denseblock%d' % (i + 1), block) num_features = num_features + num_layers * growth_rate if i != len(block_config) - 1: trans = _Transition(num_input_features=num_features, num_output_features=num_features // 2) self.features.add_module('transition%d' % (i + 1), trans) num_features = num_features // 2 # Final batch norm self.features.add_module('norm5', nn.BatchNorm2d(num_features)) # Linear layer self.classifier = nn.Linear(num_features, num_classes) # Official init from torch repo. for m in self.modules(): if isinstance(m, nn.Conv2d): nn.init.kaiming_normal_(m.weight) elif isinstance(m, nn.BatchNorm2d): nn.init.constant_(m.weight, 1) nn.init.constant_(m.bias, 0) elif isinstance(m, nn.Linear): nn.init.constant_(m.bias, 0) def init_weights(self, pretrained=None): print('--------------- no pretrained -----------------') def forward(self, x): outs = [] features = self.features(x) out = F.relu(features, inplace=True) out = F.adaptive_avg_pool2d(out, (1, 1)).view(features.size(0), -1) out = self.classifier(out) for i in range(4): outs.append(out) return tuple(out)def _load_state_dict(model, model_url, progress): # '.'s are no longer allowed in module names, but previous _DenseLayer # has keys 'norm.1', 'relu.1', 'conv.1', 'norm.2', 'relu.2', 'conv.2'. # They are also in the checkpoints in model_urls. This pattern is used # to find such keys. pattern = re.compile( r'^(.*denselayer\d+\.(?:norm|relu|conv))\.((?:[12])\.(?:weight|bias|running_mean|running_var))$') state_dict = load_state_dict_from_url(model_url, progress=progress) for key in list(state_dict.keys()): res = pattern.match(key) if res: new_key = res.group(1) + res.group(2) state_dict[new_key] = state_dict[key] del state_dict[key] model.load_state_dict(state_dict)def _densenet(arch, growth_rate, block_config, num_init_features, pretrained, progress, **kwargs): model = DenseNet(growth_rate, block_config, num_init_features, **kwargs) if pretrained: _load_state_dict(model, model_urls[arch], progress) return modeldef densenet121(pretrained=False, progress=True, **kwargs): r"""Densenet-121 model from `"Densely Connected Convolutional Networks" &lt;https://arxiv.org/pdf/1608.06993.pdf&gt;`_ Args: pretrained (bool): If True, returns a model pre-trained on ImageNet progress (bool): If True, displays a progress bar of the download to stderr """ return _densenet('densenet121', 32, (6, 12, 24, 16), 64, pretrained, progress, **kwargs)def densenet161(pretrained=False, progress=True, **kwargs): r"""Densenet-161 model from `"Densely Connected Convolutional Networks" &lt;https://arxiv.org/pdf/1608.06993.pdf&gt;`_ Args: pretrained (bool): If True, returns a model pre-trained on ImageNet progress (bool): If True, displays a progress bar of the download to stderr """ return _densenet('densenet161', 48, (6, 12, 36, 24), 96, pretrained, progress, **kwargs)def densenet169(pretrained=False, progress=True, **kwargs): r"""Densenet-169 model from `"Densely Connected Convolutional Networks" &lt;https://arxiv.org/pdf/1608.06993.pdf&gt;`_ Args: pretrained (bool): If True, returns a model pre-trained on ImageNet progress (bool): If True, displays a progress bar of the download to stderr """ return _densenet('densenet169', 32, (6, 12, 32, 32), 64, pretrained, progress, **kwargs)def densenet201(pretrained=False, progress=True, **kwargs): r"""Densenet-201 model from `"Densely Connected Convolutional Networks" &lt;https://arxiv.org/pdf/1608.06993.pdf&gt;`_ Args: pretrained (bool): If True, returns a model pre-trained on ImageNet progress (bool): If True, displays a progress bar of the download to stderr """ return _densenet('densenet201', 32, (6, 12, 48, 32), 64, pretrained, progress, **kwargs)]]></content>
      <categories>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>Two Stage</tag>
        <tag>RCNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MMdet 可视化]]></title>
    <url>%2F%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%A1%86%E6%9E%B6%2Fmmdet%20%E6%B5%8B%E8%AF%95%E7%BB%93%E6%9E%9C%E5%8F%AF%E8%A7%86%E5%8C%96%2F</url>
    <content type="text"><![CDATA[mmdet 测试结果可视化123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167# ! /usr/bin/env python# -*- coding:utf-8 -*-# @author: Edison Jia-hao-Chen# time: 2019-6-28# email: JiahaoChen@whu.edu.cn# Draw Boximport osimport jsonimport cv2import numpy as npimport collections as collectfile_path = ['/home/edison/tools/datasets/zmvision-datasets/newDatasets_Phd_He_0628/changguang/dataset0628/', '/home/edison/tools/github_src/mmdetection/mmdetection/OutPutDir/', 'imgfiles/test/']file_name = ['annotations/changguang0628LAB_he_test_0701.json', 'cg-epoch300-dense121_0703_2311/result.pkl.json']coco_keys = ('image_id', 'bbox', 'score', 'category_id', 'segmentation')line_format = ('x', 'y', 'w', 'h', 'score', 'category_id')class DrawBox(object): def __init__(self, png_path=file_path[0]+file_path[2], ret_path=file_path[1], ret_file=file_name[1], gt_path=file_path[0], gt_file=file_name[0]): self.png_path = png_path self.det_ret_file = ret_path + ret_file self.gt_file = gt_path + gt_file imgs_name = self._load_gt_file() pred_anns = self._load_pred_file() self.imgs_name = imgs_name self.pred_anns = pred_anns self._draw_box() @staticmethod def read_files(_file_path, file_type='.json'): file_list = os.listdir(_file_path) file_list.sort() file_list = [file for file in file_list if file.endswith(file_type)] return file_list @staticmethod def imshow_withbox(_img_name, anns=[0, 0, 0, 0, 0, 1]): if not len(anns) % len(line_format) == 0: raise Exception('Please Check anns &#123;&#125;'.format(anns)) img = cv2.imread(_img_name) font = cv2.FONT_HERSHEY_SIMPLEX len_anns = len(anns) / len(line_format) x = len(line_format) * np.arange(len_anns) x = np.array(x, int) flag_wait8 = False for idx in x: label = '' # segment # points = np.array([[round(anns[idx], anns[idx + 1]], # [anns[idx] + anns[idx + 2], anns[idx + 1] + anns[idx + 3]]], # np.int32) # points = points.reshape((-1, 1, 2)) cv2.rectangle(img, (round(anns[idx]), round(anns[idx + 1])), (round(anns[idx]) + round(anns[idx + 2]), round(anns[idx + 1]) + round(anns[idx + 3])), (0, 0, 255), thickness=2) # 1 if anns[idx + 5] == 1: label = 'ship' label += ' ' + '%.2f' % (100 * anns[idx + 4]) + '%' y = anns[idx + 1] - 1 if anns[idx + 1] - 1 &lt; 0: y = anns[idx + 2] + 1 y = round(y) cv2.putText(img, label, (round(anns[idx]), y), font, 0.3, (255, 255, 255), 1) if anns[4 + idx] &lt; 0.5: flag_wait8 = True cv2.imshow(_img_name, img) if flag_wait8 is True: # cv2.waitKey(8000) pass00 else: cv2.imshow(_img_name, img) cv2.waitKey(1000) cv2.destroyWindow(_img_name) # cv2.destroyWindow(_img_name) # @staticmethod # def load_txts(_file_name, file_type='.txt'): # ret_ann = [] # with open(_file_name, 'r') as f: # for line in f.readlines(): # ret_ann.append(line.rstrip('\n')) # return ret_ann# @property def _load_pred_file(self): with open(self.det_ret_file, 'r') as f: ret_anns = json.loads(f.read()) coco_keys = ('image_id', 'bbox', 'score', 'category_id', 'segmentation') ann_ret = dict() img_id = None # 初始 image id for ann in ret_anns: if not coco_keys == tuple(ann.keys()): raise Exception('Keys are not matching. Please check.&#123;&#125;'.format(ann.keys())) list_tmp = [] list_tmp.extend(ann['bbox']) list_tmp.append(ann['score']) list_tmp.append(ann['category_id']) if ann['image_id'] == img_id: ann_ret[img_id].extend(list_tmp) else: ann_ret.update(&#123;ann['image_id']: list_tmp&#125;) img_id = ann['image_id'] return collect.OrderedDict(ann_ret) def _load_gt_file(self): with open(self.gt_file, 'r') as f: json_list = json.loads(f.read()) imgs_name = dict() img_id = None # 初始 image id print((json_list.keys())) print(json_list['images']) for imgs in json_list['images']: if imgs['id'] == img_id: imgs_name[img_id] = imgs['file_name'] else: imgs_name.update(&#123;imgs['id']: imgs['file_name']&#125;) img_id = imgs['id'] return collect.OrderedDict(imgs_name) def _draw_box(self): png_list = DrawBox.read_files(self.png_path, '.png') for (img_id, anns_box) in self.imgs_name.items(): # print(self.png_path + self.imgs_name[img_id]) print('fuck', self.pred_anns[img_id]) ann = [] # for item in self.pred_anns[img_id]: # ann.append(round(item)) DrawBox.imshow_withbox(self.png_path + self.imgs_name[img_id], self.pred_anns[img_id])if __name__ == '__main__': drow = DrawBox() # print(type(drow.pred_anns)) # for (k, v) in drow.pred_anns.items(): # print(k, v)]]></content>
      <categories>
        <category>目标检测框架</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>检测框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习入门]]></title>
    <url>%2F%E5%85%A5%E9%97%A8%E8%B5%84%E6%96%99%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[深度学习入门 仅作为个人学习笔记，无任何商业用途，转载请注明。联系作者：JiahaoChen@whu.edu.cn时间：2019/2/18 日常心得【心得2019/2/18】 今天收获蛮大，看了 torch 教程的线性回归和感知机部分，并且手动在 pycharm 和 jupyter notebook 上撸了一遍代码，对神经网络的写法有了很大的领悟。 但是看书的时候，发现很多东西只知道这样调用，却不知道具体怎么做的，于是开始了查博客、看西瓜书等…一系列操作。一下子将梯度下降法弄的差不多了。 发现 torch 很方便，而且要注意：网上的代码很多是早期的torch版本；然而我用的是 0.4.1 版本，用法有一些变化，具体会在下面整理。 神经网络的学习过程，就是根据训练数据来调整神经元之间的“连接权”以及每个功能神经元的阈值。 知识点总结 代码Error或Warning的解决方法 size_average=False 替代成 **reduction=&#39;sum&#39;** **12345# torch0.3中loss_fn = nn.MSELoss(size_average=False)# torch0.4中loss_fn = nn.MSELoss(reduction='sum') **lose.data[0]** 替换成 loss.item() 123# 显示损失if (t + 1) % 50 == 0: print(loss.item()) 构建神经网络步骤 构建好网络模型 参数初始化 前向传播 计算损失 反向传播计算梯度 更新权重 线性回归模板12345678class LinearRegression(nn.Module): def __init__(self): super(LinearRegression, self).__init__() self.linear = nn.Linear(1, 1) # y = w*x + b def forward(self, x): ret = self.linear(x) return ret 神经网络通用模板12345678910class net_name(nn.Module): def __init__(self): super(net_name, self).__init__() # 可添加子网络 self.conv1 = nn.Conv2d(3, 10, 3) def forward(self, x): ret = self.conv1(x) return ret 神经网络实例（1）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869import torchimport torch.nn as nnfrom torch.autograd import Variableclass TwoLayerNet(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(TwoLayerNet, self).__init__() ''' 关于 nn.Sequential见 “实例（1）备注” ''' self.twolayernet = nn.Sequential( nn.Linear(input_size, hidden_size), nn.ReLU(), nn.Linear(hidden_size, output_size), ) def forward(self, x): ''' 在forward函数中，我们会接受一个Variable，然后我们也会返回一个Varible ''' y_pred = self.twolayernet(x) return y_pred''''''''''''''# M是样本数量，input_size是输入层大小# hidden_size是隐含层大小，output_size是输出层大小M, input_size, hidden_size, output_size = 64, 1000, 100, 10# 生成随机数当作样本，同时用Variable 来包装这些数据，设置 requires_grad=False 表示在方向传播的时候，# 我们不需要求这几个 Variable 的导数x = Variable(torch.randn(M, input_size))y = Variable(torch.randn(M, output_size))model = TwoLayerNet(input_size, hidden_size, output_size)# 定义损失函数loss_fn = nn.MSELoss(reduction='sum')'''''''设置超参数'''''''learning_rate = 1e-4EPOCH = 300# 使用optim包来定义优化算法，可以自动的帮我们对模型的参数进行梯度更新。这里我们使用的是随机梯度下降法。# 第一个传入的参数是告诉优化器，我们需要进行梯度更新的Variable 是哪些，# 第二个参数就是学习速率了。optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)'''''''开始训练'''''''for t in range(EPOCH): # 向前传播 y_pred = model(x) # 计算损失 loss = loss_fn(y_pred, y) # 显示损失 if (t + 1) % 50 == 0: print(loss.item()) # 在我们进行梯度更新之前，先使用optimier对象提供的清除已经积累的梯度。 optimizer.zero_grad() # 计算梯度 loss.backward() # 更新梯度 optimizer.step() 实例（1）备注 使用 nn 包的 Sequential 来快速构建模型，Sequential可以看成一个组件的容器。它涵盖神经网络中的很多层，并将这些层组合在一起构成一个模型之后，我们输入的数据会按照这个Sequential的流程进行数据的传输，最后一层就是输出层。默认会帮我们进行参数初始化。 12345model = nn.Sequential( nn.Linear(input_size, hidden_size), nn.ReLU(), nn.Linear(hidden_size, output_size),) 神经网络实例（2）注意：代码不完整，待继续补充。 12345678910111213141516171819202122232425262728293031323334353637# -*- coding: utf-8 -*-class NeuralNetwork(nn.Module): # 跟之前一样的框架，只不过多了神经网络的层数定义 def __init__(self, in_dim, n_hidden_1, n_hidden_2, out_dim): super(NeuralNetwork, self).__init__() self.layer1 = nn.Linear(in_dim, n_hidden_1) self.layer2 = nn.Linear(n_hidden_1, n_hidden_2) self.layer3 = nn.Linear(n_hidden_2, out_dim) def forward(self, x): x = self.layer1(x) x = self.layer2(x) x = self.layer3(x) return x# 图片尺寸：28*28# 隐藏层大小： 300，100# 10分类model = NeuralNetwork(28*28, 300, 100, 10)# GPU 加速if torch.cuda.is_available(): model = model.cuda()'''定义loss'''# 使用交叉熵criterion = nn.CrossEntropyLoss()# 最小二乘法：criterion = nn.MSELoss()'''定义optimizer'''# lr: learning-rateoptimizer = torch.optim.SGD(model.parameters(), lr=1e-4)'''开始训练'''# TODO 公式理解 西瓜书笔记 权重w，阈值θ，学习率η 概念理解 激活函数 Sigmoid函数 公式： ![](https://cdn.nlark.com/yuque/0/2019/png/220248/1550487698436-efd2bfc2-5a10-441f-97c7-e8ae58cf1de7.png#align=left&amp;display=inline&amp;height=39&amp;originHeight=35&amp;originWidth=99&amp;size=0&amp;status=done&amp;width=109) &lt;br /&gt;图像：&lt;br /&gt; ![](https://cdn.nlark.com/yuque/0/2019/png/220248/1550487751644-62c420f2-199a-4f18-970f-86679533dbe0.png#align=left&amp;display=inline&amp;height=247&amp;originHeight=400&amp;originWidth=400&amp;size=0&amp;status=done&amp;width=247) 性质： 感知机公式： 性质： 只有输出层神经元进行激活函数处理。 适用于线性可分问题。 多层前馈神经网络前馈，并不意味着不能后传；而是指网络拓扑结构上不存在环或者回路。 BP 算法（error Back Propagation）误差逆传播算法，或“反向传播算法” 手动笔记 梯度下降和反向求导 关于反向求导以 为例，对 b 进行求导前向求导： 反向求导： 区别一看便知，后向求导，不仅求出了对b的偏导数，还求出了对a的偏导数。 关于梯度下降（Gradient descent） 对于 为一个够小数值时成立，那么 附录优化器选择 torch.optim UFLDL教程 十分钟看懂神经网络]]></content>
      <categories>
        <category>入门资料</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习环境配置]]></title>
    <url>%2F%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[在windows平台下 一、使用 powershell 激活 python 环境进行一下操作1）安装库： conda install -n root -`c pscondaenvs pscondaenvs&lt;br /&gt;2）更改PowerShell配置：以管理员身份启动PowerShell，并执行Set-ExecutionPolicy RemoteSigned` 二、安装显卡驱动和 cuda 相关1）安装显卡驱动： 390.77-desktop-win10-64bit-international-whql.exe2）安装cuda：cuda_8.0.61_win10.exe3）安装cuda补丁：cuda_8.0.61.2_windows.exe4）配置cudnn：将cudnn文件夹的三个文件拷贝到cuda相应路径下5）配置环境变量：CUDA_PATH=C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v8.0；CUDA_PATH_V7_5 = C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v8.0；CUDA_SDK_PATH = C:\ProgramData\NVIDIA Corporation\CUDA Samples\v8.0；CUDA_LIB_PATH = %CUDA_PATH%\lib\x64CUDA_BIN_PATH = %CUDA_PATH%\bin；CUDA_SDK_BIN_PATH = %CUDA_SDK_PATH%\bin\win64；CUDA_SDK_LIB_PATH = %CUDA_SDK_PATH%\common\lib\x64在 PATH 里添加：%CUDA_LIB_PATH%%CUDA_BIN_PATH%%CUDA_SDK_BIN_PATH%%CUDA_SDK_LIB_PATH%C:\Program Files\NVIDIA Corporation\NVSMI6）检查显卡驱动nvidia-smi7）检查cudanvcc -V 三、让 Jupyter-notebook 选择环境conda install ipykernelpython -m ipykernel install --user --name 环境名称 --display-name &quot;Python (环境名称)&quot; 四、推荐的 Anaconda版本为 4.5.4清华源Anaconda版本https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/ 在ubuntu平台下 ubuntu-18.04NVIDIA 1060 6GB 一、安装显卡驱动和 cuda 相关 ubuntu 16.04 安装nvdia驱动 驱动安装方法一：使用 .run 安装驱动 1）禁用 nouveau 驱动sudo bash -c “echo blacklist nouveau &gt; /etc/modprobe.d/blacklist-nvidia-nouveau.conf”sudo bash -c “echo options nouveau modeset=0 &gt;&gt; /etc/modprobe.d/blacklist-nvidia-nouveau.conf” 2）关闭图形界面 若无法进入 tty， 则执行下面步骤 编辑 /etc/default/grub sudo gedit /etc/default/grub 修改 GRUB_CMDLINE_LINUX_DEFAULT 的值由“quiet splash”为”nomodest” 更新grub：sudo update-grub 重启 按Ctrl+Alt+f2进入tty2关闭x服务： sudo service lightdm stop 卸载nvidia驱动： 123sudo apt-get remove --purge nvidia*sudo apt-get autoremovesudo apt-get clean 更新 kernel source 12apt-get install linux-sourceapt-get install linux-headers-x.x.x-x-generic 安装 1sudo ./NVIDIA-Linux-x86_64-390.42.run -no-x-check -no-nouveau-check -no-opengl-files 开启x服务 sudo service lightdm start 驱动安装方法二：使用自带的装 1）禁用 nouveau 驱动 2）安装12ubuntu-drivers devicessudo ubuntu-drivers autoinstall cuda 安装方法一：使用 .run 安装sudo ./cuda_8.0.61_375.26_linux.run --no-opengl-libs --no-opengl-libs：表示只安装驱动文件，不安装OpenGL文件。必需参数，原因同上。注意：不是-no-opengl-files。 --uninstall (deprecated)：用于卸载CUDA Driver（已废弃）。 --toolkit：表示只安装CUDA Toolkit，不安装Driver和Samples。 --help：查看更多高级选项。 安装选项选择：123456accept #同意安装n #不安装Driver，因为已安装最新驱动y #安装CUDA Toolkit&lt;Enter&gt; #安装到默认目录y #创建安装目录的软链接n #不复制Samples，因为在安装目录下有/samples 创建环境变量：导出CUDA的bin和lib路径：export PATH=/usr/local/cuda/bin:$PATHexport LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH安装及路径测试：输入nvcc -V 查看CUDA版本。 cuda 安装方法二：使用 .deb 安装官方文档 deb cudnn 安装 驱动安装问题：运行 nvidia-smi 命令提示出错NVIDIA-SMI has failed because it couldn’t communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.这个问题比较坑，网上说法要不就是重启一下就能解决，要不就是切换内核。（实际上，我都尝试过，重启在我电脑上不靠谱，切换内核由于我安装的Ubuntu 18.04中的内核就是当前时间的最新内核，曾经考虑过是不是内核太新了，于是降级内核，然后把系统内核搞崩了。不建议尝试切换内核。）网上查找到的博客没能解决我的问题，后来去 Nvidia 官网论坛上寻找客服人员的解决方案在一个帖子上发现了重要的线索：[https://devtalk.nvidia.com/default/topic/1037021/linux/ubuntu-18-04-on-asus-zenbook-ux550ve-crashes-soon-after-boot-to-desktop/]这里面客服人员提到： 12345678"No, ubuntu just seems have an alias set to turn off the nvidia driver.rungrep nvidia /etc/modprobe.dand look for something likealias nvidia-drm offand alias nvidia-modprobe offand remove that file. " 然后在 /etc/modprobe.d/blacklist-nvidia.conf 里面发现了这个： 12345678# Do not modify# This file was generated by nvidia-primeblacklist nvidiablacklist nvidia-drmblacklist nvidia-modesetalias nvidia offalias nvidia-drm offalias nvidia-modeset off 注意后面三句话，可以说十分尴尬了这里将 nvidia 的三个模块全部禁用掉，安装过程和执行 nvidia-smi 过程在这里翻车了。解决方案就是更改此文件，将后面三句话加上注释或者直接：sudo mv blacklist-nvidia.conf blacklist-nvidia.conf.bak之后运行 nvidia-smi 就很顺利，如果你对上面的drm模块安装不放心的话也可以选择再次安装 nvidia 驱动，之后你会发现安装过程十分流畅。 官方推荐库1234567sudo apt-get install g++ freeglut3-dev build-essential libx11-dev libxmu-dev libxi-dev libglu1-mesa libglu1-mesa-dev# 其实很多你的环境已经安装了，所以一般并没有多少时间。sudo apt-get update sudo apt-get install dkms build-essential linux-headers-genericsudo apt-get install gcc-multilib xorg-devsudo apt-get install freeglut3-dev libx11-dev libxmu-dev install libxi-dev libgl1-mesa-glx libglu1-mesa libglu1-mesa-dev 二、配置gcc版本1）安装 gcc-7 1234567sudo add-apt-repository ppa:ubuntu-toolchain-r/testsudo apt-get updatesudo apt-get install gcc-7# 配置sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-7 100sudo update-alternatives --config gcc 2）安装 gcc-4.9 三、Linux 文本界面和图形界面切换123telinit 5telinit 3# 可修改/etc/initab改变默认启动方式。其中3为文本，5为图形，这里默认文本模式。命令行输入vi /etc/inittab找到/id;3;initdefault，修改成需要的值。]]></content>
      <categories>
        <category>环境配置</category>
      </categories>
      <tags>
        <tag>标签1</tag>
        <tag>标签2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Torch深度学习框架]]></title>
    <url>%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%2FTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%2F</url>
    <content type="text"><![CDATA[近期转Pytorch进行模型开发，本文为Pytorch模型开发过程中学习笔记；包含数据预处理、数据增强、模型定义、权值初始化、模型Finetune、学习率调整策略、损失函数选取、优化器选取、可视化等等.Pytorch非常适合用来做学术研究，tensorflow适合所有场景(研究，生产，移动端),caffe2适合做生产、移动端 数据预处理 Cifar10转png 下载数据集，解压 运行代码，将cifar10的data_batch_1~5 转换成 png格式的图片，每个类别单独存放在一个文件夹，文件夹名称为0-9 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162from scipy.misc import imsaveimport numpy as npimport osimport pickledata_dir = '../../Data/cifar-10-batches-py/'train_o_dir = '../../Data/cifar-10-png/raw_train/'test_o_dir = '../../Data/cifar-10-png/raw_test/'Train = False # 不解压训练集，仅解压测试集# 解压缩，返回解压后的字典def unpickle(file): fo = open(file, 'rb') dict_ = pickle.load(fo, encoding='bytes') fo.close() return dict_def my_mkdir(my_dir): if not os.path.isdir(my_dir): os.makedirs(my_dir)# 生成训练集图片，if __name__ == '__main__': if Train: for j in range(1, 6): data_path = data_dir + "data_batch_" + str(j) # data_batch_12345 train_data = unpickle(data_path) print(data_path + " is loading...") for i in range(0, 10000): img = np.reshape(train_data[b'data'][i], (3, 32, 32)) img = img.transpose(1, 2, 0) label_num = str(train_data[b'labels'][i]) o_dir = os.path.join(train_o_dir, label_num) my_mkdir(o_dir) img_name = label_num + '_' + str(i + (j - 1)*10000) + '.png' img_path = os.path.join(o_dir, img_name) imsave(img_path, img) print(data_path + " loaded.") print("test_batch is loading...") # 生成测试集图片 test_data_path = data_dir + "test_batch" test_data = unpickle(test_data_path) for i in range(0, 10000): img = np.reshape(test_data[b'data'][i], (3, 32, 32)) img = img.transpose(1, 2, 0) label_num = str(test_data[b'labels'][i]) o_dir = os.path.join(test_o_dir, label_num) my_mkdir(o_dir) img_name = label_num + '_' + str(i) + '.png' img_path = os.path.join(o_dir, img_name) imsave(img_path, img) print("test_batch loaded.") 数据集划分123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# 将原始数据集进行划分成训练集、验证集和测试集 # 按照8:1:1的比例import osimport globimport randomimport shutildataset_dir = './cifar-10-png/raw_test/'train_dir = './train/'valid_dir = './valid/'test_dir = './test/'train_per = 0.8valid_per = 0.1test_per = 0.1def makedir(new_dir): if not os.path.exists(new_dir): os.makedirs(new_dir)if __name__ == '__main__': for root, dirs, files in os.walk(dataset_dir): for sDir in dirs: #获取指定目录下的所有图片 imgs_list = glob.glob(os.path.join(root, sDir)+'/*.png') random.seed(666) random.shuffle(imgs_list) imgs_num = len(imgs_list) train_point = int(imgs_num * train_per) valid_point = int(imgs_num * (train_per + valid_per)) for i in range(imgs_num): if i &lt; train_point: out_dir = train_dir + sDir + '/' elif i &lt; valid_point: out_dir = valid_dir + sDir + '/' else: out_dir = test_dir + sDir + '/' makedir(out_dir) out_path = out_dir + os.path.split(imgs_list[i])[-1] shutil.copy(imgs_list[i], out_path) print('Class:&#123;&#125;, train:&#123;&#125;, valid:&#123;&#125;, test:&#123;&#125;'.format(sDir, train_point, valid_point-train_point, imgs_num-valid_point)) 制作标签文本制作存放有图片路径及其标签的txt，Pytotch依据该txt上的信息进行寻找图片，并读取图片数据和标签数据Pytorch读取图片主要是通过Dataset类，Dataset类为所有datasets的基类，其中getitem函数接收一个index，然后返回图片数据和标签，index通常指的是list的index，这个list的每个元素包含了图片数据的路径和标签信息1.为数据集生成对应的txt文件 1234567891011121314151617181920212223242526272829303132import os''' 为数据集生成对应的txt文件'''train_txt_path = './train.txt'train_dir = './train/'valid_txt_path = './valid.txt'valid_dir = './valid/'def gen_txt(txt_path, img_dir): f = open(txt_path, 'w') for root, s_dirs, _ in os.walk(img_dir, topdown=True): # 获取 train文件下各文件夹名称 for sub_dir in s_dirs: i_dir = os.path.join(root, sub_dir) # 获取各类的文件夹 绝对路径 img_list = os.listdir(i_dir) # 获取类别文件夹下所有png图片的路径 for i in range(len(img_list)): if not img_list[i].endswith('png'): # 若不是png文件，跳过 continue label = img_list[i].split('_')[0] img_path = os.path.join(i_dir, img_list[i]) line = img_path + ' ' + label + '\n' f.write(line) f.close()if __name__ == '__main__': gen_txt(train_txt_path, train_dir) gen_txt(valid_txt_path, valid_dir) 2.构建Dataset子类 1234567891011121314151617181920212223242526272829# coding: utf-8from PIL import Imagefrom torch.utils.data import Datasetclass MyDataset(Dataset): def __init__(self, txt_path, transform=None, target_transform=None): fh = open(txt_path, 'r') imgs = [] for line in fh: line = line.rstrip() words = line.split() imgs.append((words[0], int(words[1]))) self.imgs = imgs # 最主要就是要生成这个list， 然后DataLoader中给index，通过getitem读取图片数据 self.transform = transform self.target_transform = target_transform def __getitem__(self, index): fn, label = self.imgs[index] img = Image.open(fn).convert('RGB') # 像素值 0~255，在transfrom.totensor会除以255，使像素值变成 0~1 if self.transform is not None: img = self.transform(img) # 在这里做transform，转为tensor等等 return img, label def __len__(self): return len(self.imgs) 初始化中还会初始化 transform，transform 是一个 Compose 类型，里边有一个 list，list中就会定义了各种对图像进行处理的操作，可以设置减均值，除标准差，随机裁剪，旋转，翻转，仿射变换等操作 一张图片读取进来之后，会经过数据处理（数据增强），最终变成输入模型的数据。这里就有一点需要注意，PyTorch的数据增强是将原始图片进行了处理，并不会生成新的一份图片，而是“覆盖”原图，当采用randomcrop之类的随机操作时，每个epoch输入进来的图片几乎不会是一模一样的，这达到了样本多样性的功能。 使用pytorch读取图片后一定要转换为RGB模式图片从硬盘到模型在 MyDataset 中，主要获取图片的索引以及定义如何通过索引读取图片及其标签在 DataLoder 中，主要是触发MyDataset去读取硬盘上的图片及其标签，并拼接成一个batch返回，作为模型真正的输入 123456789101112131415161718#从 MyDataset 类中初始化 txt，txt 中有图片路径和标签1. main.py: train_data = MyDataset(txt_path=train_txt_path, ...） ---&gt;#初始化 DataLoder 时，将 train_data 传入，从而使 DataLoder 拥有图片的路径2. main.py: train_loader = DataLoader(dataset=train_data, ...) ---&gt;#在一个 iteration 进行时，才读取一个 batch 的图片数据 enumerate()函数会返回可迭代数据的一个“元素”,在这里 data 是一个 batch 的图片数据和标签，data 是一个 list 3. main.py: for i, data in enumerate(train_loader, 0) ---&gt;#class DataLoader()中再调用 class _DataLoderIter()4. dataloder.py: class DataLoader(): def __iter__(self): return _DataLoaderIter(self) ---&gt;#在 _DataLoderiter()类中会跳到__next__(self)函数，在该函数中会通过next获取一个batch的indices，再通过collate_fn获取一个batch的数据5. dataloder.py: class _DataLoderIter(): def __next__(self): batch = self.collate_fn([self.dataset[i] for i in indices]) ---&gt;#self.collate_fn 中会调用 MyDataset 类中的__getitem__()函数，读取图片6. tool.py: class MyDataset(): def __getitem__(): img = Image.open(fn).convert('RGB') ---&gt;#读取图片后，对图片进行预处理transform7. tool.py: class MyDataset(): img = self.transform(img) ---&gt;#将图片数据转换成 Variable 类型，然后称为模型真正的输入8. main.py: inputs, labels = data inputs, labels = Variable(inputs), Variable(labels) outputs =net(inputs) 数据增强12345678910111213141516171819202122232425262728293031323334353637383940# 数据预处理设置normMean = [0.4948052, 0.48568845, 0.44682974]normStd = [0.24580306, 0.24236229, 0.2603115]normTransform = transforms.Normalize(normMean, normStd)# transforms.Compose()函数则是将各种预处理的操作组合到了一起trainTransform = transforms.Compose([ transforms.Resize(32), transforms.RandomCrop(32, padding=4), transforms.ToTensor(), normTransform])# 构建MyDataset实例train_data = MyDataset(txt_path=train_txt_path, transform=trainTransform)valid_data = MyDataset(txt_path=valid_txt_path, transform=validTransform)# 构建DataLodertrain_loader = DataLoader(dataset=train_data, batch_size=train_bs, shuffle=True)valid_loader = DataLoader(dataset=valid_data, batch_size=valid_bs)# 数据转换为Variable类型，输入到网络模型for epoch in range(max_epoch): loss_sigma = 0.0 # 记录一个epoch的loss之和 correct = 0.0 total = 0.0 scheduler.step() # 更新学习率 for i, data in enumerate(train_loader): # if i == 30 : break # 获取图片和标签 inputs, labels = data inputs, labels = Variable(inputs), Variable(labels) # forward, backward, update weights optimizer.zero_grad() outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() transforms.RandomCrop(32, padding=4) 随机裁剪，在裁剪之前先对图片的上下左右均填充上 4 个pixel，值为0，即变成一个 36_36 的数据，然后再随机进行 32_32 的裁剪 transforms.ToTensor() 会对数据进行 transpose，原来是 h_w_c，会经过 img =img.transpose(0,1).transpose(0, 2).contiguous()，变成 c_h_w再除以255，使得像素值归一化至[0-1]之间 transforms.Normalize(normMean, normStd) 数据标准化 ( 减均值，除以标准差 ） 其中均值和方差是需要经过计算的出来的，见如下代码 1234567891011121314151617181920212223with open(train_txt_path, 'r') as f: lines = f.readlines() random.shuffle(lines) # shuffle , 随机挑选图片 for i in range(CNum): img_path = lines[i].rstrip().split()[0] img = cv2.imread(img_path) img = cv2.resize(img, (img_h, img_w)) img = img[:, :, :, np.newaxis] imgs = np.concatenate((imgs, img), axis=3) print(i)imgs = imgs.astype(np.float32)/255.for i in range(3): pixels = imgs[:,:,i,:].ravel() # 拉成一行 means.append(np.mean(pixels)) stdevs.append(np.std(pixels))means.reverse() # BGR --&gt; RGBstdevs.reverse() transforms本节对transform中的各种预处理方法进行介绍和总结 裁剪 ——Crop 中心裁剪：transforms.CenterCrop 随机裁剪：transforms.RandomCrop 随机长宽比裁剪：transforms.RandomResizedCrop 上下左右中心裁剪：transforms.FiveCrop 对图片进行上下左右以及中心裁剪，获得 5 张图片，返回一个 4D-tensor 上下左右中心裁剪后翻转，transforms.TenCrop 对图片进行上下左右以及中心裁剪，然后全部翻转（水平或者垂直），获得 10 张图片，返回一个 4D-tensor。 翻转和旋转 ——Flip and Rotation 依概率 p 水平翻转：transforms.RandomHorizontalFlip(p=0.5) 依概率 p 垂直翻转：transforms.RandomVerticalFlip(p=0.5) 随机旋转：transforms.RandomRotation 图像变换 resize：transforms.Resize 重置图像分辨率 标准化：transforms.Normalize 对数据按通道进行标准化，即先减均值，再除以标准差，注意是 h_w_c 转为 tensor，并归一化至[0-1]：transforms.ToTensor 将 PIL Image 或者 ndarray 转换为 tensor，并且归一化至[0-1] 填充：transforms.Pad 修改亮度、对比度和饱和度：transforms.ColorJitter 转灰度图：transforms.Grayscale 线性变换：transforms.LinearTransformation() 对矩阵做线性变化，可用于白化处理！ 仿射变换：transforms.RandomAffine 依概率 p 转为灰度图：transforms.RandomGrayscale 将数据转换为 PILImage：transforms.ToPILImage transforms.Lambda：Apply a user-defined lambda as a transform. 对 transforms 操作，使数据增强更灵活 transforms.RandomChoice(transforms)， 从给定的一系列 transforms 中选一个进行操作 transforms.RandomApply(transforms, p=0.5)，给一个 transform 加上概率，依概率进行操作 transforms.RandomOrder，将 transforms 中的操作顺序随机打乱 cat|view|squeezetorch.cat((A,B),dim)：拼接两个tensor，除了拼接维数dim数值可以不同外，其余维数数值需相同，方能对齐+：相同维度数据，相同位置相加x.view(-1,2):在不改变tensor数据的情况下，随意改变张量的大小和形状，类似于np.reshape()x.view(x.size(0), -1)：将原始(50,32,7,7)输出变为两维(50,32_7_7),即将输出拉伸为一行后进行全连接操作a.view_as(torch.Tensor(4, 2)):返回与给定的tensor相同大小的原tensora.squeeze(N) 就是去掉a中指定位置N上维数为1的维度a.unsqueeze(N) 就是在a中指定位置N加上一个维数为1的维度 模型定义 三要素：先继承nn.Module，在构建组件init,最后组装forward 12345678910111213141516171819202122232425262728293031323334353637class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(3, 6, 5) self.pool1 = nn.MaxPool2d(2, 2) self.conv2 = nn.Conv2d(6, 16, 5) self.pool2 = nn.MaxPool2d(2, 2) self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): x = self.pool1(F.relu(self.conv1(x))) x = self.pool2(F.relu(self.conv2(x))) x = x.view(-1, 16 * 5 * 5) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x # 定义权值初始化 def initialize_weights(self): for m in self.modules(): if isinstance(m, nn.Conv2d): torch.nn.init.xavier_normal_(m.weight.data) if m.bias is not None: m.bias.data.zero_() elif isinstance(m, nn.BatchNorm2d): m.weight.data.fill_(1) m.bias.data.zero_() elif isinstance(m, nn.Linear): torch.nn.init.normal_(m.weight.data, 0, 0.01) m.bias.data.zero_()net = Net() # 创建一个网络net.initialize_weights() # 初始化权值 全连接层self.fc1 = nn.Linear(input_size, hidden_size) nn.Sequetialtorch.nn.Sequential是Sequential容器，该容器将一系列操作按先后顺序包起来，方便重复使用 初始化权重123456789101112131415def initialize_weights(self): #依次返回模型中各层 for m in self.modules(): # 判断类型 if isinstance(m, nn.Conv2d): # 进行初始化 torch.nn.init.xavier_normal(m.weight.data) if m.bias is not None: m.bias.data.zero_() elif isinstance(m, nn.BatchNorm2d): m.weight.data.fill_(1) m.bias.data.zero_() elif isinstance(m, nn.Linear): torch.nn.init.normal(m.weight.data, 0, 0.01) m.bias.data.zero_() 模型定义完成后，还需要对权重进行初始化，权重初始化方法会直接影响到模型的收敛与否 Xavier均匀分布 torch.nn.init.xavier_uniform_(tensor, gain=1) Xavier 正态分布 torch.nn.init.xavier_normal_(tensor, gain=1) kaiming 均匀分布 torch.nn.init.kaiming_uniform_(tensor, a=0, mode=&#39;fan_in&#39;, nonlinearity=&#39;leaky_relu&#39;) kaiming 正态分布 torch.nn.init.kaiming_normal_(tensor, a=0, mode=&#39;fan_in&#39;, nonlinearity=&#39;leaky_relu&#39;) 均匀分布初始化 torch.nn.init.uniform_(tensor, a=0, b=1) 正态分布初始化 torch.nn.init.normal_(tensor, mean=0, std=1) 常数初始化 torch.nn.init.constant_(tensor, val) 单位矩阵初始化 torch.nn.init.eye_(tensor) 正交初始化 torch.nn.init.orthogonal_(tensor, gain=1) 模型Finetune 权值初始化 保存模型参数 `toch.save(net.state_dict(),’net_params.pkl’) 加载模型 `pretrained_dict = torch.load(‘net_params.pkl’) 12345678net = Net() # 创建 netnet_state_dict = net.state_dict() # 获取已创建 net 的 state_dict#接着将 pretrained_dict 里不属于 net_state_dict 的键剔除掉:pretrained_dict_1 = &#123;k: v for k, v in pretrained_dict.items() if k in net_state_dict&#125;#然后,用预训练模型的参数字典 对 新模型的参数字典 net_state_dict 进行更新:net_state_dict.update(pretrained_dict_1)#最后,将更新了参数的字典 “放”回到网络中:net.load_state_dict(net_state_dict) 不同层设置不同的学习率在利用 pre-trained model 的参数做初始化之后,我们可能想让 fc 层更新相对快一些,而希望前面的权值更新小一些,这就可以通过为不同的层设置不同的学习率来达到此目的。为不同层设置不同的学习率,主要通过优化器对多个参数组进行设置不同的参数。所以,只需要将原始的参数组,划分成两个,甚至更多的参数组,然后分别进行设置学习率。这里将原始参数“切分”成 fc3 层参数和其余参数,为 fc3 层设置更大的学习率。 12345# 挑选出特定的层的机制是利用内存地址作为过滤条件,将需要单独设定的那部分参数,从总的参数中剔除ignored_params = list(map(id, net.fc3.parameters())) # 返回的是 parameters 的 内存地址base_params = filter(lambda p: id(p) not in ignored_params, net.parameters()) # 返回 base params 的 内存地址optimizer = optim.SGD([&#123;'params': base_params&#125;,&#123;'params': net.fc3.parameters(), 'lr': 0.001*10&#125;],0.001, momentum=0.9, weight_decay=1e-4) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394from torchvision import modelsfrom pretrainedmodels.models.inceptionv4 import inceptionv4,BasicConv2d,Mixed_3a,Mixed_4a,Mixed_5a,Inception_A,Reduction_A,Inception_B,Reduction_B,Inception_Cfrom torch import nnfrom config import configdef get_net(): # 注意：当选择预训练数据集时。pretrained="imagenet"，权重参数为预训练后的权重 # 在此重写某层后，预训练权重会被覆盖成默认的初始化( uniform —— 均匀分布初始化) model = inceptionv4(pretrained="imagenet") # 笨办法(千万不要用,会影响预先训练好的权重) # model.features = nn.Sequential( # BasicConv2d(3, 32, kernel_size=3, stride=2), # BasicConv2d(32, 32, kernel_size=3, stride=1), # BasicConv2d(32, 64, kernel_size=3, stride=1, padding=1), # Mixed_3a(), # Mixed_4a(), # Mixed_5a(), # Inception_A(), # Inception_A(), # Inception_A(), # Inception_A(), # Reduction_A(), # Mixed_6a # Inception_B(), # Inception_B(), # Inception_B(), # Inception_B(), # Inception_B(), # Inception_B(), # Inception_B(), # Reduction_B(), # Mixed_7a # Inception_C(), # Inception_C(), # Inception_C() # ) # 改变前的InceptionV4 print(model) # InceptionV4( # (features): Sequential( # (0): BasicConv2d( # (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False) # (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True) # (relu): ReLU(inplace) # ) # (last_linear): (last_linear): Linear(in_features=1536, out_features=1000, bias=True) # 修改模型输入维度 model.features._modules["0"] = BasicConv2d(config.channels, 32, kernel_size=3, stride=2) # 修改模型输出维度 model.last_linear = nn.Sequential( nn.BatchNorm1d(1536), nn.Dropout(0.5), nn.Linear(1536, config.num_classes), ) # 查看部分层的方法(也可以用此方法进行修改) print(model.features._modules["21"].branch0.conv) print(model.features._modules["0"]._modules["conv"]) print(model.features._modules["0"].conv) # 改变后的InceptionV4 print(model) # (conv): Conv2d(4, 32, kernel_size=(3, 3), stride=(2, 2), bias=False) # (last_linear): Sequential( # (0): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) # (1): Dropout(p=0.5) # (2): Linear(in_features=1536, out_features=28, bias=True) # ) # 新创建的所有的训练参数名字 print(model.state_dict().keys()) # 新创建的模型的所有的参数iterator for i, j in model.named_parameters(): if i in ['last_linear.weight']: print(i) print(j) # 直接从官方model_zoo下载 # print("直接从官方model_zoo下载") # import torch.utils.model_zoo as model_zoo # # pretrained_dict = model_zoo.load_url('http://data.lip6.fr/cadene/pretrainedmodels/inceptionresnetv2-520b38e4.pth') # print(pretrained_dict['conv2d_1a.bn.weight']) # print(pretrained_dict['conv2d_1a.bn.weight'].size()) # print(pretrained_dict['last_linear.bias']) # print(pretrained_dict['last_linear.bias'].size())if __name__ == '__main__': get_net() # 训练的时候，只更新 classifier 层的参数optimizer = optim.Adam(model.features.parameters(), lr=learning_rate) FAQ 从代码中发现,即使不进行初始化,我们模型的权值也不为空,而是有值的,这些值是 在什么时候赋的呢?其实,在创建网络实例的过程中, 一旦调用 nn.Conv2d 的时候就会有对权值进行初始化Conv2d 是继承_ConvNd,初始化赋值是在_ConvNd 当中的 12345678910111213141516self.weight = Parameter(torch.Tensor( out_channels, in_channels // groups, *kernel_size)) if bias: self.bias = Parameter(torch.Tensor(out_channels)) else: self.register_parameter('bias', None) self.reset_parameters() def reset_parameters(self): n = self.in_channels for k in self.kernel_size: n *= k stdv = 1. / math.sqrt(n) self.weight.data.uniform_(-stdv, stdv) if self.bias is not None: self.bias.data.uniform_(-stdv, stdv) 这些值是创建一个 Tensor 时得到的,是一些很小的随机数。 按需定义初始化方法,例如:if isinstance(m, nn.Conv2d):n = m.kernel_size[0] _m.kernel_size[1] _m.out_channelsm.weight.data.normal_(0, math.sqrt(2. / n)) 想在前向传播时，在relu之后给x乘以一个可训练的系数，只需要在init函数中添加一个nn.Parameter类型变量，并在forward函数中乘以该变量即可： 12345678910111213def __init__(self): super(MLP, self).__init__() self.linear1 = torch.nn.Linear(3, 5) self.relu = torch.nn.ReLU() self.linear2 = torch.nn.Linear(5, 2) # the para to be added and updated in train phase, note that NO cuda() at last self.coefficient = torch.nn.Parameter(torch.Tensor([1.55]))def forward(self, x): x = self.linear1(x) x = self.relu(x) x = self.coefficient * x x = self.linear2(x)return x 修改权重1.调用model.state_dict查看我们添加的参数在参数字典中的完整名称，然后打开原先的权重文件2.加载权重文件 a = torch.load(&quot;OldWeights.pth&quot;) a是一个collecitons.OrderedDict类型变量，即一个有序字典3.直接将新参数名称和初始值作为键值对插入，然后保存即可。 123456789101112a = torch.load("OldWeights.pth") a["layer1.0.coefficient"] = torch.FloatTensor([1.2])a["layer1.1.coefficient"] = torch.FloatTensor([1.5]) torch.save(a, "Weights.pth")print(model.state_dict().keys())for name, param in model.named_parameters(): #所有的参数 if param.requires_grad: #要求可训练 print(name) print(param) 不改输入维度，不改输出维度，只修改部分层,这种情况下，可以使用原先模型的预训练权重，新增的层，进行权重初始化 所有教程都是基于pretrainedmodels==0.7.4之上的操作 model = inceptionresnetv2(pretrained=&quot;imagenet&quot;) 不改输入维度，改输出维度,则覆盖的层不会使用预训练的权重，会使用Tensor中的随机赋值 调用forward方法的具体流程 12345678910forward 以一个Module为例： 1. 调用module的call方法 2. module的call里面调用module的forward方法 3. forward里面如果碰到Module的子类，回到第1步，如果碰到的是Function的子类，继续往下 4. 调用Function的call方法 5. Function的call方法调用了Function的forward方法。 6. Function的forward返回值 7. module的forward返回值 8. 在module的call进行forward_hook操作，然后返回值。 backward当执行model(x)的时候，底层自动调用forward方法计算结果 1234567# 保存和加载整个模型torch.save(model_object, 'model.pkl')model = torch.load('model.pkl')# 仅保存和加载模型参数(推荐使用)torch.save(model_object.state_dict(), 'params.pkl')model_object.load_state_dict(torch.load('params.pkl')) 损失函数优化器12345# ------------------------------------ step 3/5 : 定义损失函数和优化器 ------------------------------------criterion = nn.CrossEntropyLoss() # 选择损失函数optimizer = optim.SGD(net.parameters(), lr=lr_init, momentum=0.9, dampening=0.1) # 选择优化器scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1) # 设置学习率下降策略 损失函数 L1loss：class torch.nn.L1Loss(size_average=None, reduce=None) 功能：计算 output 和 target 之差的绝对值,可选返回同维度的 tensor 或者是一个标量。ln=|xn−yn|ln=|xn−yn| MSELoss：class torch.nn.MSELoss(size_average=None, reduce=None, reduction=’elementwise_mean’) 功能：计算 output 和 target 之差的平方,可选返回同维度的 tensor 或者是一个标量。ln=(xn−yn)2ln=(xn−yn)2 CrossEntropyLoss：class torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction=’elementwise_mean’) 功能：将输入经过 softmax 激活函数之后转换成概率分布后,再计算其与 target 的交叉熵损失。即该方法将nn.LogSoftmax()和 nn.NLLLoss()进行了结合。 NLLLoss：class torch.nn.NLLLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction=’elementwise_mean’) 功能：直接求交叉熵损失 L=−sigama(yi∗log(xi))L=−sigama(yi∗log(xi)) PoissonNLLLoss:class torch.nn.PoissonNLLLoss(log_input=True, full=False, size_average=None, eps=1e-08, reduce=None, reduction=’elementwise_mean’) 功能：用于 target 服从泊松分布的分类任务。 KLDivLoss: 功能：计算 input 和 target 之间的 KL 散度( Kullback–Leibler divergence) 又称为相对熵(Relative Entropy),用于描述两个概率分布之间的差异。 ln=yn(logyn−xn)ln=yn(logyn−xn) 信息熵 = 交叉熵 - 相对熵 BCELoss 功能：二分类任务时的交叉熵计算函数。此函数可以认为是 nn.CrossEntropyLoss 函数的特例。其分类限定为二分类,y 必须是{0,1}。还需要注意的是,input 应该为概率分布的形式,这样才符合交叉熵的应用。ln=−wn[ynlogxn+(1−yn)log(1−xn)]ln=−wn[ynlogxn+(1−yn)log(1−xn)] BCEWithLogitsLoss 功能：将 Sigmoid 与 BCELoss 结合,类似于 CrossEntropyLoss(将 nn.LogSoftmax()和 nn.NLLLoss()进行结合)。即 input 会经过 Sigmoid 激活函数,将 input 变成概率分布的形式 MarginRankingLoss 功能：计算两个向量之间的相似度,当两个向量之间的距离大于 margin,则 loss 为正,小于margin,loss 为 0。 SmoothL1Loss 功能：计算平滑 L1 损失,属于 Huber Loss 中的一种(因为参数 δ 固定为 1 了)。Huber Loss 常用于回归问题,其最大的特点是对离群点( outliers )、噪声不敏感,具有较强的鲁棒性。在bbox loss中常用 当误差绝对值小于 δ ,采用 L2 损失;若大于 δ ,采用 L1 损失。 TripletMarginLoss:class torch.nn.TripletMarginLoss(margin=1.0, p=2, eps=1e-06, swap=False, size_average=None, reduce=None, reduction=’elementwise_mean’) 功能：计算三元组损失,人脸验证中常用。目标是让 Positive 元和 Anchor 元之间的距离尽可能的小,Positive 元和 Negative 元之间的距离尽可能的大。 优化器参数组(param_groups) zero_grad() 功能:将梯度清零。由于 PyTorch 不会自动清零梯度,所以在每一次更新前会进行此操作。 state_dict() 功能:获取模型当前的参数,以一个有序字典形式返回。这个有序字典中,key 是各层参数名,value 就是参数。 load_state_dict(state_dict) 功能:将 state_dict 中的参数加载到当前网络,常用于 finetune。 add_param_group()功能:给 optimizer 管理的参数组中增加一组参数,可为该组参数定制 lr,momentum, weight_decay 等,在 finetune 中常用。 例如:optimizer_1.add_param_group({‘params’: w3, ‘lr’: 0.001, ‘momentum’:0.8}) step(closure) 功能:执行一步权值更新, 其中可传入参数 closure(一个闭包)。如,当采用 LBFGS 优化方法时,需要多次计算,因此需要传入一个闭包去允许它们重新计算 loss 12345678for input, target in dataset: def closure(): optimizer.zero_grad() output = model(input) loss = loss_fn(output, target) loss.backward() return lossoptimizer.step(closure) 优化器 torch.optim.SGD：class torch.optim.SGD(params, lr=&lt;object,object&gt;, momentum=0, dampening=0, weight_decay=0, nesterov=False) 功能:可实现 SGD 优化算法,带动量 SGD 优化算法,带 NAG(Nesterov acceleratedgradient)动量 SGD 优化算法,并且均可拥有 weight_decay 项。 p=p−lr∗v=p−lr∗ρ∗v−lr∗gp=p−lr∗v=p−lr∗ρ∗v−lr∗g ,ρ 是动量,v 是更新学习速率,g 是梯度，lr是初始学习速率 torch.optim.ASGD：class torch.optim.ASGD(params, lr=0.01, lambd=0.0001, alpha=0.75, t0=1000000.0, weight_decay=0) 功能：ASGD 也成为 SAG,均表示随机平均梯度下降( Averaged Stochastic GradientDescent ),简单地说 ASGD 就是用空间换时间的一种 SGD torch.optim.Adagrad：class torch.optim.Adagrad(params, lr=0.01, lr_decay=0, weight_decay=0, initial_accumulator_value=0) 功能：实现 Adagrad 优化方法( Adaptive Gradient ),Adagrad 是一种自适应优化方法,是自适应的为各个参数分配不同的学习率。这个学习率的变化,会受到梯度的大小和迭代次数的影响。梯度越大,学习率越小;梯度越小,学习率越大。缺点是训练后期,学习率过小,因为 Adagrad 累加之前所有的梯度平方作为分母 torch.optim.Adadelta 功能：实现 Adadelta 优化方法。Adadelta 是 Adagrad 的改进。Adadelta 分母中采用距离当前时间点比较近的累计项,这可以避免在训练后期,学习率过小。 torch.optim.Adam(AMSGrad) 功能：实现 Adam(Adaptive Moment Estimation))优化方法。Adam 是一种自适应学习率的优化方法,Adam 利用梯度的一阶矩估计和二阶矩估计动态的调整学习率。吴老师课上说过,Adam 是结合了 Momentum 和 RMSprop,并进行了偏差修正学习率调整方法优化器中最重要的一个参数就是学习率,合理的学习率可以使优化器快速收敛。一般在训练初期给予较大的学习率,随着训练的进行,学习率逐渐减小。学习率什么时候减小,减小多少,这就涉及到学习率调整方法。 lr_scheduler.StepLR：class torch.optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.1, last_epoch=-1) 功能：等间隔调整学习率,调整倍数为 gamma 倍,调整间隔为 step_size。间隔单位是step。需要注意的是,step 通常是指 epoch,不要弄成 iteration 了。 lr_scheduler.ExponentialLR 功能：按指数衰减调整学习率,调整公式: lr = lr * gamma**epoch lr_scheduler.CosineAnnealingLR 功能：以余弦函数为周期,并在每个周期最大值时重新设置学习率 lr_scheduler.ReduceLROnPlateau：class torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=’min’,factor=0.1, patience=10, verbose=False, threshold=0.0001, threshold_mode=’rel’, cooldown=0, min_lr=0, eps=1e-08) 功能：当某指标不再变化(下降或升高),调整学习率,这是非常实用的学习率调整策略。例如,当验证集的 loss 不再下降时,进行学习率调整;或者监测验证集的 accuracy,当accuracy 不再上升时,则调整学习率。 lr_scheduler.LambdaLR 功能：为不同参数组设定不同学习率调整策略。调整规则为,lr = base_lr *lmbda(self.last_epoch) PyTorch 提供了六种学习率调整方法,可分为三大类,分别是 有序调整;依一定规律有序进行调整,这一类是最常用的,分别是等间隔下降(Step),按需设定下降间隔(MultiStep),指数下降(Exponential)和 CosineAnnealing。这四种方法的调整时机都是人为可控的,也是训练时常用到的。 自适应调整;依训练状况伺机调整,这就是 ReduceLROnPlateau 方法。该法通过监测某一指标的变化情况,当该指标不再怎么变化的时候,就是调整学习率的时机,因而属于自适应的调整。 自定义调整。自定义调整,Lambda。Lambda 方法提供的调整策略十分灵活,我们可以为不同的层设定不同的学习率调整方法,这在 fine-tune 中十分有用,我们不仅可为不同的层设定不同的学习率,还可以为其设定不同的学习率调整策略,简直不能更棒! 监控模块可视化本章将介绍如何在 PyTorch 中使用 TensorBoardX 对神经网络进行统计可视化，如Loss 曲线、Accuracy 曲线、卷积核可视化、权值直方图及多分位数折线图、特征图可视化、梯度直方图及多分位数折线图及混淆矩阵图等 TensorBoardXPyTorch 自身的可视化功能没有 TensorFlow 的 tensorboard 那么优秀，所以 PyTorch通常是借助 tensorboard(是借助，非直接使用)进行可视化，目前流行的有如下两种方法，本文仅介绍第二种——TensorBoardX。 构建Logger类 Logger类中”包”了tf.summary.FileWriter,目前有三种操作，scalar_summary(), image_summary(), histo_summary()，优点： 轻便，可满足大部分需求。Logger 类参考 github 借助 TensorBoardX 包 TensorBoardX 包的功能就比较全,目前,支持除 tensorboard beholder 之外的所有 tensorboard 的记录类型 github api 文档 安装方法 pip uninstall tensorboardX pip install tensorboard 操作参考 简而言之：tensorboard --logdir=runs 然后到浏览器中打开：localhost:6006 函数介绍 add_scalar(): add_scalar(tag, scalar_value, global_step=None, walltime=None) 在一个图表中记录一个标量的变化，常用于 Loss 和 Accuracy 曲线的记录。writer.add_scalar(‘data/scalar1’,dummy_s1[0], n_iter) add_scalars(): add_scalars(main_tag, tag_scalar_dict, global_step=None, walltime=None) 在一个图表中记录多个标量的变化，常用于对比，如 trainLoss 和 validLoss 的比较等。 writer.add_scalars(&#39;data/scalar_group&#39;, {&#39;xsinx&#39;: n_iter * np.sin(n_iter),&#39;xcosx&#39;: n_iter * np.cos(n_iter),&#39;arctanx&#39;: np.arctan(n_iter)}, n_iter) add_histogram() 绘制直方图和多分位数折线图，常用于监测权值及梯度的分布变化情况，便于诊断网络更新方向是否正确。 writer.add_histogram(name, param.clone().cpu().data.numpy(), n_iter) add_image(): add_image(tag, img_tensor, global_step=None, walltime=None) 绘制图片，可用于检查模型的输入，监测 feature map 的变化，或是观察 weight torchvision.utils.make_grid():torchvision.utils.make_grid(tensor, nrow=8, padding=2, normalize=False, range=None, scale_each=False, pad_value=0) 将一组图片拼接成一张图片，便于可视化 12345import torchvision.utils as vutilsdummy_img = torch.rand(32, 3, 64, 64) # (B x C x H x W)if n_iter % 10 == 0:x = vutils.make_grid(dummy_img, normalize=True, scale_each=True)writer.add_image('Image', x, n_iter) # x.size= (3, 266, 530) (C*H*W) add_graph():add_graph(model, input_to_model=None, verbose=False, **kwargs) 绘制网络结构拓扑图。 1234import torchvision.models as modelsresnet18 = models.resnet18(False)dummy_input = torch.rand(6, 3, 224, 224)writer.add_graph(resnet18, dummy_input) add_embedding() 在三维空间或二维空间展示数据分布，可选 T-SNE、PCA 和 CUSTOM 方法。 12345dataset = datasets.MNIST('mnist', train=False, download=True)images = dataset.test_data[:100].float()label = dataset.test_labels[:100]features = images.view(100, 784)writer.add_embedding(features, metadata=label, label_img=images.unsqueeze(1)) add_text():add_text(tag, text_string, global_step=None, walltime=None) 记录文字 add_video():add_video(tag, vid_tensor, global_step=None, fps=4, walltime=None) 记录video add_figure():add_figure(tag, figure, global_step=None, close=True, walltime=None) 添加 matplotlib 图片到图像中 add_image_with_boxes():add_image_with_boxes(tag, img_tensor, box_tensor, global_step=None, walltime=None, **kwargs) 图像中绘制 Box，目标检测中会用到 add_pr_curve(): 绘制 PR 曲线 add_pr_curve_raw(): 从原始数据上绘制 PR 曲线 export_scalars_to_json(): 将 scalars 信息保存到 json 文件，便于后期使用 实现 卷积核可视化 特征图可视化 梯度及权值分布可视化 思考：1.通过观察各层的梯度，权值分布，我们可以针对性的设置学习率，为那些梯度小的层设置更大的学习率，让那些层可以有效的更新 思考：2.对权值特别大的层，可以考虑为那一层设置更大的weight_decay，也许有效降低该层权值大小 思考：3.通过对梯度的观察，可以合理的设置梯度clip 混淆矩阵及其可视化 参考Pytorch（一）入门：Tensor基础Pytorch（二）入门：autograd机制Pytorch（三）入门：线性回归Pytorch（四）入门：多层感知机Pytorch（五）入门：DataLoader 和 Dataset (torchvision 图像处理包中的数据预处理)Pytorch（七）入门参数初始化、仅训练某几层和使用改变预训练模型pytorch 学习手册Pytorch 模型训练实用教程Pytorch中文API文档]]></content>
      <categories>
        <category>深度学习基础</category>
      </categories>
      <tags>
        <tag>标签1</tag>
        <tag>标签2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mAP]]></title>
    <url>%2F%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%2FmAP%2F</url>
    <content type="text"><![CDATA[mAP 目标检测目标检测任务输入一张图像,输出图像中所存在的目标的类别信息predict和位置信息bbox; IOUIOU(Intersection over Union):预测位置信息bbox(bounding box)与实际位置信息gt(Ground Truth)之间交集和并集的比值 评估指标 精确度Precision = TP / (TP + FP) 预测的bbox中预测正确的数量 召回率Recall = TP / (TP + FN) = TP / len(gt) ground-true box中被正确预测的数量 FP:误检 | FN:漏检 评价目标检测的性能不能只用精确度或者召回率,一定要考虑到误检和漏检两个场景 mAP mAP是对各目标类计算得到的AP取平均值； 设置一组IOU阈值，这组阈值为 (0.5, 0.55, 0.6, …, 0.9, 0.95),单个阈值下可以计算出一个性能mAP,每个阈值取性能平均则为mmAP 算法过程 举个例子一个例子有助于更好地理解插值平均精度的概念。考虑下面的检测：有7个图像，其中15个gt对象由绿色边界框表示，24个predict对象由红色边界框表示。每个predict对象都具有置信得分score，并由字母（A，B，…，Y）标识。下表显示了具有相应置信度的边界框。最后一列将检测标识为TP或FP。在此示例中，如果IOU为30％，则考虑TP ，否则为FP。通过查看上面的图像，我们可以粗略地判断检测是TP还是FP。重叠情况：在一些图像中，存在多于一个与GT重叠的检测（图像2,3,4,5,6和7）。对于这些情况，采用具有最高IOU的检测，丢弃其他检测。此规则适用于PASCAL VOC 2012度量标准：“例如，单个对象的5个检测（TP）被计为1个正确检测和4个错误检测”。通过计算累积Acc的TP或FP检测的精度和召回值来绘制PR曲线。为此，首先我们需要通过置信度对检测进行排序，然后我们计算每个累积Acc检测的精度和召回率，如下表所示： 备注： Acc TP为按照置信度降序排列的bbox中，预测正确的累加数和 Acc FP为按照置信度降序排列的bbox中，预测错误的累加数和 Precision = Acc TP /(Acc TP + Acc FP) Recall = Acc TP /15 由公式可以看得出，随着数据集增大，Precision由1逐渐减小，Recall由0逐渐增加至1 绘制精度和召回值，我们有以下Precision x Recall曲线： 11点插值11点插值平均精度的想法是在一组11个召回级别（0,0.1，…，1）处平均精度。插值精度值是通过采用其召回值大于其当前调用值的最大精度获得的，如下所示： 计算在所有点中执行的插值通过内插所有点，平均精度（AP）可以解释为PR曲线的近似AUC。目的是减少曲线中摆动的影响。通过应用之前给出的方程，我们可以获得这里将要展示的区域。我们还可以通过查看从最高（0.4666）到0（从右到左看图）的Recall来直观地获得插值精度点，并且当我们减少Recall时，我们收集最高的精度值如下图所示：看一下上图，我们可以将AUC划分为4个区域（A1，A2，A3和A4）： 过程描述1.获取每张img的预测框bbox,取前max_detections张大于得分score_threshold的bbox(并不会把所有检测结果都考虑进来，因为那样总是可以达到Recall=100%;只会考虑每张图片的前N个结果。这个数字应该随着数据集变化而改变，比如如果是密集场景数据集，这个数字应该提高)2.迭代每一个分类，迭代每一张图片并记录真实框的数目gt，迭代每一个预测框bbox2.计算一个预测框bbox与真实框gt之间的重叠率IOU，获取最大的IOU3.如果真实框为0，直接将该预测框标记为‘错检’FP4.如果重叠率IOU大于阈值，且真实框没有被标记，则为TP，并标记真实框bbox(以免重复检测)，否则为FP(错检)。(gt真实框中没有被标记的为FN漏检)5.每个分类下：TP，FP根据分类得分值进行逆序排序，并进行梯度累加和6.计算recall = TP / (TP + FN) = TP / len(gt) precision = TP / (TP + FP)7.计算AP。以R为横轴，P为纵轴，将每次的“截取”观察到的P和R画成一个点（R，P），利用这些（R，P）点绘制成P-R曲线(AP就是这个P-R曲线下的面积_AP_=∫10_PdR_)。AP计算了不同Recall下的Precision，综合性地评价了检测器，并不会对P和R有任何“偏好”;实际上在计算AP时，都要对P-R曲线做一次修正，将P值修正为当R&gt;R_0时最大的P（R_0即为该点对应的R），即 _AP_=∫10max(_P_(_r_)|_r_≥_R_)_dR_.故得出某个类别下的AP.ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])蓝色实曲线为原始P-R曲线、橘色虚曲线为修正后P-R曲线。为什么要修正P-R曲线呢？这是因为评价检测性能时，更应该关心“当R大于某个值时，能达到的最高的P是多少”、而不是“当R等于某个值时，此时的P是多少”。8.求所有类别的AP，并将其平均起来，作为这个IOU阈值下的检测性能，称为mAP(比如mAP=0.5就表示IOU阈值为0.5时的mAP)；最后，将所有IOU阈值下的mAP进行平均，就得到了最终的性能评价指标：mmAP]]></content>
      <categories>
        <category>模型评估</category>
      </categories>
      <tags>
        <tag>评估</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cudnn]]></title>
    <url>%2F%E7%A1%AC%E4%BB%B6%E5%8A%A0%E9%80%9F%2FcuDNN%2F</url>
    <content type="text"><![CDATA[cudnn 官方文档cudnn 例子：(1) (2)使用cudnn进行卷积Cudnn v5.1与V 6.0的特性 创建开始句柄 对于cudnn的使用，第一步当然是在代码中包含cudnn的头文件。在使用cudnn的函数之前，需要创建cudnn的句柄，定义为：cudnnHandle_t 类型。 12345#include&lt;cudnn.h&gt;int main(int argc,char const *argv[])&#123; cudnnHandle_t cudnn; cudnnCreate(&amp;cudnn)&#125; (需要注意的是，对于cudnn的每个函数，都会返回一个类型为cudnnStatus_t的错误码。成功执行的话，返回CUDNN_STATUS_SUCCESS。其他错误的话，可以用cudnnGetErrorString(status)来获得具体的错误信息。) tensor 描述卷积计算主要有三个参数：输入，输出和权重 输入 123456789cudnnTensorDescriptor_t input_descriptor;cudnnCreateTensorDescriptor(&amp;input_descriptor);cudnnSetTensor4dDescriptor(input_descriptor, /*format=*/CUDNN_TENSOR_NHWC, /*dataType=*/CUDNN_DATA_FLOAT, /*batch_size=*/1, /*channels=*/3, /*image_height=*/image.rows, /*image_width=*/image.cols); 输出 123456789cudnnTensorDescriptor_t output_descriptor;cudnnCreateTensorDescriptor(&amp;output_descriptor);cudnnSetTensor4dDescriptor(output_descriptor, /*format=*/CUDNN_TENSOR_NHWC, /*dataType=*/CUDNN_DATA_FLOAT, /*batch_size=*/1, /*channels=*/3, /*image_height=*/image.rows, /*image_width=*/image.cols); 因为我们输入是一张图，所以batch_size为1。而我们的图像是RGB三个通道，所以channels为3.对于权重数组的大小为input_channeloutput_channelkernel_size*kernel_size。在我们的例子中kernel_size为3 权重123456789cudnnFilterDescriptor_t kernel_descriptor;cudnnCreateFilterDescriptor(&amp;kernel_descriptor);cudnnSetFilter4dDescriptor(kernel_descriptor, /*dataType=*/CUDNN_DATA_FLOAT, /*format=*/CUDNN_TENSOR_NCHW, /*out_channels=*/3, /*in_channels=*/3, /*kernel_height=*/3, /*kernel_width=*/3); 对于权重tensor的格式，取决于我们保存的方式。output_channel input_channel kernel_h * kernel_w 卷积核描述1234567891011cudnnConvolutionDescriptor_t convolution_descriptor;cudnnCreateConvolutionDescriptor(&amp;convolution_descriptor);cudnnSetConvolution2dDescriptor(convolution_descriptor, /*pad_height=*/1, /*pad_width=*/1, /*vertical_stride=*/1, /*horizontal_stride=*/1, /*dilation_height=*/1, /*dilation_width=*/1, /*mode=*/CUDNN_CROSS_CORRELATION, /*computeType=*/CUDNN_DATA_FLOAT)); 我们对tensor做了描述，对卷积内核也做了描述，是否就可以计算卷积了呢？如果你这么以为，那就too young too simple了。对于卷积算法而言，有多种计算方式，在cudnn中集成了多种计算方式。需要我们根据当前硬件等情况设置选用哪种算法。 123456789cudnnConvolutionFwdAlgo_t convolution_algorithm;cudnnGetConvolutionForwardAlgorithm(cudnn, input_descriptor, kernel_descriptor, convolution_descriptor, output_descriptor, CUDNN_CONVOLUTION_FWD_PREFER_FASTEST, /*memoryLimitInBytes=*/0, &amp;convolution_algorithm); 选定采用哪种算法来计算卷积后，我们需要查询当前算法所需要的额外GPU内存消耗： 12345678size_t workspace_bytes = 0;cudnnGetConvolutionForwardWorkspaceSize(cudnn, input_descriptor, kernel_descriptor, convolution_descriptor, output_descriptor, convolution_algorithm, &amp;workspace_bytes); 终于到了这里，我们把卷积执行的准备工作做好了。现在就是针对我们的例子来分配数据了： 123456789101112void* d_workspace = nullptr;cudaMalloc(&amp;d_workspace, workspace_bytes);int image_bytes = batch_size * channels * height * width * sizeof(float);float* d_input =nullptr;cudaMalloc(&amp;d_input, image_bytes);cudaMemcpy(d_input, image.ptr&lt;float&gt;(0), image_bytes, cudaMemcpyHostToDevice);float* d_output = nullptr;cudaMalloc(&amp;d_output, image_bytes);cudaMemset(d_output, 0, image_bytes); 权重数据，对于我们的例子，就随便初始化一组了： 1234567891011121314151617181920const float kernel_template[3][3] = &#123; &#123;1, 1, 1&#125;, &#123;1, -8, 1&#125;, &#123;1, 1, 1&#125;&#125;;float h_kernel[3][3][3][3];for (int kernel = 0; kernel &lt; 3; ++kernel) &#123; for (int channel = 0; channel &lt; 3; ++channel) &#123; for (int row = 0; row &lt; 3; ++row) &#123; for (int column = 0; column &lt; 3; ++column) &#123; h_kernel[kernel][channel][row][column] = kernel_template[row][column]; &#125; &#125; &#125;&#125;float* d_kernel = nullptr;cudaMalloc(&amp;d_kernel, sizeof(h_kernel));cudaMemcpy(d_kernel, h_kernel, sizeof(h_kernel), cudaMemcpyHostToDevice); 终于，我们要计算卷积了： 1234567891011121314const float alpha = 1, beta = 0;cudnnConvolutionForward(cudnn, &amp;alpha, input_descriptor, d_input, kernel_descriptor, d_kernel, convolution_descriptor, convolution_algorithm, d_workspace, workspace_bytes, &amp;beta, output_descriptor, d_output); 后续的操作就是把计算完的数据拷贝回主机端，并释放分配的各种资源： 1234567891011121314151617float* h_output = new float[image_bytes];cudaMemcpy(h_output, d_output, image_bytes, cudaMemcpyDeviceToHost);// Do something with h_output ...delete[] h_output;cudaFree(d_kernel);cudaFree(d_input);cudaFree(d_output);cudaFree(d_workspace);cudnnDestroyTensorDescriptor(input_descriptor);cudnnDestroyTensorDescriptor(output_descriptor);cudnnDestroyFilterDescriptor(kernel_descriptor);cudnnDestroyConvolutionDescriptor(convolution_descriptor);cudnnDestroy(cudnn); 终于把一个正向的卷积计算给说完了。但是大伙可能会说，实际应用中一般卷积以后立马都会接激活函数，这个如何处理呢？一，把卷积计算后的结果，调用cudnnAddTensor加上bias值，再通过cudnnConvolutionBiasActivationForward函数；第二：在cudnn6以后添加了cudnnConvolutionBiasActivationForward函数。这个三个函数怎么用，看手册就会明白的。如上，便是cudnn卷积计算的说明。 NVIDIA cuDNN是一个GPU加速深层神经网络原语库。它提供了在DNN应用程序中频繁出现的例程的高度优化的实现： 卷积前馈和反馈， pooling前馈和反馈 softmax前馈和反馈 神经元前馈和反馈： 整流线性（ReLU） -sigmoid 双曲线正切（TANH） 张量转换函数 LRN，LCN和批量归一化前进和后退 cuDNN的卷积程序旨在提高性能，以最快的GEMM（矩阵乘法）为基础实现此类例程，同时使用更少的内存。cuDNN提供基于上下文的API，可以轻松实现与CUDA流的多线程和（可选）互操作性。编程模型：cuDNN库公开了一个Host API，但是假定对于使用GPU的操作，可以从设备直接访问必要的数据。使用cuDNN的应用程序必须通过调用来初始化库上下文的句柄 cudnnCreate（）。这个句柄被显式地传递给每一个在GPU数据上运行的后续库函数。一旦应用程序完成使用cuDNN，它就可以释放与库处理相关的资源 cudnnDestroy（）。这种方法允许用户在使用多个主机线程，GPU和CUDA流时显式控制库的功能。例如，一个应用程序可以使用cudaSetDevice（）要将不同的设备与不同的主机线程关联起来，并在每个主机线程中关联，请使用独特的cuDNN句柄，该句柄将库调用指向与其关联的设备。用不同的手柄创建的cuDNN库调用将自动运行在不同的设备上。与特定cuDNN上下文相关联的设备被假定在相应的之间保持不变cudnnCreate（） 和cudnnDestroy（）调用。为了使cuDNN库在同一个主机线程中使用不同的设备，应用程序必须设置通过调用使用的新设备cudaSetDevice（） 然后通过调用创建另一个与新设备关联的cuDNN上下文 cudnnCreate（）。现在所有的层都遵循推理期间的统一惯例y = layerFunction（x，otherParams）并在反向传播（dx，dOtherParams）= layerFunctionGradient（x，y，dy，otherParams）对于卷积来说，公式是y = x w + bw 是滤波器权重的矩阵， X 是上一层的数据（在推理期间）， y 是下一层的数据， b 是偏见和 是卷积运算符。在反向传播例程中，参数保持其含义。 DX，DY，DW，DB始终参考最终网络误差函数相对于给定参数的梯度。所以DY在所有后向传播例程中，总是指通过网络计算图反向传播的错误梯度。类似于更专门化层中的其他参数，例如，dMeans 或 dBnBias 指的是这些参数的损失函数的梯度。 Tensor描述符：该cuDNN库描述了数据保存图像，视频和任何其他数据的内容与通用n-D张量定义与以下参数： 一个尺寸dim为3到8 数据类型（32位浮点，64位浮点，16位浮点…） dim整数类型定义每个维度的大小 dim整数类型定义每个维度的步幅（例如，为了从同一维度到达下一个元素而添加的元素的数量） 前两个维度分别定义批量（batch）大小n和特征图c的数量。 该张量定义允许例如通过使一维的步幅小于下一维的尺寸和步幅的乘积而在相同张量内具有彼此重叠的一些尺寸。 在cuDNN中，除非另有规定，否则所有程序都将支持具有重叠尺寸的张量，以用于正向输入张量，但是输出张量的尺寸不能重叠。 尽管这种张量格式支持负跨度（对于数据镜像可能有用），但除非另有说明，否则cuDNN例程不支持负跨度的张量。 WXYZ张量描述符：张量描述符格式使用首字母缩略词来标识，每个字母都引用相应的维度。 所有的进步都是严格肯定的 字母所引用的尺寸按其各自步幅的降序排列 4-D张量描述符：使用4维张量描述符来定义4个字母的批量2D图像的格式：N，C，H，W分别表示批量大小，特征图的数量，高度和宽度。 这些字母按照步幅的降序排列。 常用的四维张量格式是： NCHW NHWC CHWN 5-D张量描述符：5-D Tensor描述符用于定义批量3D图像的格式，包含5个字母：N，C，D，H，W分别表示批量大小，特征图的数量，深度，高度和宽度。 这些字母按步伐的递减顺序排序。 常用的5维张量格式称为： NCDHW NDHWC CDHWN Fully-packed 张量：张量被定义为XYZ-fully-packed当且仅当： 张量维数等于完整填充后缀之前的字母数。 第i维的步幅等于（i + 1）维乘以第（i + 1）步的乘积。 最后一个维度的步幅是1。 Partially-packed 张量：部分“XYZ压缩”术语仅适用于用用于定义部分压缩张量的字母超集描述的张量格式的上下文。 WXYZ张量定义为XYZ-packed当且仅当： 在-packed后缀中未引用的所有维度的跨度大于或等于下一个跨度的下一个维度的乘积。 在位置i的-packed后缀中引用的每个维度的步幅等于（i + 1）-st维度乘以（i + 1）-st步幅的乘积。 如果最后张量的维数存在于-packed后缀中，则步幅为1 例如，NHWC张量WC包装意味着c_stride等于1，w_stride等于c_dim x c_stride。 实际上，-packed后缀通常具有张量变化最小的尺寸，但是也可以参考仅N-packed的NCHW张量。 空间填充张量：空间填充张量定义为空间维度中的部分填充。例如，空间压缩的4D张量将意味着张量是NCHW HW-packed或CNHW HW-packed。重叠张量：如果遍历整个维度范围多次产生相同的地址，则张量定义为重叠。在实践中，重叠张量对于来自[1，nbDims]区间的一些i，将具有步幅[i-1] 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189cudnnHandle_t cudnn; cudnnCreate(&amp;cudnn); // ================================== Input ====================================== cudnnTensorDescriptor_t input_descriptor; checkCUDNN(cudnnCreateTensorDescriptor(&amp;input_descriptor)); checkCUDNN(cudnnSetTensor4dDescriptor(input_descriptor, /*format=*/CUDNN_TENSOR_NHWC, /*dataType=*/CUDNN_DATA_FLOAT, /*batch_size=*/1, /*channels=*/3, /*image_height=*/image.rows, /*image_width=*/image.cols)); // ================================== Kernel ====================================== cudnnFilterDescriptor_t kernel_descriptor; checkCUDNN(cudnnCreateFilterDescriptor(&amp;kernel_descriptor)); checkCUDNN(cudnnSetFilter4dDescriptor(kernel_descriptor, /*dataType=*/CUDNN_DATA_FLOAT, /*format=*/CUDNN_TENSOR_NCHW, /*out_channels=*/3, /*in_channels=*/3, /*kernel_height=*/3, /*kernel_width=*/3)); // ================================== Conv ====================================== cudnnConvolutionDescriptor_t convolution_descriptor; checkCUDNN(cudnnCreateConvolutionDescriptor(&amp;convolution_descriptor)); checkCUDNN(cudnnSetConvolution2dDescriptor(convolution_descriptor, /*pad_height=*/1, /*pad_width=*/1, /*vertical_stride=*/1, /*horizontal_stride=*/1, /*dilation_height=*/1, /*dilation_width=*/1, /*mode=*/CUDNN_CONVOLUTION, /*computeType=*/CUDNN_DATA_FLOAT)); // ================================== Set group-conv ====================================== //int flag_group = 0; //std::cout &lt;&lt; "Enter group flag: "; //std::cin &gt;&gt; flag_group; //if (flag_group &gt;= 1) &#123; // checkCUDNN(cudnnSetConvolutionGroupCount(convolution_descriptor, flag_group)); //&#125; //int *ret = 0; //checkCUDNN(cudnnSetConvolutionGroupCount(convolution_descriptor, *ret)); //std::cout &lt;&lt; "ret: " &lt;&lt; *ret &lt;&lt; std::endl; // ================================== Output ====================================== int batch_size&#123; 0 &#125;, channels&#123; 0 &#125;, height&#123; 0 &#125;, width&#123; 0 &#125;; checkCUDNN(cudnnGetConvolution2dForwardOutputDim(convolution_descriptor, input_descriptor, kernel_descriptor, &amp;batch_size, &amp;channels, &amp;height, &amp;width)); std::cerr &lt;&lt; "Output Image: " &lt;&lt; height &lt;&lt; " x " &lt;&lt; width &lt;&lt; " x " &lt;&lt; channels &lt;&lt; std::endl; cudnnTensorDescriptor_t output_descriptor; checkCUDNN(cudnnCreateTensorDescriptor(&amp;output_descriptor)); checkCUDNN(cudnnSetTensor4dDescriptor(output_descriptor, /*format=*/CUDNN_TENSOR_NHWC, /*dataType=*/CUDNN_DATA_FLOAT, /*batch_size=*/1, /*channels=*/3, /*image_height=*/image.rows, /*image_width=*/image.cols)); // ================================== Algorithm ====================================== cudnnConvolutionFwdAlgo_t convolution_algorithm; checkCUDNN( cudnnGetConvolutionForwardAlgorithm(cudnn, input_descriptor, kernel_descriptor, convolution_descriptor, output_descriptor, CUDNN_CONVOLUTION_FWD_PREFER_FASTEST, /*memoryLimitInBytes=*/0, &amp;convolution_algorithm)); // ================================== Get space-size ====================================== size_t workspace_bytes = 0; checkCUDNN(cudnnGetConvolutionForwardWorkspaceSize(cudnn, input_descriptor, kernel_descriptor, convolution_descriptor, output_descriptor, convolution_algorithm, &amp;workspace_bytes)); std::cerr &lt;&lt; "Workspace size: " &lt;&lt; (workspace_bytes / 1048576.0) &lt;&lt; "MB" &lt;&lt; std::endl; //checkCUDNN(cudnnSetConvolutionGroupCount(convolution_descriptor, 3)); // ================================== Allocate memory ====================================== void* d_workspace&#123; nullptr &#125;; cudaMalloc(&amp;d_workspace, workspace_bytes); int image_bytes = batch_size * channels * height * width * sizeof(float); float* d_input&#123; nullptr &#125;; cudaMalloc(&amp;d_input, image_bytes); cudaMemcpy(d_input, image.ptr&lt;float&gt;(0), image_bytes, cudaMemcpyHostToDevice); float* d_output&#123; nullptr &#125;; cudaMalloc(&amp;d_output, image_bytes); cudaMemset(d_output, 0, image_bytes); // ================================== ====================================== // Mystery kernel const float kernel_template[3][3] = &#123; &#123; 1, 1, 1 &#125;, &#123; 1, -8, 1 &#125;, &#123; 1, 1, 1 &#125; &#125;; float h_kernel[3][3][3][3]; for (int kernel = 0; kernel &lt; 3; ++kernel) &#123; for (int channel = 0; channel &lt; 3; ++channel) &#123; for (int row = 0; row &lt; 3; ++row) &#123; for (int column = 0; column &lt; 3; ++column) &#123; h_kernel[kernel][channel][row][column] = kernel_template[row][column]; &#125; &#125; &#125; &#125; float* d_kernel&#123; nullptr &#125;; cudaMalloc(&amp;d_kernel, sizeof(h_kernel)); cudaMemcpy(d_kernel, h_kernel, sizeof(h_kernel), cudaMemcpyHostToDevice); // ================================== ====================================== const float alpha = 1, beta = 0; checkCUDNN(cudnnConvolutionForward(cudnn, &amp;alpha, input_descriptor, d_input, kernel_descriptor, d_kernel, convolution_descriptor, convolution_algorithm, d_workspace, workspace_bytes, &amp;beta, output_descriptor, d_output)); // ================================== Destroy ====================================== float* h_output = new float[image_bytes]; cudaMemcpy(h_output, d_output, image_bytes, cudaMemcpyDeviceToHost); // Save img save_image("cudnn-out.jpg", h_output, height, width); // Do something with h_output ... delete[] h_output; cudaFree(d_kernel); cudaFree(d_input); cudaFree(d_output); cudaFree(d_workspace); cudnnDestroyTensorDescriptor(input_descriptor); cudnnDestroyTensorDescriptor(output_descriptor); cudnnDestroyFilterDescriptor(kernel_descriptor); cudnnDestroyConvolutionDescriptor(convolution_descriptor); cudnnDestroy(cudnn); std::cout &lt;&lt; "\n[Procedure end...]" &lt;&lt; std::endl;]]></content>
      <categories>
        <category>硬件加速</category>
      </categories>
      <tags>
        <tag>标签1</tag>
        <tag>标签2</tag>
      </tags>
  </entry>
</search>
